import aiohttp
import asyncio
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional, Set
import pytz
from bs4 import BeautifulSoup
import feedparser
import openai
import os
import hashlib
import re

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    def __init__(self, config):
        self.config = config
        self.session = None
        self.news_buffer = []
        self.emergency_alerts_sent = {}  # ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€ìš©
        self.processed_news_hashes = set()  # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ ì €ì¥
        self.news_title_cache = {}  # ì œëª©ë³„ ìºì‹œ
        self.company_news_count = {}  # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸
        self.news_first_seen = {}  # ë‰´ìŠ¤ ìµœì´ˆ ë°œê²¬ ì‹œê°„
        
        # ë²ˆì—­ ìºì‹œ ë° rate limit ê´€ë¦¬
        self.translation_cache = {}  # ë²ˆì—­ ìºì‹œ
        self.translation_count = 0  # ë²ˆì—­ íšŸìˆ˜ ì¶”ì 
        self.last_translation_reset = datetime.now()
        self.max_translations_per_15min = 100  # 15ë¶„ë‹¹ ìµœëŒ€ ë²ˆì—­ ìˆ˜
        self.translation_reset_interval = 900  # 15ë¶„
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë²ˆì—­ìš©)
        self.openai_client = None
        if hasattr(config, 'OPENAI_API_KEY') and config.OPENAI_API_KEY:
            self.openai_client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        
        # ëª¨ë“  API í‚¤ë“¤
        self.newsapi_key = getattr(config, 'NEWSAPI_KEY', None)
        self.newsdata_key = getattr(config, 'NEWSDATA_KEY', None)
        self.alpha_vantage_key = getattr(config, 'ALPHA_VANTAGE_KEY', None)
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ (ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì˜í–¥ë§Œ)
        self.critical_keywords = [
            # ë¹„íŠ¸ì½”ì¸ ETF ê´€ë ¨ (ìµœìš°ì„ )
            'bitcoin etf approved', 'bitcoin etf rejected', 'spot bitcoin etf', 'etf decision',
            'blackrock bitcoin etf', 'fidelity bitcoin etf', 'ark bitcoin etf',
            'SEC ë¹„íŠ¸ì½”ì¸ ETF', 'ETF ìŠ¹ì¸', 'ETF ê±°ë¶€',
            
            # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ (ì§ì ‘ì )
            'tesla bought bitcoin', 'microstrategy bought bitcoin', 'bought bitcoin',
            'gamestop bitcoin purchase', 'metaplanet bitcoin', 'corporate bitcoin purchase',
            'bitcoin acquisition', 'adds bitcoin', 'bitcoin investment',
            'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë¹„íŠ¸ì½”ì¸ ë§¤ì…', 'BTC êµ¬ë§¤',
            
            # êµ­ê°€/ì€í–‰ ì±„íƒ
            'central bank bitcoin', 'russia bitcoin', 'sberbank bitcoin', 'bitcoin bonds',
            'government bitcoin', 'country adopts bitcoin', 'bitcoin legal tender',
            'ì¤‘ì•™ì€í–‰ ë¹„íŠ¸ì½”ì¸', 'ëŸ¬ì‹œì•„ ë¹„íŠ¸ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸ ì±„ê¶Œ',
            
            # ë¹„íŠ¸ì½”ì¸ ê·œì œ (ì§ì ‘ì )
            'sec bitcoin lawsuit', 'bitcoin ban', 'bitcoin regulation', 'bitcoin lawsuit',
            'china bans bitcoin', 'government bans bitcoin', 'court bitcoin',
            'regulatory approval bitcoin', 'regulatory rejection bitcoin',
            'SEC ë¹„íŠ¸ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸ ê¸ˆì§€', 'ë¹„íŠ¸ì½”ì¸ ê·œì œ',
            
            # ë¹„íŠ¸ì½”ì¸ ì‹œì¥ ê¸‰ë³€ë™
            'bitcoin crash', 'bitcoin surge', 'bitcoin breaks', 'bitcoin plunge',
            'bitcoin all time high', 'bitcoin ath', 'bitcoin tumbles', 'bitcoin soars',
            'ë¹„íŠ¸ì½”ì¸ í­ë½', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë½',
            
            # ëŒ€ëŸ‰ ë¹„íŠ¸ì½”ì¸ ì´ë™
            'whale alert bitcoin', 'large bitcoin transfer', 'bitcoin moved exchange',
            'massive bitcoin', 'billion bitcoin', 'btc whale',
            'ê³ ë˜ ë¹„íŠ¸ì½”ì¸', 'ëŒ€ëŸ‰ ë¹„íŠ¸ì½”ì¸', 'BTC ì´ë™',
            
            # ë¹„íŠ¸ì½”ì¸ í•´í‚¹/ë³´ì•ˆ
            'bitcoin stolen', 'bitcoin hack', 'exchange hacked bitcoin',
            'bitcoin security breach', 'btc stolen',
            'ë¹„íŠ¸ì½”ì¸ ë„ë‚œ', 'ë¹„íŠ¸ì½”ì¸ í•´í‚¹', 'ê±°ë˜ì†Œ í•´í‚¹',
            
            # Fed ê¸ˆë¦¬ ê²°ì • (ë¹„íŠ¸ì½”ì¸ ì˜í–¥)
            'fed rate decision bitcoin', 'fomc bitcoin', 'interest rate bitcoin',
            'powell bitcoin', 'federal reserve bitcoin',
            'ì—°ì¤€ ë¹„íŠ¸ì½”ì¸', 'ê¸ˆë¦¬ ë¹„íŠ¸ì½”ì¸'
        ]
        
        # ì œì™¸ í‚¤ì›Œë“œ (ë¹„íŠ¸ì½”ì¸ê³¼ ë¬´ê´€í•œ ê²ƒë“¤)
        self.exclude_keywords = [
            'how to mine', 'ì§‘ì—ì„œ ì±„êµ´', 'mining at home', 'mining tutorial',
            'price prediction tutorial', 'ê°€ê²© ì˜ˆì¸¡ ë°©ë²•', 'technical analysis tutorial',
            'altcoin', 'ethereum', 'ripple', 'cardano', 'solana', 'dogecoin', 'shiba',
            'defi', 'nft', 'web3', 'metaverse', 'gamefi',
            'stock market', 'dow jones', 'nasdaq', 's&p 500',
            'oil price', 'gold price', 'commodity',
            'sports', 'entertainment', 'celebrity'
        ]
        
        # ì¤‘ìš” ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (ë¹„íŠ¸ì½”ì¸ ë³´ìœ /ê´€ë ¨)
        self.important_companies = [
            'tesla', 'microstrategy', 'square', 'block', 'paypal', 'mastercard',
            'gamestop', 'gme', 'blackrock', 'fidelity', 'ark invest',
            'coinbase', 'binance', 'kraken', 'bitget',
            'metaplanet', 'ë©”íƒ€í”Œë˜ë‹›', 'í…ŒìŠ¬ë¼', 'ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€',
            'sberbank', 'ìŠ¤ë² ë¥´ë°©í¬', 'jpmorgan', 'goldman sachs'
        ]
        
        # RSS í”¼ë“œ - ì•”í˜¸í™”í ì „ë¬¸ ì†ŒìŠ¤ ìœ„ì£¼
        self.rss_feeds = [
            # ì•”í˜¸í™”í ì „ë¬¸ (ìµœìš°ì„ )
            {'url': 'https://cointelegraph.com/rss', 'source': 'Cointelegraph', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'source': 'CoinDesk', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://decrypt.co/feed', 'source': 'Decrypt', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://bitcoinmagazine.com/.rss/full/', 'source': 'Bitcoin Magazine', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://cryptopotato.com/feed/', 'source': 'CryptoPotato', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://u.today/rss', 'source': 'U.Today', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://ambcrypto.com/feed/', 'source': 'AMBCrypto', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://cryptonews.com/news/feed/', 'source': 'Cryptonews', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://www.watcher.guru/news/feed', 'source': 'Watcher.Guru', 'weight': 9, 'category': 'crypto'},  # WatcherGuru ì¶”ê°€
            
            # ê¸ˆìœµ (Fed/ê·œì œ ê´€ë ¨)
            {'url': 'https://feeds.bloomberg.com/markets/news.rss', 'source': 'Bloomberg Markets', 'weight': 9, 'category': 'finance'},
            {'url': 'https://feeds.bloomberg.com/economics/news.rss', 'source': 'Bloomberg Economics', 'weight': 8, 'category': 'finance'},
            {'url': 'https://www.marketwatch.com/rss/topstories', 'source': 'MarketWatch', 'weight': 7, 'category': 'finance'},
            {'url': 'https://feeds.reuters.com/reuters/businessNews', 'source': 'Reuters Business', 'weight': 8, 'category': 'news'},
        ]
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.api_usage = {
            'newsapi_today': 0,
            'newsdata_today': 0,
            'alpha_vantage_today': 0,
            'last_reset': datetime.now().date()
        }
        
        # API ì¼ì¼ í•œë„
        self.api_limits = {
            'newsapi': 20,
            'newsdata': 10,
            'alpha_vantage': 2
        }
        
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ë¹„íŠ¸ì½”ì¸ ì „ìš© í•„í„°ë§ ê°•í™”")
        logger.info(f"ğŸ“Š ì„¤ì •: RSS 15ì´ˆ ì²´í¬, ë²ˆì—­ 15ë¶„ë‹¹ {self.max_translations_per_15min}ê°œ, í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ {len(self.critical_keywords)}ê°œ")
    
    def _reset_translation_count_if_needed(self):
        """í•„ìš”ì‹œ ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        now = datetime.now()
        if (now - self.last_translation_reset).total_seconds() > self.translation_reset_interval:
            old_count = self.translation_count
            self.translation_count = 0
            self.last_translation_reset = now
            if old_count > 0:
                logger.info(f"ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {old_count} â†’ 0")
    
    def _should_translate(self, article: Dict) -> bool:
        """ë‰´ìŠ¤ë¥¼ ë²ˆì—­í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •"""
        # ì´ë¯¸ í•œê¸€ ì œëª©ì´ ìˆìœ¼ë©´ ë²ˆì—­ ë¶ˆí•„ìš”
        if article.get('title_ko') and article['title_ko'] != article.get('title', ''):
            return False
        
        # í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ëŠ” í•­ìƒ ë²ˆì—­
        if self._is_critical_news(article):
            return True
        
        # ë†’ì€ ê°€ì¤‘ì¹˜ + ì•”í˜¸í™”í ì¹´í…Œê³ ë¦¬
        if article.get('weight', 0) >= 8 and article.get('category') == 'crypto':
            return True
        
        # API ë‰´ìŠ¤
        if article.get('category') == 'api' and article.get('weight', 0) >= 9:
            return True
        
        return False
    
    async def translate_text(self, text: str, max_length: int = 400) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ - ì™„ë²½íˆ ìì—°ìŠ¤ëŸ½ê²Œ"""
        if not self.openai_client:
            return text
        
        # ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ì²´í¬
        self._reset_translation_count_if_needed()
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]
        
        # Rate limit ì²´í¬
        if self.translation_count >= self.max_translations_per_15min:
            logger.warning(f"ë²ˆì—­ í•œë„ ì´ˆê³¼: {self.translation_count}/{self.max_translations_per_15min}")
            return text
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {
                        "role": "system", 
                        "content": """ë‹¹ì‹ ì€ í•œêµ­ì˜ ë¸”ë¡ì²´ì¸ ì „ë¬¸ ê¸°ìì…ë‹ˆë‹¤. ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ë¥¼ í•œêµ­ ë…ìë“¤ì´ ì¦‰ì‹œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë§¤ë„ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.

ë²ˆì—­ ì›ì¹™:
1. í•œêµ­ ê²½ì œ ë‰´ìŠ¤ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­
2. í•µì‹¬ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ì „ë‹¬:
   - ì£¼ì²´ (ê¸°ì—…/ì¸ë¬¼/êµ­ê°€)
   - í–‰ë™ (ë§¤ì…, ë§¤ë„, ë°œí‘œ, ìŠ¹ì¸ ë“±)
   - ê·œëª¨ (ê¸ˆì•¡, ìˆ˜ëŸ‰)
   - ì˜í–¥/ì˜ë¯¸
3. ì „ë¬¸ ìš©ì–´ ì²˜ë¦¬:
   - MicroStrategy â†’ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€
   - Tesla â†’ í…ŒìŠ¬ë¼  
   - Sberbank â†’ ìŠ¤ë² ë¥´ë°©í¬
   - BlackRock â†’ ë¸”ë™ë¡
   - SEC â†’ SEC (ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒ)
   - ETF â†’ ETF
   - Bitcoin bonds â†’ ë¹„íŠ¸ì½”ì¸ ì—°ê³„ ì±„ê¶Œ
4. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ êµ¬ì¡° ì‚¬ìš©
5. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°, í•µì‹¬ë§Œ ì „ë‹¬

ì˜ˆì‹œ:
"MicroStrategy buys 500 BTC" â†’ "ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€, ë¹„íŠ¸ì½”ì¸ 500ê°œ ì¶”ê°€ ë§¤ì…"
"Russia's Sberbank launches Bitcoin-linked bonds" â†’ "ëŸ¬ì‹œì•„ ìµœëŒ€ ì€í–‰ ìŠ¤ë² ë¥´ë°©í¬, ë¹„íŠ¸ì½”ì¸ ì—°ê³„ ì±„ê¶Œ ì¶œì‹œ"
"SEC approves spot Bitcoin ETF" â†’ "SEC, í˜„ë¬¼ ë¹„íŠ¸ì½”ì¸ ETF ìŠ¹ì¸""""
                    },
                    {
                        "role": "user", 
                        "content": f"ë‹¤ìŒ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš” (ìµœëŒ€ {max_length}ì):\n\n{text}"
                    }
                ],
                max_tokens=600,
                temperature=0.3
            )
            
            translated = response.choices[0].message.content.strip()
            
            # ê¸¸ì´ ì²´í¬
            if len(translated) > max_length:
                # ì˜ë¯¸ê°€ ëŠê¸°ì§€ ì•Šë„ë¡ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
                sentences = translated.split('.')
                result = ""
                for sentence in sentences:
                    if len(result + sentence + ".") <= max_length - 3:
                        result += sentence + "."
                    else:
                        break
                translated = result.strip()
                if not translated:
                    translated = translated[:max_length-3] + "..."
            
            # ìºì‹œ ì €ì¥ ë° ì¹´ìš´íŠ¸ ì¦ê°€
            self.translation_cache[cache_key] = translated
            self.translation_count += 1
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.translation_cache) > 1000:
                keys_to_remove = list(self.translation_cache.keys())[:500]
                for key in keys_to_remove:
                    del self.translation_cache[key]
            
            return translated
            
        except openai.RateLimitError:
            logger.warning("OpenAI Rate limit ë„ë‹¬")
            self.translation_count = self.max_translations_per_15min
            return text
        except Exception as e:
            logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:50]}")
            # GPT-3.5ë¡œ í´ë°±
            try:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system", 
                            "content": "ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. ê¸°ì—…ëª…ì€ í•œêµ­ì‹ìœ¼ë¡œ (í…ŒìŠ¬ë¼, ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€ ë“±), ê¸ˆì•¡ê³¼ í–‰ë™ì„ ëª…í™•íˆ í‘œí˜„í•´ì£¼ì„¸ìš”."
                        },
                        {
                            "role": "user", 
                            "content": f"ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ìµœëŒ€ {max_length}ì):\n{text}"
                        }
                    ],
                    max_tokens=400,
                    temperature=0.3
                )
                
                translated = response.choices[0].message.content.strip()
                if len(translated) > max_length:
                    translated = translated[:max_length-3] + "..."
                
                self.translation_cache[cache_key] = translated
                self.translation_count += 1
                
                return translated
            except:
                return text
    
    async def summarize_article(self, title: str, description: str, max_length: int = 500) -> str:
        """ê¸°ì‚¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìƒì„¸ ìš”ì•½"""
        if not self.openai_client or not description:
            return ""
        
        # ì´ë¯¸ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(description) <= 200:
            return ""
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {
                        "role": "system", 
                        "content": """ë‹¹ì‹ ì€ í•œêµ­ì˜ ë¹„íŠ¸ì½”ì¸ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ í•œêµ­ íˆ¬ììë“¤ì´ ì¦‰ì‹œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ìƒì„¸íˆ ìš”ì•½í•©ë‹ˆë‹¤.

ìš”ì•½ ì›ì¹™:
1. íˆ¬ì íŒë‹¨ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ í¬í•¨:
   - ëˆ„ê°€: ê¸°ì—…/ì¸ë¬¼/êµ­ê°€ëª… (í•œêµ­ì‹ í‘œê¸°)
   - ë¬´ì—‡ì„: êµ¬ì²´ì  í–‰ë™ (ë§¤ì…, ë§¤ë„, ë°œí‘œ, ì¶œì‹œ ë“±)
   - ì–¼ë§ˆë‚˜: ì •í™•í•œ ê¸ˆì•¡/ìˆ˜ëŸ‰
   - ì–¸ì œ: ì‹œê¸° ì •ë³´
   - ì™œ: ë°°ê²½ê³¼ ì´ìœ 
   - ì˜í–¥: ì‹œì¥ì— ë¯¸ì¹  ì˜í–¥
2. íˆ¬ìì ê´€ì ì—ì„œ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë¦¬
3. êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì‚¬ì‹¤ ìœ„ì£¼
4. ë¶ˆí™•ì‹¤í•œ ì¶”ì¸¡ì€ ì œì™¸
5. í•œêµ­ íˆ¬ììê°€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” í‘œí˜„ ì‚¬ìš©

ì˜ˆì‹œ:
"ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€ê°€ 12ì›” 15ì¼ 580,955ê°œì˜ ë¹„íŠ¸ì½”ì¸ì„ ë³´ìœ í•˜ê²Œ ë˜ì—ˆë‹¤. ì´ëŠ” ì•½ 270ì–µ ë‹¬ëŸ¬ ê·œëª¨ë¡œ, ì „ì²´ ë¹„íŠ¸ì½”ì¸ ê³µê¸‰ëŸ‰ì˜ 2.7%ì— í•´ë‹¹í•œë‹¤. í‰ê·  ë§¤ì…ê°€ëŠ” 46,500ë‹¬ëŸ¬ì´ë©°, í˜„ì¬ ì‹œì„¸ ëŒ€ë¹„ 30% ìˆ˜ìµì„ ë³´ê³  ìˆë‹¤."
"""
                    },
                    {
                        "role": "user", 
                        "content": f"ë‹¤ìŒ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„¸ ìš”ì•½í•´ì£¼ì„¸ìš” (ìµœëŒ€ {max_length}ì):\n\nì œëª©: {title}\n\në‚´ìš©: {description[:1500]}"
                    }
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            
            if len(summary) > max_length:
                sentences = summary.split('.')
                result = ""
                for sentence in sentences:
                    if len(result + sentence + ".") <= max_length - 3:
                        result += sentence + "."
                    else:
                        break
                summary = result.strip() or summary[:max_length-3] + "..."
            
            return summary
            
        except Exception as e:
            logger.warning(f"ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}")
            # GPT-3.5ë¡œ í´ë°±
            try:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system", 
                            "content": "ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. ëˆ„ê°€, ë¬´ì—‡ì„, ì–¼ë§ˆë‚˜, ì™œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”."
                        },
                        {
                            "role": "user", 
                            "content": f"ìš”ì•½ (ìµœëŒ€ {max_length}ì):\nì œëª©: {title}\në‚´ìš©: {description[:1000]}"
                        }
                    ],
                    max_tokens=600,
                    temperature=0.3
                )
                
                summary = response.choices[0].message.content.strip()
                if len(summary) > max_length:
                    summary = summary[:max_length-3] + "..."
                
                return summary
            except:
                return description[:max_length] + "..." if len(description) > max_length else description
    
    def _extract_company_from_content(self, title: str, description: str = "") -> str:
        """ì»¨í…ì¸ ì—ì„œ ê¸°ì—…ëª… ì¶”ì¶œ"""
        content = (title + " " + description).lower()
        
        # ì¤‘ìš” ê¸°ì—… í™•ì¸
        found_companies = []
        for company in self.important_companies:
            if company.lower() in content:
                # ì›ë˜ ëŒ€ì†Œë¬¸ì ìœ ì§€
                for original in self.important_companies:
                    if original.lower() == company.lower():
                        found_companies.append(original)
                        break
        
        # ì²« ë²ˆì§¸ ë°œê²¬ëœ ê¸°ì—… ë°˜í™˜
        if found_companies:
            return found_companies[0]
        
        # íŠ¹ì • íŒ¨í„´ìœ¼ë¡œ ê¸°ì—…ëª… ì°¾ê¸°
        patterns = [
            r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+(?:bought|purchased|acquired|adds)',
            r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+bitcoin',
            r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+BTC',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, title + " " + description)
            if matches:
                # ì•Œë ¤ì§„ ê¸°ì—…ëª…ì¸ì§€ í™•ì¸
                for match in matches:
                    if len(match) > 2 and match.lower() not in ['the', 'and', 'for', 'with']:
                        return match
        
        return ""
    
    def _generate_content_hash(self, title: str, description: str = "") -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì˜ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©) - ë” ì—„ê²©í•˜ê²Œ"""
        # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        content = f"{title} {description[:200]}".lower()
        
        # ìˆ«ì ì •ê·œí™” (580,955 -> 580955)
        content = re.sub(r'[\d,]+', lambda m: m.group(0).replace(',', ''), content)
        
        # íšŒì‚¬ëª… ì •ê·œí™”
        companies_found = []
        for company in self.important_companies:
            if company.lower() in content:
                companies_found.append(company.lower())
        
        # ì•¡ì…˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        action_keywords = []
        actions = ['bought', 'purchased', 'acquired', 'adds', 'buys', 'sells', 'sold', 
                  'announced', 'launches', 'approves', 'rejects', 'bans']
        for action in actions:
            if action in content:
                action_keywords.append(action)
        
        # BTC ìˆ˜ëŸ‰ ì¶”ì¶œ
        btc_amounts = re.findall(r'(\d+(?:,\d+)*)\s*(?:btc|bitcoin)', content)
        
        # ê³ ìœ  ì‹ë³„ì ìƒì„±
        unique_parts = []
        if companies_found:
            unique_parts.append('_'.join(sorted(companies_found)))
        if action_keywords:
            unique_parts.append('_'.join(sorted(action_keywords)))
        if btc_amounts:
            unique_parts.append('_'.join(btc_amounts))
        
        # í•´ì‹œ ìƒì„±
        if unique_parts:
            hash_content = '|'.join(unique_parts)
        else:
            # í•µì‹¬ ë‹¨ì–´ë§Œ ì¶”ì¶œ
            words = re.findall(r'\b[a-z]{4,}\b', content)
            important_words = [w for w in words if w not in ['that', 'this', 'with', 'from', 'have', 'been', 'their', 'about']]
            hash_content = ' '.join(sorted(important_words[:10]))
        
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def _is_duplicate_emergency(self, article: Dict, time_window: int = 120) -> bool:
        """ê¸´ê¸‰ ì•Œë¦¼ì´ ì¤‘ë³µì¸ì§€ í™•ì¸ (120ë¶„ ì´ë‚´) - ë” ì—„ê²©í•˜ê²Œ"""
        try:
            current_time = datetime.now()
            content_hash = self._generate_content_hash(
                article.get('title', ''), 
                article.get('description', '')
            )
            
            # ì‹œê°„ì´ ì§€ë‚œ ì•Œë¦¼ ì œê±°
            cutoff_time = current_time - timedelta(minutes=time_window)
            self.emergency_alerts_sent = {
                k: v for k, v in self.emergency_alerts_sent.items()
                if v > cutoff_time
            }
            
            # ì¤‘ë³µ ì²´í¬
            if content_hash in self.emergency_alerts_sent:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€: {article.get('title', '')[:50]}...")
                return True
            
            # ì œëª© ìœ ì‚¬ì„± ì²´í¬
            current_title = article.get('title', '').lower()
            for sent_hash in self.emergency_alerts_sent:
                # ì´ë¯¸ ì „ì†¡ëœ ë‰´ìŠ¤ë“¤ê³¼ ìœ ì‚¬ì„± ì²´í¬
                if self._calculate_title_similarity(current_title, sent_hash) > 0.8:
                    logger.info(f"ğŸ”„ ìœ ì‚¬ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€: {article.get('title', '')[:50]}...")
                    return True
            
            # ìƒˆë¡œìš´ ì•Œë¦¼ ê¸°ë¡
            self.emergency_alerts_sent[content_hash] = current_time
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _calculate_title_similarity(self, title1: str, title2_hash: str) -> float:
        """ì œëª© ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(re.findall(r'\b\w+\b', title1.lower()))
        # í•´ì‹œëŠ” ì§ì ‘ ë¹„êµ ë¶ˆê°€í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜
        return 0.0
    
    def _is_similar_news(self, title1: str, title2: str) -> bool:
        """ë‘ ë‰´ìŠ¤ ì œëª©ì´ ìœ ì‚¬í•œì§€ í™•ì¸"""
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean1 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title1.lower())
        clean2 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title2.lower())
        
        clean1 = re.sub(r'\s+', ' ', clean1).strip()
        clean2 = re.sub(r'\s+', ' ', clean2).strip()
        
        # íŠ¹ì • íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ ë‰´ìŠ¤ì¸ì§€ ì²´í¬
        for company in self.important_companies:
            company_lower = company.lower()
            if company_lower in clean1 and company_lower in clean2:
                bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought']
                if any(keyword in clean1 for keyword in bitcoin_keywords) and \
                   any(keyword in clean2 for keyword in bitcoin_keywords):
                    return True
        
        # ë‹¨ì–´ ì§‘í•© ë¹„êµ
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        if not words1 or not words2:
            return False
        
        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        similarity = intersection / union if union > 0 else 0
        
        # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
        return similarity > 0.7
    
    def _is_recent_news(self, article: Dict, hours: int = 2) -> bool:
        """ë‰´ìŠ¤ê°€ ìµœê·¼ ê²ƒì¸ì§€ í™•ì¸ - 2ì‹œê°„ ì´ë‚´"""
        try:
            pub_time_str = article.get('published_at', '')
            if not pub_time_str:
                return True
            
            try:
                if 'T' in pub_time_str:
                    pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                else:
                    from dateutil import parser
                    pub_time = parser.parse(pub_time_str)
                
                if pub_time.tzinfo is None:
                    pub_time = pytz.UTC.localize(pub_time)
                
                time_diff = datetime.now(pytz.UTC) - pub_time
                return time_diff.total_seconds() < (hours * 3600)
            except:
                return True
        except:
            return True
    
    async def start_monitoring(self):
        """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10),
                connector=aiohttp.TCPConnector(limit=100, limit_per_host=30)
            )
        
        logger.info("ğŸ” ë¹„íŠ¸ì½”ì¸ ì „ìš© ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        self.company_news_count = {}
        
        tasks = [
            self.monitor_rss_feeds(),      # RSS (15ì´ˆë§ˆë‹¤)
            self.monitor_reddit(),         # Reddit (10ë¶„ë§ˆë‹¤)
            self.aggressive_api_rotation() # API ìˆœí™˜ ì‚¬ìš©
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_rss_feeds(self):
        """RSS í”¼ë“œ ëª¨ë‹ˆí„°ë§ - 15ì´ˆë§ˆë‹¤"""
        while True:
            try:
                # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì†ŒìŠ¤ë¶€í„° ì²˜ë¦¬
                sorted_feeds = sorted(self.rss_feeds, key=lambda x: x['weight'], reverse=True)
                successful_feeds = 0
                processed_articles = 0
                
                for feed_info in sorted_feeds:
                    try:
                        articles = await self._parse_rss_feed(feed_info)
                        
                        if articles:
                            successful_feeds += 1
                            
                            for article in articles:
                                # ìµœì‹  ë‰´ìŠ¤ë§Œ ì²˜ë¦¬ (2ì‹œê°„ ì´ë‚´)
                                if not self._is_recent_news(article, hours=2):
                                    continue
                                
                                # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ì²´í¬
                                if not self._is_bitcoin_related(article):
                                    continue
                                
                                # ê¸°ì—…ëª… ì¶”ì¶œ
                                company = self._extract_company_from_content(
                                    article.get('title', ''),
                                    article.get('description', '')
                                )
                                if company:
                                    article['company'] = company
                                
                                # ë²ˆì—­ í•„ìš” ì—¬ë¶€ ì²´í¬
                                if self.openai_client and self._should_translate(article):
                                    article['title_ko'] = await self.translate_text(article['title'])
                                    
                                    # ìš”ì•½ ìƒì„± (í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ë§Œ)
                                    if self._is_critical_news(article):
                                        summary = await self.summarize_article(
                                            article['title'],
                                            article.get('description', '')
                                        )
                                        if summary:
                                            article['summary'] = summary
                                else:
                                    article['title_ko'] = article.get('title', '')
                                
                                # í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ì²´í¬
                                if self._is_critical_news(article):
                                    if not self._is_duplicate_emergency(article):
                                        article['expected_change'] = self._estimate_price_impact(article)
                                        await self._trigger_emergency_alert(article)
                                        processed_articles += 1
                                
                                # ì¤‘ìš” ë‰´ìŠ¤ëŠ” ë²„í¼ì— ì¶”ê°€
                                elif self._is_important_news(article):
                                    await self._add_to_news_buffer(article)
                                    processed_articles += 1
                    
                    except Exception as e:
                        logger.warning(f"RSS í”¼ë“œ ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:50]}")
                        continue
                
                if processed_articles > 0:
                    logger.info(f"ğŸ“° RSS ìŠ¤ìº” ì™„ë£Œ: {successful_feeds}ê°œ í”¼ë“œ, {processed_articles}ê°œ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤")
                
                await asyncio.sleep(15)  # 15ì´ˆë§ˆë‹¤
                
            except Exception as e:
                logger.error(f"RSS ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)
    
    def _is_bitcoin_related(self, article: Dict) -> bool:
        """ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ê´€ë ¨ì„± ì²´í¬"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ë¨¼ì € ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰
        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸']
        has_bitcoin = any(keyword in content for keyword in bitcoin_keywords)
        
        if has_bitcoin:
            return True
        
        # ì•”í˜¸í™”í ì¼ë°˜ + ì¤‘ìš” ë‚´ìš©
        crypto_keywords = ['crypto', 'cryptocurrency', 'ì•”í˜¸í™”í']
        has_crypto = any(keyword in content for keyword in crypto_keywords)
        
        if has_crypto:
            # ETF, SEC, ê·œì œ ë“± ì¤‘ìš” í‚¤ì›Œë“œì™€ í•¨ê»˜ ë‚˜ì˜¤ë©´ í¬í•¨
            important_terms = ['etf', 'sec', 'regulation', 'ban', 'approval', 'court', 'lawsuit', 'bonds', 'russia', 'sberbank']
            if any(term in content for term in important_terms):
                return True
        
        # Fed ê¸ˆë¦¬ ê²°ì • (ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰ ì—†ì–´ë„ ì¤‘ìš”)
        fed_keywords = ['fed rate decision', 'fomc decides', 'interest rate hike', 'interest rate cut', 'powell announces']
        if any(keyword in content for keyword in fed_keywords):
            return True
        
        return False
    
    def _estimate_price_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ì˜ ì˜ˆìƒ ê°€ê²© ì˜í–¥ ì¶”ì • - ëª…í™•í•˜ê²Œ ìƒìŠ¹/í•˜ë½ í‘œì‹œ"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ETF ê´€ë ¨
        if 'etf approved' in content or 'etf approval' in content:
            return 'ğŸ“ˆ ìƒìŠ¹ +1~3%'
        elif 'etf rejected' in content or 'etf rejection' in content:
            return 'ğŸ“‰ í•˜ë½ -1~3%'
        elif 'etf' in content:
            return 'âš¡ ë³€ë™ Â±0.5~1%'
        
        # ê¸°ì—…/êµ­ê°€ êµ¬ë§¤
        for entity in ['tesla', 'microstrategy', 'gamestop', 'blackrock', 'russia', 'sberbank']:
            if entity in content and any(word in content for word in ['bought', 'purchased', 'buys', 'adds', 'launches', 'bonds']):
                if 'billion' in content:
                    return 'ğŸ“ˆ ìƒìŠ¹ +0.5~2%'
                elif 'million' in content:
                    return 'ğŸ“ˆ ìƒìŠ¹ +0.3~1%'
                else:
                    return 'ğŸ“ˆ ìƒìŠ¹ +0.2~0.5%'
        
        # ê·œì œ/ê¸ˆì§€
        if any(word in content for word in ['ban', 'banned', 'prohibit']):
            if 'china' in content:
                return 'ğŸ“‰ í•˜ë½ -2~4%'
            else:
                return 'ğŸ“‰ í•˜ë½ -1~3%'
        elif 'lawsuit' in content or 'sue' in content:
            return 'ğŸ“‰ í•˜ë½ -0.5~2%'
        elif 'regulation' in content:
            return 'âš¡ ë³€ë™ Â±0.5~1.5%'
        
        # Fed ê¸ˆë¦¬
        if any(word in content for word in ['rate hike', 'rates higher', 'hawkish']):
            return 'ğŸ“‰ í•˜ë½ -0.5~2%'
        elif any(word in content for word in ['rate cut', 'rates lower', 'dovish']):
            return 'ğŸ“ˆ ìƒìŠ¹ +0.5~2%'
        elif 'fed' in content or 'fomc' in content:
            return 'âš¡ ë³€ë™ Â±0.3~1%'
        
        # ì‹œì¥ ê¸‰ë³€ë™
        if any(word in content for word in ['crash', 'plunge', 'tumble']):
            return 'ğŸ“‰ í•˜ë½ -3~5%'
        elif any(word in content for word in ['surge', 'soar', 'rally', 'all time high', 'ath']):
            return 'ğŸ“ˆ ìƒìŠ¹ +2~4%'
        
        # í•´í‚¹/ë³´ì•ˆ
        if any(word in content for word in ['hack', 'stolen', 'breach']):
            if 'billion' in content:
                return 'ğŸ“‰ í•˜ë½ -1~3%'
            else:
                return 'ğŸ“‰ í•˜ë½ -0.5~1.5%'
        
        # ê³ ë˜ ì´ë™
        if 'whale' in content or 'large transfer' in content:
            if 'exchange' in content:
                return 'âš¡ ë³€ë™ Â±0.5~1.5%'
            else:
                return 'âš¡ ë³€ë™ Â±0.2~0.5%'
        
        # ê¸°ë³¸ê°’
        return 'âš¡ ë³€ë™ Â±0.3~1%'
    
    def _is_critical_news(self, article: Dict) -> bool:
        """í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨ - ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì˜í–¥ë§Œ"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ì²´í¬
        if not self._is_bitcoin_related(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ê°€ì¤‘ì¹˜ ì²´í¬ (8 ì´ìƒë§Œ)
        if article.get('weight', 0) < 8:
            return False
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.critical_keywords:
            if keyword.lower() in content:
                # ë¶€ì •ì  í•„í„° (ë£¨ë¨¸, ì¶”ì¸¡ ë“±)
                negative_filters = ['rumor', 'speculation', 'unconfirmed', 'fake', 'false', 'ë£¨ë¨¸', 'ì¶”ì¸¡', 'ë¯¸í™•ì¸']
                if any(neg in content for neg in negative_filters):
                    continue
                
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ê°ì§€: {keyword} - {article.get('title', '')[:50]}...")
                return True
        
        # ì¶”ê°€ í¬ë¦¬í‹°ì»¬ íŒ¨í„´
        critical_patterns = [
            ('bitcoin', 'billion', 'bought'),  # ëŒ€ê·œëª¨ êµ¬ë§¤
            ('bitcoin', 'court', 'ruling'),     # ë²•ì› íŒê²°
            ('bitcoin', 'sec', 'decision'),     # SEC ê²°ì •
            ('bitcoin', 'ban', 'government'),   # ì •ë¶€ ê¸ˆì§€
            ('bitcoin', 'etf', 'launch'),       # ETF ì¶œì‹œ
            ('fed', 'rate', 'decision'),        # Fed ê¸ˆë¦¬ ê²°ì •
            ('russia', 'bitcoin', 'bonds'),     # ëŸ¬ì‹œì•„ ë¹„íŠ¸ì½”ì¸ ì±„ê¶Œ
            ('sberbank', 'bitcoin'),            # ìŠ¤ë² ë¥´ë°©í¬ ë¹„íŠ¸ì½”ì¸
        ]
        
        for pattern in critical_patterns:
            if all(word in content for word in pattern):
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ íŒ¨í„´ ê°ì§€: {pattern} - {article.get('title', '')[:50]}...")
                return True
        
        return False
    
    def _is_important_news(self, article: Dict) -> bool:
        """ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ì²´í¬
        if not self._is_bitcoin_related(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ê°€ì¤‘ì¹˜ì™€ ì¹´í…Œê³ ë¦¬ ì²´í¬
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # ì¡°ê±´ë“¤
        conditions = [
            # ì•”í˜¸í™”í ì „ë¬¸ ì†ŒìŠ¤ + ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰
            category == 'crypto' and any(word in content for word in ['bitcoin', 'btc']) and weight >= 7,
            
            # ê¸ˆìœµ ì†ŒìŠ¤ + ë¹„íŠ¸ì½”ì¸ ë˜ëŠ” ì¤‘ìš” í‚¤ì›Œë“œ
            category == 'finance' and weight >= 7 and (
                any(word in content for word in ['bitcoin', 'btc', 'crypto']) or
                any(word in content for word in ['fed', 'rate', 'regulation', 'sec'])
            ),
            
            # API ë‰´ìŠ¤ + ë†’ì€ ê°€ì¤‘ì¹˜
            category == 'api' and weight >= 9,
            
            # ê¸°ì—… + ë¹„íŠ¸ì½”ì¸
            any(company.lower() in content for company in self.important_companies) and 
            any(word in content for word in ['bitcoin', 'btc', 'crypto']),
        ]
        
        return any(conditions)
    
    def _determine_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ ì˜í–¥ë„ íŒë‹¨ - ê°„ë‹¨ëª…ë£Œí•˜ê²Œ"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        expected_change = self._estimate_price_impact(article)
        
        # ì˜ˆìƒ ë³€ë™ë¥ ì— ë”°ë¥¸ ì˜í–¥ë„
        if 'ğŸ“ˆ' in expected_change:
            if any(x in expected_change for x in ['3%', '4%', '5%']):
                return "ğŸ“ˆ ê°•í•œ í˜¸ì¬"
            else:
                return "ğŸ“ˆ í˜¸ì¬"
        elif 'ğŸ“‰' in expected_change:
            if any(x in expected_change for x in ['3%', '4%', '5%']):
                return "ğŸ“‰ ê°•í•œ ì•…ì¬"
            else:
                return "ğŸ“‰ ì•…ì¬"
        else:
            return "âš¡ ë³€ë™ì„±"
    
    async def _trigger_emergency_alert(self, article: Dict):
        """ê¸´ê¸‰ ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        try:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return
            
            # ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¡œ ê¸°ë¡
            self.processed_news_hashes.add(content_hash)
            
            # ì˜¤ë˜ëœ í•´ì‹œ ì •ë¦¬
            if len(self.processed_news_hashes) > 1000:
                self.processed_news_hashes = set(list(self.processed_news_hashes)[-500:])
            
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ ê¸°ë¡
            if content_hash not in self.news_first_seen:
                self.news_first_seen[content_hash] = datetime.now()
            
            event = {
                'type': 'critical_news',
                'title': article.get('title', ''),
                'title_ko': article.get('title_ko', article.get('title', '')),
                'description': article.get('description', '')[:1400],  # 1400ì ìœ ì§€
                'summary': article.get('summary', ''),  # ìš”ì•½ ì¶”ê°€
                'company': article.get('company', ''),  # ê¸°ì—…ëª… ì¶”ê°€
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self._determine_impact(article),
                'expected_change': article.get('expected_change', 'Â±0.3%'),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown'),
                'published_at': article.get('published_at', ''),
                'first_seen': self.news_first_seen[content_hash]
            }
            
            # ë°ì´í„° ì»¬ë ‰í„°ì— ì „ë‹¬
            if hasattr(self, 'data_collector') and self.data_collector:
                self.data_collector.events_buffer.append(event)
            
            logger.critical(f"ğŸš¨ ë¹„íŠ¸ì½”ì¸ ê¸´ê¸‰ ë‰´ìŠ¤: {event['impact']} - {event['title_ko'][:60]}... (ì˜ˆìƒ: {event['expected_change']})")
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _add_to_news_buffer(self, article: Dict):
        """ë‰´ìŠ¤ ë²„í¼ì— ì¶”ê°€"""
        try:
            # ì¤‘ë³µ ì²´í¬
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return
            
            # ì œëª© ìœ ì‚¬ì„± ì²´í¬
            new_title = article.get('title', '').lower()
            for existing in self.news_buffer:
                if self._is_similar_news(new_title, existing.get('title', '')):
                    return
            
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì²´í¬
            for company in self.important_companies:
                if company.lower() in new_title:
                    bitcoin_keywords = ['bitcoin', 'btc', 'purchase', 'bought']
                    if any(keyword in new_title for keyword in bitcoin_keywords):
                        if self.company_news_count.get(company.lower(), 0) >= 2:  # íšŒì‚¬ë‹¹ ìµœëŒ€ 2ê°œ
                            return
                        self.company_news_count[company.lower()] = self.company_news_count.get(company.lower(), 0) + 1
            
            # ë²„í¼ì— ì¶”ê°€
            self.news_buffer.append(article)
            self.processed_news_hashes.add(content_hash)
            
            # ë²„í¼ í¬ê¸° ê´€ë¦¬ (ìµœëŒ€ 50ê°œ)
            if len(self.news_buffer) > 50:
                # ê°€ì¤‘ì¹˜ì™€ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                self.news_buffer.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
                self.news_buffer = self.news_buffer[:50]
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    async def monitor_reddit(self):
        """Reddit ëª¨ë‹ˆí„°ë§"""
        reddit_subreddits = [
            {'name': 'Bitcoin', 'threshold': 500, 'weight': 8},
            {'name': 'CryptoCurrency', 'threshold': 1000, 'weight': 7},
            {'name': 'BitcoinMarkets', 'threshold': 300, 'weight': 8},
        ]
        
        while True:
            try:
                for sub_info in reddit_subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{sub_info['name']}/hot.json?limit=10"
                        
                        async with self.session.get(url, headers={'User-Agent': 'Bitcoin Monitor Bot 1.0'}) as response:
                            if response.status == 200:
                                data = await response.json()
                                posts = data['data']['children']
                                
                                for post in posts:
                                    post_data = post['data']
                                    
                                    if post_data['ups'] > sub_info['threshold']:
                                        article = {
                                            'title': post_data['title'],
                                            'title_ko': post_data['title'],
                                            'description': post_data.get('selftext', '')[:1400],
                                            'url': f"https://reddit.com{post_data['permalink']}",
                                            'source': f"Reddit r/{sub_info['name']}",
                                            'published_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                                            'upvotes': post_data['ups'],
                                            'weight': sub_info['weight']
                                        }
                                        
                                        if self._is_bitcoin_related(article):
                                            # ê¸°ì—…ëª… ì¶”ì¶œ
                                            company = self._extract_company_from_content(
                                                article['title'],
                                                article.get('description', '')
                                            )
                                            if company:
                                                article['company'] = company
                                            
                                            if self._is_critical_news(article):
                                                if self._should_translate(article):
                                                    article['title_ko'] = await self.translate_text(article['title'])
                                                    summary = await self.summarize_article(
                                                        article['title'],
                                                        article.get('description', '')
                                                    )
                                                    if summary:
                                                        article['summary'] = summary
                                                
                                                if not self._is_duplicate_emergency(article):
                                                    article['expected_change'] = self._estimate_price_impact(article)
                                                    await self._trigger_emergency_alert(article)
                    
                    except Exception as e:
                        logger.warning(f"Reddit ì˜¤ë¥˜ {sub_info['name']}: {str(e)[:50]}")
                
                await asyncio.sleep(600)  # 10ë¶„ë§ˆë‹¤
                
            except Exception as e:
                logger.error(f"Reddit ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(900)
    
    async def aggressive_api_rotation(self):
        """API ìˆœí™˜ ì‚¬ìš©"""
        while True:
            try:
                self._reset_daily_usage()
                
                # NewsAPI
                if self.newsapi_key and self.api_usage['newsapi_today'] < self.api_limits['newsapi']:
                    try:
                        await self._call_newsapi()
                        self.api_usage['newsapi_today'] += 1
                        logger.info(f"âœ… NewsAPI í˜¸ì¶œ ({self.api_usage['newsapi_today']}/{self.api_limits['newsapi']})")
                    except Exception as e:
                        logger.error(f"NewsAPI ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(900)  # 15ë¶„ ëŒ€ê¸°
                
                # NewsData API
                if self.newsdata_key and self.api_usage['newsdata_today'] < self.api_limits['newsdata']:
                    try:
                        await self._call_newsdata()
                        self.api_usage['newsdata_today'] += 1
                        logger.info(f"âœ… NewsData í˜¸ì¶œ ({self.api_usage['newsdata_today']}/{self.api_limits['newsdata']})")
                    except Exception as e:
                        logger.error(f"NewsData ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(900)  # 15ë¶„ ëŒ€ê¸°
                
                # Alpha Vantage
                if self.alpha_vantage_key and self.api_usage['alpha_vantage_today'] < self.api_limits['alpha_vantage']:
                    try:
                        await self._call_alpha_vantage()
                        self.api_usage['alpha_vantage_today'] += 1
                        logger.info(f"âœ… Alpha Vantage í˜¸ì¶œ ({self.api_usage['alpha_vantage_today']}/{self.api_limits['alpha_vantage']})")
                    except Exception as e:
                        logger.error(f"Alpha Vantage ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(1800)  # 30ë¶„ ëŒ€ê¸°
                
            except Exception as e:
                logger.error(f"API ìˆœí™˜ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1800)
    
    async def _parse_rss_feed(self, feed_info: Dict) -> List[Dict]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        articles = []
        try:
            async with self.session.get(
                feed_info['url'], 
                timeout=aiohttp.ClientTimeout(total=8),
                headers={'User-Agent': 'Mozilla/5.0 (compatible; BitcoinNewsBot/1.0)'}
            ) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    if feed.entries:
                        limit = min(15, max(5, feed_info['weight']))
                        
                        for entry in feed.entries[:limit]:
                            try:
                                # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                                pub_time = datetime.now().isoformat()
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    pub_time = datetime(*entry.published_parsed[:6]).isoformat()
                                elif hasattr(entry, 'published'):
                                    try:
                                        from dateutil import parser
                                        pub_time = parser.parse(entry.published).isoformat()
                                    except:
                                        pass
                                
                                article = {
                                    'title': entry.get('title', '').strip(),
                                    'description': entry.get('summary', '').strip()[:1400],  # 1400ì ìœ ì§€
                                    'url': entry.get('link', '').strip(),
                                    'source': feed_info['source'],
                                    'published_at': pub_time,
                                    'weight': feed_info['weight'],
                                    'category': feed_info.get('category', 'unknown')
                                }
                                
                                if article['title'] and article['url']:
                                    articles.append(article)
                                        
                            except Exception as e:
                                logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)[:50]}")
                                continue
        
        except asyncio.TimeoutError:
            logger.debug(f"â° {feed_info['source']}: íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.debug(f"âŒ {feed_info['source']}: {str(e)[:50]}")
        
        return articles
    
    async def _call_newsapi(self):
        """NewsAPI í˜¸ì¶œ"""
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': '(bitcoin OR btc) AND (etf OR sec OR "bought bitcoin" OR "tesla bitcoin" OR "microstrategy bitcoin" OR "bitcoin ban" OR "bitcoin regulation" OR "bitcoin hack" OR "whale alert" OR "fed rate" OR "russia bitcoin" OR "sberbank")',
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.newsapi_key,
                'pageSize': 50,
                'from': (datetime.now() - timedelta(hours=2)).isoformat()
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('articles', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('description', '')[:1400],
                            'url': article.get('url', ''),
                            'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                            'published_at': article.get('publishedAt', ''),
                            'weight': 9,
                            'category': 'api'
                        }
                        
                        if self._is_bitcoin_related(formatted_article):
                            # ê¸°ì—…ëª… ì¶”ì¶œ
                            company = self._extract_company_from_content(
                                formatted_article['title'],
                                formatted_article.get('description', '')
                            )
                            if company:
                                formatted_article['company'] = company
                            
                            if self._should_translate(formatted_article):
                                formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                                if self._is_critical_news(formatted_article):
                                    summary = await self.summarize_article(
                                        formatted_article['title'],
                                        formatted_article.get('description', '')
                                    )
                                    if summary:
                                        formatted_article['summary'] = summary
                            
                            if self._is_critical_news(formatted_article):
                                if not self._is_duplicate_emergency(formatted_article):
                                    formatted_article['expected_change'] = self._estimate_price_impact(formatted_article)
                                    await self._trigger_emergency_alert(formatted_article)
                                processed += 1
                            elif self._is_important_news(formatted_article):
                                await self._add_to_news_buffer(formatted_article)
                                processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° NewsAPI: {processed}ê°œ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"NewsAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsAPI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_newsdata(self):
        """NewsData API í˜¸ì¶œ"""
        try:
            url = "https://newsdata.io/api/1/news"
            params = {
                'apikey': self.newsdata_key,
                'q': 'bitcoin OR btc OR "bitcoin etf" OR "bitcoin regulation" OR "russia bitcoin" OR "sberbank bitcoin"',
                'language': 'en',
                'category': 'business,top',
                'size': 30
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('results', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('description', '')[:1400],
                            'url': article.get('link', ''),
                            'source': f"NewsData ({article.get('source_id', 'Unknown')})",
                            'published_at': article.get('pubDate', ''),
                            'weight': 8,
                            'category': 'api'
                        }
                        
                        if self._is_bitcoin_related(formatted_article):
                            # ê¸°ì—…ëª… ì¶”ì¶œ
                            company = self._extract_company_from_content(
                                formatted_article['title'],
                                formatted_article.get('description', '')
                            )
                            if company:
                                formatted_article['company'] = company
                            
                            if self._should_translate(formatted_article):
                                formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                                if self._is_critical_news(formatted_article):
                                    summary = await self.summarize_article(
                                        formatted_article['title'],
                                        formatted_article.get('description', '')
                                    )
                                    if summary:
                                        formatted_article['summary'] = summary
                            
                            if self._is_critical_news(formatted_article):
                                if not self._is_duplicate_emergency(formatted_article):
                                    formatted_article['expected_change'] = self._estimate_price_impact(formatted_article)
                                    await self._trigger_emergency_alert(formatted_article)
                                processed += 1
                            elif self._is_important_news(formatted_article):
                                await self._add_to_news_buffer(formatted_article)
                                processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° NewsData: {processed}ê°œ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"NewsData ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsData í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_alpha_vantage(self):
        """Alpha Vantage API í˜¸ì¶œ"""
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': 'CRYPTO:BTC',
                'topics': 'financial_markets,technology,earnings',
                'apikey': self.alpha_vantage_key,
                'sort': 'LATEST',
                'limit': 20
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('feed', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('summary', '')[:1400],
                            'url': article.get('url', ''),
                            'source': f"Alpha Vantage ({article.get('source', 'Unknown')})",
                            'published_at': article.get('time_published', ''),
                            'weight': 9,
                            'category': 'api',
                            'sentiment': article.get('overall_sentiment_label', 'Neutral')
                        }
                        
                        if self._is_bitcoin_related(formatted_article):
                            # ê¸°ì—…ëª… ì¶”ì¶œ
                            company = self._extract_company_from_content(
                                formatted_article['title'],
                                formatted_article.get('description', '')
                            )
                            if company:
                                formatted_article['company'] = company
                            
                            if self._should_translate(formatted_article):
                                formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                                if self._is_critical_news(formatted_article):
                                    summary = await self.summarize_article(
                                        formatted_article['title'],
                                        formatted_article.get('description', '')
                                    )
                                    if summary:
                                        formatted_article['summary'] = summary
                            
                            if self._is_critical_news(formatted_article):
                                if not self._is_duplicate_emergency(formatted_article):
                                    formatted_article['expected_change'] = self._estimate_price_impact(formatted_article)
                                    await self._trigger_emergency_alert(formatted_article)
                                processed += 1
                            elif self._is_important_news(formatted_article):
                                await self._add_to_news_buffer(formatted_article)
                                processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° Alpha Vantage: {processed}ê°œ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"Alpha Vantage ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"Alpha Vantage í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    def _reset_daily_usage(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        today = datetime.now().date()
        if today > self.api_usage['last_reset']:
            old_usage = dict(self.api_usage)
            self.api_usage.update({
                'newsapi_today': 0,
                'newsdata_today': 0,
                'alpha_vantage_today': 0,
                'last_reset': today
            })
            self.company_news_count = {}
            self.translation_count = 0
            self.last_translation_reset = datetime.now()
            self.news_first_seen = {}
            logger.info(f"ğŸ”„ ì¼ì¼ ë¦¬ì…‹ ì™„ë£Œ")
    
    async def get_recent_news(self, hours: int = 6) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_news = []
            seen_hashes = set()
            
            for article in self.news_buffer:
                try:
                    # ì‹œê°„ ì²´í¬
                    if article.get('published_at'):
                        pub_time_str = article.get('published_at', '')
                        try:
                            if 'T' in pub_time_str:
                                pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                            else:
                                from dateutil import parser
                                pub_time = parser.parse(pub_time_str)
                            
                            if pub_time > cutoff_time:
                                # ì¤‘ë³µ ì²´í¬
                                content_hash = self._generate_content_hash(article.get('title', ''), '')
                                if content_hash not in seen_hashes:
                                    recent_news.append(article)
                                    seen_hashes.add(content_hash)
                        except:
                            pass
                except:
                    pass
            
            # ì •ë ¬: ê°€ì¤‘ì¹˜ â†’ ì‹œê°„
            recent_news.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
            
            return recent_news[:15]
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self.session:
                await self.session.close()
                logger.info("ğŸ”š ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

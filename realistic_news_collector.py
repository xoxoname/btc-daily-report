import aiohttp
import asyncio
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional, Set
import pytz
from bs4 import BeautifulSoup
import feedparser
import openai
import os
import hashlib
import re

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    def __init__(self, config):
        self.config = config
        self.session = None
        self.news_buffer = []
        self.emergency_alerts_sent = {}  # ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€ìš©
        self.processed_news_hashes = set()  # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ ì €ì¥
        self.news_title_cache = {}  # ì œëª©ë³„ ìºì‹œ
        self.company_news_count = {}  # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸
        self.news_first_seen = {}  # ë‰´ìŠ¤ ìµœì´ˆ ë°œê²¬ ì‹œê°„
        
        # ë²ˆì—­ ìºì‹œ ë° rate limit ê´€ë¦¬ - í•œë„ ì¦ê°€
        self.translation_cache = {}  # ë²ˆì—­ ìºì‹œ
        self.translation_count = 0  # ë²ˆì—­ íšŸìˆ˜ ì¶”ì 
        self.last_translation_reset = datetime.now()
        self.max_translations_per_15min = 150  # 15ë¶„ë‹¹ ìµœëŒ€ ë²ˆì—­ ìˆ˜ (ëŒ€í­ ì¦ê°€)
        self.translation_reset_interval = 900  # 15ë¶„ (ê¸°ì¡´ 30ë¶„ì—ì„œ ë‹¨ì¶•)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë²ˆì—­ìš©)
        self.openai_client = None
        if hasattr(config, 'OPENAI_API_KEY') and config.OPENAI_API_KEY:
            self.openai_client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        
        # ëª¨ë“  API í‚¤ë“¤
        self.newsapi_key = getattr(config, 'NEWSAPI_KEY', None)
        self.newsdata_key = getattr(config, 'NEWSDATA_KEY', None)
        self.alpha_vantage_key = getattr(config, 'ALPHA_VANTAGE_KEY', None)
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ (ì¦‰ì‹œ ì•Œë¦¼ìš©) - ëŒ€í­ í™•ì¥
        self.critical_keywords = [
            # íŠ¸ëŸ¼í”„ ê´€ë ¨ - ëŒ€í­ í™•ì¥
            'trump', 'donald trump', 'president trump', 'trump administration', 'trump says', 'trump announces', 
            'trump declares', 'trump signs', 'trump executive order', 'trump policy', 'trump statement', 
            'trump twitter', 'trump social media', 'trump interview', 'trump speech', 'trump meeting',
            'trump china', 'trump tariff', 'trump trade', 'trump federal', 'trump bitcoin', 'trump crypto',
            'íŠ¸ëŸ¼í”„', 'íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹', 'íŠ¸ëŸ¼í”„ ì •ë¶€', 'íŠ¸ëŸ¼í”„ ë°œì–¸', 'íŠ¸ëŸ¼í”„ ì •ì±…', 'íŠ¸ëŸ¼í”„ í–‰ì •ëª…ë ¹',
            
            # ë¯¸ì¤‘ ë¬´ì—­/ê´€ê³„ - ì‹ ê·œ ì¶”ê°€
            'us china trade', 'china trade war', 'trade talks', 'trade deal', 'trade agreement',
            'china tariff', 'tariff war', 'xi jinping', 'biden china', 'us china relations',
            'trade dispute', 'trade negotiations', 'china exports', 'china imports',
            'ë¯¸ì¤‘ ë¬´ì—­', 'ë¬´ì—­ ì „ìŸ', 'ë¬´ì—­ í˜‘ìƒ', 'ê´€ì„¸', 'ì‹œì§„í•‘', 'ì¤‘êµ­ ë¬´ì—­',
            
            # ì—°ì¤€/ê¸ˆë¦¬ ê´€ë ¨ - í™•ì¥
            'fed rate decision', 'fed raises', 'fed cuts', 'powell says', 'fomc decides', 'fed meeting',
            'interest rate hike', 'interest rate cut', 'monetary policy', 'federal reserve',
            'jerome powell', 'fed chair', 'fed statement', 'fed minutes', 'fed policy',
            'rate decision', 'rate hike', 'rate cut', 'inflation data', 'cpi data', 'ppi data',
            'ì—°ì¤€', 'ì—°ë°©ì¤€ë¹„ì œë„', 'FOMC', 'íŒŒì›”', 'ì œë¡¬ íŒŒì›”', 'ê¸ˆë¦¬ ì¸ìƒ', 'ê¸ˆë¦¬ ì¸í•˜', 'ê¸ˆë¦¬ ê²°ì •',
            
            # ê²½ì œ ì§€í‘œ - ì‹ ê·œ ì¶”ê°€
            'gdp growth', 'unemployment rate', 'jobs report', 'nonfarm payrolls', 'retail sales',
            'consumer confidence', 'manufacturing pmi', 'inflation rate', 'consumer price index',
            'producer price index', 'housing data', 'durable goods', 'trade balance',
            
            # SEC/ê·œì œ ê´€ë ¨ - í™•ì¥
            'sec lawsuit bitcoin', 'sec sues', 'sec enforcement', 'sec charges bitcoin',
            'sec approves', 'sec rejects', 'sec bitcoin etf', 'gary gensler', 'sec chair',
            'cftc bitcoin', 'cftc crypto', 'regulatory approval', 'regulatory rejection',
            'SEC', 'CFTC', 'ê²Œë¦¬ ê²ìŠ¬ëŸ¬', 'SEC ì†Œì†¡', 'SEC ê·œì œ', 'SEC ë¹„íŠ¸ì½”ì¸', 'SEC ìŠ¹ì¸', 'SEC ê±°ë¶€',
            
            # ê·œì œ/ê¸ˆì§€ ê´€ë ¨ - í™•ì¥
            'china bans bitcoin', 'china crypto ban', 'government bans crypto', 'regulatory ban',
            'court blocks', 'federal court', 'supreme court crypto', 'legal ruling',
            'regulatory crackdown', 'crypto regulation', 'digital asset regulation',
            'ì¤‘êµ­ ë¹„íŠ¸ì½”ì¸ ê¸ˆì§€', 'ì •ë¶€ ê·œì œ', 'ì•”í˜¸í™”í ê¸ˆì§€', 'ë²•ì› íŒê²°', 'ê·œì œ ë‹¹êµ­',
            
            # ì‹œì¥ ê¸‰ë³€ë™ - í™•ì¥
            'bitcoin crash', 'crypto crash', 'market crash', 'flash crash', 'bitcoin plunge',
            'bitcoin surge', 'bitcoin rally', 'bitcoin breaks', 'bitcoin soars', 'bitcoin tumbles',
            'market meltdown', 'sell-off', 'massive liquidation', 'whale move', 'whale alert',
            'ë¹„íŠ¸ì½”ì¸ í­ë½', 'ì•”í˜¸í™”í ê¸‰ë½', 'ì‹œì¥ ë¶•ê´´', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±', 'ëŒ€ëŸ‰ ì²­ì‚°',
            
            # ETF ê´€ë ¨ - í™•ì¥  
            'bitcoin etf approved', 'bitcoin etf rejected', 'etf decision', 'etf filing',
            'spot bitcoin etf', 'bitcoin etf launch', 'etf flows', 'etf inflows', 'etf outflows',
            'blackrock etf', 'fidelity etf', 'grayscale etf', 'ark etf',
            'ETF ìŠ¹ì¸', 'ETF ê±°ë¶€', 'ETF ê²°ì •', 'í˜„ë¬¼ ETF', 'ë¹„íŠ¸ì½”ì¸ ETF',
            
            # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ - í™•ì¥
            'bought bitcoin', 'buys bitcoin', 'purchased bitcoin', 'bitcoin purchase', 'bitcoin acquisition',
            'tesla bitcoin', 'microstrategy bitcoin', 'square bitcoin', 'paypal bitcoin',
            'gamestop bitcoin', 'gme bitcoin', '$gme bitcoin', 'metaplanet bitcoin',
            'corporate bitcoin', 'institutional bitcoin', 'treasury bitcoin',
            'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë¹„íŠ¸ì½”ì¸ ë§¤ì…', 'ë¹„íŠ¸ì½”ì¸ íˆ¬ì', 'ë¹„íŠ¸ì½”ì¸ ë³´ìœ ', 'ê¸°ì—… ë¹„íŠ¸ì½”ì¸',
            
            # ëŒ€ëŸ‰ ê±°ë˜/ì´ë™ - í™•ì¥
            'whale alert', 'large bitcoin transfer', 'bitcoin moved', 'btc transferred',
            'exchange inflow', 'exchange outflow', 'massive transfer', 'billion dollar move',
            'cold wallet', 'hot wallet', 'wallet movement', 'address activity',
            'ê³ ë˜ ì´ë™', 'ëŒ€ëŸ‰ ì´ì²´', 'ë¹„íŠ¸ì½”ì¸ ì´ë™', 'ê±°ë˜ì†Œ ìœ ì…', 'ê±°ë˜ì†Œ ìœ ì¶œ', 'ì§€ê°‘ ì´ë™',
            
            # í•´í‚¹/ë³´ì•ˆ - í™•ì¥
            'exchange hacked', 'bitcoin stolen', 'crypto hack', 'security breach',
            'wallet compromised', 'private key stolen', 'smart contract exploit',
            'defi hack', 'bridge hack', 'cross-chain hack',
            'ê±°ë˜ì†Œ í•´í‚¹', 'ë¹„íŠ¸ì½”ì¸ ë„ë‚œ', 'ë³´ì•ˆ ì‚¬ê³ ', 'ì§€ê°‘ í•´í‚¹', 'ìŠ¤ë§ˆíŠ¸ ì»¨íŠ¸ë™íŠ¸ í•´í‚¹',
            
            # ê¸€ë¡œë²Œ ê²½ì œ/ì •ì¹˜ - ì‹ ê·œ ëŒ€í­ ì¶”ê°€
            'war', 'military action', 'geopolitical', 'sanctions', 'embargo',
            'energy crisis', 'oil price surge', 'oil price crash', 'opec decision',
            'bank crisis', 'banking system', 'financial crisis', 'recession warning',
            'stock market crash', 'dow jones crash', 'nasdaq crash', 's&p 500 crash',
            'dollar strength', 'dollar weakness', 'currency crisis', 'inflation shock',
            'ì „ìŸ', 'ì§€ì •í•™ì ', 'ì œì¬', 'ìœ ê°€', 'ì˜¤ì¼ì‡¼í¬', 'ê¸ˆìœµìœ„ê¸°', 'ê²½ê¸°ì¹¨ì²´', 'ë‹¬ëŸ¬',
            
            # ì¤‘ì•™ì€í–‰ ë””ì§€í„¸í™”í - ì‹ ê·œ ì¶”ê°€
            'cbdc', 'digital dollar', 'digital yuan', 'central bank digital currency',
            'fed digital currency', 'china digital currency', 'digital currency pilot',
            'ì¤‘ì•™ì€í–‰ ë””ì§€í„¸í™”í', 'ë””ì§€í„¸ ë‹¬ëŸ¬', 'ë””ì§€í„¸ ìœ„ì•ˆ',
            
            # ê¸°ìˆ /ì±„êµ´ ê´€ë ¨ - í™•ì¥
            'bitcoin mining ban', 'mining crackdown', 'hash rate', 'mining difficulty',
            'energy consumption', 'carbon footprint', 'proof of stake', 'ethereum merge',
            'ë¹„íŠ¸ì½”ì¸ ì±„êµ´', 'ì±„êµ´ ê¸ˆì§€', 'í•´ì‹œë ˆì´íŠ¸', 'ì—ë„ˆì§€ ì†Œë¹„',
        ]
        
        # ì œì™¸ í‚¤ì›Œë“œ (ë¹„íŠ¸ì½”ì¸ê³¼ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ê²ƒë“¤) - ì¶•ì†Œí•˜ì—¬ ë” ë§ì€ ë‰´ìŠ¤ í¬í•¨
        self.exclude_keywords = [
            'how to mine', 'ì§‘ì—ì„œ ì±„êµ´', 'mining at home',
            'price prediction tutorial', 'ê°€ê²© ì˜ˆì¸¡ ë°©ë²•'
        ]
        
        # ì¤‘ìš” ê¸°ì—… ë¦¬ìŠ¤íŠ¸ - í™•ì¥
        self.important_companies = [
            'tesla', 'microstrategy', 'square', 'block', 'paypal', 'mastercard', 'visa',
            'apple', 'google', 'amazon', 'meta', 'facebook', 'microsoft', 'netflix',
            'gamestop', 'gme', 'amc', 'blackrock', 'fidelity', 'jpmorgan', 'goldman',
            'morgan stanley', 'bank of america', 'wells fargo', 'citigroup',
            'samsung', 'lg', 'sk', 'kakao', 'naver', 'ì‚¼ì„±', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„',
            'metaplanet', 'ë©”íƒ€í”Œë˜ë‹›', 'coinbase', 'binance', 'ftx', 'kraken'
        ]
        
        # RSS í”¼ë“œ - ë” ë¹ ë¥¸ ì†ŒìŠ¤ ì¶”ê°€
        self.rss_feeds = [
            # ì‹¤ì‹œê°„ ë‰´ìŠ¤ (ìµœìš°ì„ ) - ì‹ ê·œ ì¶”ê°€
            {'url': 'https://feeds.reuters.com/reuters/businessNews', 'source': 'Reuters Business', 'weight': 10, 'category': 'news'},
            {'url': 'https://feeds.reuters.com/Reuters/worldNews', 'source': 'Reuters World', 'weight': 10, 'category': 'news'},
            {'url': 'http://feeds.feedburner.com/ap/business', 'source': 'AP Business', 'weight': 10, 'category': 'news'},
            {'url': 'https://feeds.bloomberg.com/politics/news.rss', 'source': 'Bloomberg Politics', 'weight': 10, 'category': 'news'},
            {'url': 'https://feeds.bloomberg.com/economics/news.rss', 'source': 'Bloomberg Economics', 'weight': 10, 'category': 'news'},
            
            # ì•”í˜¸í™”í ì „ë¬¸ (ìµœìš°ì„ )
            {'url': 'https://cointelegraph.com/rss', 'source': 'Cointelegraph', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'source': 'CoinDesk', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://decrypt.co/feed', 'source': 'Decrypt', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://bitcoinmagazine.com/.rss/full/', 'source': 'Bitcoin Magazine', 'weight': 9, 'category': 'crypto'},
            
            # ìƒˆë¡œìš´ ì•”í˜¸í™”í ì†ŒìŠ¤
            {'url': 'https://ambcrypto.com/feed/', 'source': 'AMBCrypto', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://cryptopotato.com/feed/', 'source': 'CryptoPotato', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://u.today/rss', 'source': 'U.Today', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://cryptonews.com/news/feed/', 'source': 'Cryptonews', 'weight': 8, 'category': 'crypto'},
            
            # ì¼ë°˜ ê¸ˆìœµ - ë¹ ë¥¸ ì†ŒìŠ¤ ìš°ì„ 
            {'url': 'https://feeds.bloomberg.com/markets/news.rss', 'source': 'Bloomberg Markets', 'weight': 9, 'category': 'finance'},
            {'url': 'https://www.marketwatch.com/rss/topstories', 'source': 'MarketWatch', 'weight': 8, 'category': 'finance'},
            {'url': 'https://seekingalpha.com/feed.xml', 'source': 'Seeking Alpha', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.feedburner.com/InvestingcomAnalysis', 'source': 'Investing.com', 'weight': 8, 'category': 'finance'},
            {'url': 'https://www.fool.com/feeds/index.aspx', 'source': 'Motley Fool', 'weight': 7, 'category': 'finance'},
            
            # ì •ì¹˜/ì •ì±… ë‰´ìŠ¤ - ì‹ ê·œ ì¶”ê°€
            {'url': 'https://feeds.washingtonpost.com/rss/politics', 'source': 'Washington Post Politics', 'weight': 9, 'category': 'politics'},
            {'url': 'https://feeds.npr.org/1014/rss.xml', 'source': 'NPR Politics', 'weight': 8, 'category': 'politics'},
            {'url': 'https://feeds.cnn.com/rss/edition_politics.rss', 'source': 'CNN Politics', 'weight': 8, 'category': 'politics'},
            
            # ì¼ë°˜ ë‰´ìŠ¤ (í™•ì‹¤í•œ ê²ƒë“¤)
            {'url': 'https://rss.cnn.com/rss/edition.rss', 'source': 'CNN World', 'weight': 8, 'category': 'news'},
            {'url': 'http://feeds.bbci.co.uk/news/business/rss.xml', 'source': 'BBC Business', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.npr.org/1001/rss.xml', 'source': 'NPR News', 'weight': 7, 'category': 'news'},
            
            # í…Œí¬/ë¹„ì¦ˆë‹ˆìŠ¤
            {'url': 'https://techcrunch.com/feed/', 'source': 'TechCrunch', 'weight': 7, 'category': 'tech'},
            {'url': 'https://www.wired.com/feed/rss', 'source': 'Wired', 'weight': 6, 'category': 'tech'},
            {'url': 'https://feeds.feedburner.com/venturebeat/SZYF', 'source': 'VentureBeat', 'weight': 7, 'category': 'tech'},
        ]
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì  - ë” ìì£¼ ì‚¬ìš©
        self.api_usage = {
            'newsapi_today': 0,
            'newsdata_today': 0,
            'alpha_vantage_today': 0,
            'last_reset': datetime.now().date()
        }
        
        # API ì¼ì¼ í•œë„ - ì¦ê°€
        self.api_limits = {
            'newsapi': 25,  # 15 â†’ 25
            'newsdata': 15,  # 8 â†’ 15
            'alpha_vantage': 3  # 1 â†’ 3
        }
        
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ - API í‚¤ ìƒíƒœ: NewsAPI={bool(self.newsapi_key)}, NewsData={bool(self.newsdata_key)}, AlphaVantage={bool(self.alpha_vantage_key)}")
        logger.info(f"ğŸ“Š ê°œì„ ëœ ì„¤ì •: RSS 15ì´ˆ ì²´í¬, ë²ˆì—­ 15ë¶„ë‹¹ {self.max_translations_per_15min}ê°œ, í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ {len(self.critical_keywords)}ê°œ")
    
    def _reset_translation_count_if_needed(self):
        """í•„ìš”ì‹œ ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ - 15ë¶„ë§ˆë‹¤"""
        now = datetime.now()
        if (now - self.last_translation_reset).total_seconds() > self.translation_reset_interval:
            old_count = self.translation_count
            self.translation_count = 0
            self.last_translation_reset = now
            logger.info(f"ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {old_count} â†’ 0 (15ë¶„ ê²½ê³¼)")
    
    def _should_translate(self, article: Dict) -> bool:
        """ë‰´ìŠ¤ë¥¼ ë²ˆì—­í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ - ìš°ì„ ìˆœìœ„ ì¡°ì •"""
        # ì´ë¯¸ í•œê¸€ ì œëª©ì´ ìˆìœ¼ë©´ ë²ˆì—­ ë¶ˆí•„ìš”
        if article.get('title_ko') and article['title_ko'] != article.get('title', ''):
            return False
        
        # ë²ˆì—­ ìš°ì„ ìˆœìœ„ ê²°ì •
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # 1ìˆœìœ„: í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ëŠ” í•­ìƒ ë²ˆì—­
        if self._is_critical_news(article):
            return True
        
        # 2ìˆœìœ„: ì •ì¹˜/ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ + ë†’ì€ ê°€ì¤‘ì¹˜ (íŠ¸ëŸ¼í”„, ë¯¸ì¤‘ ë¬´ì—­ ë“±)
        if category in ['politics', 'news'] and weight >= 8:
            return True
        
        # 3ìˆœìœ„: ì¤‘ìš” ë‰´ìŠ¤ + ë†’ì€ ê°€ì¤‘ì¹˜
        if self._is_important_news(article) and weight >= 8:
            return True
        
        # 4ìˆœìœ„: ì•”í˜¸í™”í ì¹´í…Œê³ ë¦¬ + ì¤‘ìš” ë‰´ìŠ¤
        if category == 'crypto' and self._is_important_news(article):
            return True
        
        # 5ìˆœìœ„: API ë‰´ìŠ¤ (NewsAPI, NewsData ë“±)
        if category == 'api' and weight >= 9:
            return True
        
        # ë‚˜ë¨¸ì§€ëŠ” ë²ˆì—­ í•˜ì§€ ì•ŠìŒ
        return False
    
    async def translate_text(self, text: str, max_length: int = 100) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ (Rate limit ì²˜ë¦¬ í¬í•¨)"""
        if not self.openai_client:
            return text
        
        # ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ì²´í¬
        self._reset_translation_count_if_needed()
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]
        
        # Rate limit ì²´í¬
        if self.translation_count >= self.max_translations_per_15min:
            logger.warning(f"ë²ˆì—­ í•œë„ ì´ˆê³¼: {self.translation_count}/{self.max_translations_per_15min} (15ë¶„)")
            return text[:max_length] + "..." if len(text) > max_length else text
        
        try:
            # ê¸¸ì´ ì œí•œ
            if len(text) > max_length:
                text = text[:max_length] + "..."
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional translator. Translate the following text to Korean in a natural and easy-to-understand way. Keep it concise and under 80 characters. If it's about cryptocurrency scams or hacks, make sure to clearly distinguish between 'losses decreasing' (positive) and 'scam amounts' (negative)."},
                    {"role": "user", "content": text}
                ],
                max_tokens=150,
                temperature=0.3
            )
            
            translated = response.choices[0].message.content.strip()
            # ë²ˆì—­ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(translated) > 80:
                translated = translated[:77] + "..."
            
            # ìºì‹œ ì €ì¥ ë° ì¹´ìš´íŠ¸ ì¦ê°€
            self.translation_cache[cache_key] = translated
            self.translation_count += 1
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.translation_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ 500ê°œ ì œê±°
                keys_to_remove = list(self.translation_cache.keys())[:500]
                for key in keys_to_remove:
                    del self.translation_cache[key]
            
            return translated
            
        except openai.RateLimitError as e:
            logger.warning(f"OpenAI Rate limit ì˜¤ë¥˜: {str(e)}")
            self.translation_count = self.max_translations_per_15min  # ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•Šë„ë¡
            return text[:80] + "..." if len(text) > 80 else text
        except Exception as e:
            logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:50]}")
            return text[:80] + "..." if len(text) > 80 else text
    
    def _generate_content_hash(self, title: str, description: str = "") -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì˜ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©) - ê°•í™”ëœ ë²„ì „"""
        # ì œëª©ì—ì„œ ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ì—¬ ìœ ì‚¬í•œ ë‰´ìŠ¤ ê°ì§€
        clean_title = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title.lower())
        clean_title = re.sub(r'\s+', ' ', clean_title).strip()
        
        # íšŒì‚¬ëª…ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        companies = []
        keywords = []
        
        for company in self.important_companies:
            if company.lower() in clean_title.lower():
                companies.append(company.lower())
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        key_terms = ['bitcoin', 'btc', 'purchase', 'bought', 'buys', 'acquisition', 'êµ¬ë§¤', 'ë§¤ì…', 'first', 'ì²«', 'trump', 'china', 'trade']
        for term in key_terms:
            if term in clean_title.lower():
                keywords.append(term)
        
        # íšŒì‚¬ëª… + í•µì‹¬ í‚¤ì›Œë“œë¡œ í•´ì‹œ ìƒì„±
        if companies and keywords:
            # íšŒì‚¬ë³„ë¡œ í•˜ë‚˜ì˜ í•´ì‹œë§Œ ìƒì„± (ìˆ«ì ë¬´ì‹œ)
            hash_content = f"{','.join(sorted(set(companies)))}_{','.join(sorted(set(keywords)))}"
        else:
            # ì¼ë°˜ ë‰´ìŠ¤ëŠ” ì „ì²´ ë‚´ìš©ìœ¼ë¡œ í•´ì‹œ
            hash_content = clean_title
        
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def _is_duplicate_emergency(self, article: Dict, time_window: int = 30) -> bool:
        """ê¸´ê¸‰ ì•Œë¦¼ì´ ì¤‘ë³µì¸ì§€ í™•ì¸ (30ë¶„ ì´ë‚´ ìœ ì‚¬ ë‚´ìš©) - ì‹œê°„ ë‹¨ì¶•"""
        try:
            current_time = datetime.now()
            content_hash = self._generate_content_hash(
                article.get('title', ''), 
                article.get('description', '')
            )
            
            # ì‹œê°„ì´ ì§€ë‚œ ì•Œë¦¼ ì œê±°
            cutoff_time = current_time - timedelta(minutes=time_window)
            self.emergency_alerts_sent = {
                k: v for k, v in self.emergency_alerts_sent.items()
                if v > cutoff_time
            }
            
            # ì¤‘ë³µ ì²´í¬
            if content_hash in self.emergency_alerts_sent:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€: {article.get('title', '')[:50]}...")
                return True
            
            # ìƒˆë¡œìš´ ì•Œë¦¼ ê¸°ë¡
            self.emergency_alerts_sent[content_hash] = current_time
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _is_similar_news(self, title1: str, title2: str) -> bool:
        """ë‘ ë‰´ìŠ¤ ì œëª©ì´ ìœ ì‚¬í•œì§€ í™•ì¸ - ë” ì—„ê²©í•œ ê¸°ì¤€"""
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean1 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title1.lower())
        clean2 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title2.lower())
        
        clean1 = re.sub(r'\s+', ' ', clean1).strip()
        clean2 = re.sub(r'\s+', ' ', clean2).strip()
        
        # íŠ¹ì • íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ ë‰´ìŠ¤ì¸ì§€ ì²´í¬
        for company in self.important_companies:
            company_lower = company.lower()
            if company_lower in clean1 and company_lower in clean2:
                # ê°™ì€ íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ë©´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
                bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                if any(keyword in clean1 for keyword in bitcoin_keywords) and \
                   any(keyword in clean2 for keyword in bitcoin_keywords):
                    return True
        
        # ë‹¨ì–´ ì§‘í•© ë¹„êµ
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        if not words1 or not words2:
            return False
        
        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        if union == 0:
            return False
        
        similarity = intersection / union
        
        # 65% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        return similarity > 0.65
    
    def _is_recent_news(self, article: Dict, hours: int = 1) -> bool:
        """ë‰´ìŠ¤ê°€ ìµœê·¼ ê²ƒì¸ì§€ í™•ì¸ - 1ì‹œê°„ ë‚´ë¡œ ë” ì—„ê²©"""
        try:
            pub_time_str = article.get('published_at', '')
            if not pub_time_str:
                return True  # ì‹œê°„ ì •ë³´ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨
            
            # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
            try:
                if 'T' in pub_time_str:
                    pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                else:
                    from dateutil import parser
                    pub_time = parser.parse(pub_time_str)
                
                # UTC to local time if needed
                if pub_time.tzinfo is None:
                    pub_time = pytz.UTC.localize(pub_time)
                
                time_diff = datetime.now(pytz.UTC) - pub_time
                return time_diff.total_seconds() < (hours * 3600)
            except:
                return True  # íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨
        except:
            return True
    
    async def start_monitoring(self):
        """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - ì†ë„ ìµœì í™”"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10),  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• 15â†’10ì´ˆ
                connector=aiohttp.TCPConnector(limit=150, limit_per_host=50)  # ì—°ê²°ìˆ˜ ì¦ê°€
            )
        
        logger.info("ğŸ” ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - ì´ˆê³ ì† RSS + ì ê·¹ì  API ì‚¬ìš©")
        logger.info(f"ğŸ“Š ì„¤ì •: RSS 15ì´ˆ ì²´í¬, ë²ˆì—­ 15ë¶„ë‹¹ ìµœëŒ€ {self.max_translations_per_15min}ê°œ, í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ {len(self.critical_keywords)}ê°œ")
        
        # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        self.company_news_count = {}
        
        tasks = [
            self.monitor_rss_feeds(),      # ë©”ì¸: RSS (15ì´ˆë§ˆë‹¤)
            self.monitor_reddit(),         # ë³´ì¡°: Reddit (10ë¶„ë§ˆë‹¤)
            self.aggressive_api_rotation() # ì ê·¹ì : API ìˆœí™˜ ì‚¬ìš© (ë” ìì£¼)
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_rss_feeds(self):
        """RSS í”¼ë“œ ëª¨ë‹ˆí„°ë§ - 15ì´ˆë§ˆë‹¤ ì´ˆê³ ì† ì²´í¬"""
        while True:
            try:
                # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì†ŒìŠ¤ë¶€í„° ì²˜ë¦¬
                sorted_feeds = sorted(self.rss_feeds, key=lambda x: x['weight'], reverse=True)
                successful_feeds = 0
                processed_articles = 0
                
                for feed_info in sorted_feeds:
                    try:
                        articles = await self._parse_rss_feed(feed_info)
                        
                        if articles:  # ì„±ê³µì ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¨ ê²½ìš°
                            successful_feeds += 1
                            
                            for article in articles:
                                # ìµœì‹  ë‰´ìŠ¤ë§Œ ì²˜ë¦¬ (1ì‹œê°„ ì´ë‚´ë¡œ ë‹¨ì¶•)
                                if not self._is_recent_news(article, hours=1):
                                    continue
                                
                                # ë²ˆì—­ í•„ìš” ì—¬ë¶€ ì²´í¬ (ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒë§Œ)
                                if self.openai_client and self._should_translate(article):
                                    article['title_ko'] = await self.translate_text(article['title'])
                                else:
                                    article['title_ko'] = article.get('title', '')
                                
                                # ê°€ì¤‘ì¹˜ 8 ì´ìƒì´ê±°ë‚˜ ì •ì¹˜/ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ëŠ” í¬ë¦¬í‹°ì»¬ ì²´í¬
                                if feed_info['weight'] >= 8 or feed_info['category'] in ['politics', 'news']:
                                    if self._is_critical_news(article):
                                        # ì¤‘ë³µ ì²´í¬ í›„ ì•Œë¦¼
                                        if not self._is_duplicate_emergency(article):
                                            # ë³€ë™ ì˜ˆìƒë¥  ì¶”ê°€
                                            article['expected_change'] = self._estimate_price_impact(article)
                                            await self._trigger_emergency_alert(article)
                                            processed_articles += 1
                                
                                # ëª¨ë“  RSSëŠ” ì¤‘ìš” ë‰´ìŠ¤ ì²´í¬
                                if self._is_important_news(article):
                                    await self._add_to_news_buffer(article)
                                    processed_articles += 1
                    
                    except Exception as e:
                        logger.warning(f"RSS í”¼ë“œ ì¼ì‹œ ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:50]}")
                        continue
                
                logger.info(f"ğŸ“° RSS ìŠ¤ìº” ì™„ë£Œ: {successful_feeds}/{len(sorted_feeds)} í”¼ë“œ ì„±ê³µ, {processed_articles}ê°œ ì²˜ë¦¬ (ë²ˆì—­: {self.translation_count}/{self.max_translations_per_15min})")
                await asyncio.sleep(15)  # 15ì´ˆë§ˆë‹¤ ì „ì²´ RSS ì²´í¬
                
            except Exception as e:
                logger.error(f"RSS ëª¨ë‹ˆí„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)
    
    def _estimate_price_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ì˜ ì˜ˆìƒ ê°€ê²© ì˜í–¥ ì¶”ì • - íŠ¸ëŸ¼í”„/ì •ì¹˜ ì´ë²¤íŠ¸ ê°•í™”"""
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        impact = article.get('impact', '')
        
        # íŠ¸ëŸ¼í”„ ê´€ë ¨ - ê°•í™”ëœ í‰ê°€
        if 'trump' in content:
            if any(word in content for word in ['china', 'trade', 'tariff']):
                return 'Â±2~5%'  # ë¯¸ì¤‘ ë¬´ì—­ ê´€ë ¨
            elif any(word in content for word in ['bitcoin', 'crypto', 'digital asset']):
                return '+1~3%'  # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰
            elif any(word in content for word in ['executive order', 'policy', 'announce']):
                return 'Â±1~3%'  # ì •ì±… ë°œí‘œ
            else:
                return 'Â±0.5~2%'  # ì¼ë°˜ íŠ¸ëŸ¼í”„ ë‰´ìŠ¤
        
        # ë¯¸ì¤‘ ë¬´ì—­/ê´€ê³„ - ì‹ ê·œ ì¶”ê°€
        if any(word in content for word in ['us china trade', 'trade war', 'china tariff', 'xi jinping']):
            return 'Â±1~4%'
        
        # Fed/ê¸ˆë¦¬ ê´€ë ¨ - ê°•í™”
        if any(word in content for word in ['fed rate', 'powell', 'fomc', 'interest rate']):
            if any(word in content for word in ['hike', 'raise', 'increase']):
                return '-1~3%'  # ê¸ˆë¦¬ ì¸ìƒ
            elif any(word in content for word in ['cut', 'lower', 'decrease']):
                return '+2~5%'  # ê¸ˆë¦¬ ì¸í•˜
            else:
                return 'Â±1~2%'  # ì¼ë°˜ Fed ë‰´ìŠ¤
        
        # ê²½ì œ ì§€í‘œ - ì‹ ê·œ ì¶”ê°€
        if any(word in content for word in ['gdp', 'unemployment', 'inflation', 'cpi', 'ppi']):
            return 'Â±0.5~2%'
        
        # ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ - ì‹ ê·œ ì¶”ê°€
        if any(word in content for word in ['war', 'military', 'sanctions', 'geopolitical']):
            return 'Â±2~7%'  # ë†’ì€ ë³€ë™ì„±
        
        # ë¹„íŠ¸ì½”ì¸ ìš°ì„¸/ë„ë¯¸ë„ŒìŠ¤ ê´€ë ¨ - ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
        if any(word in content for word in ['dominance', 'ìš°ì„¸', 'ì ìœ ìœ¨']):
            return 'Â±0.5%'  # ì´ë¯¸ ë°˜ì˜ëœ ì›€ì§ì„
        
        # ì‚¬ê¸°/í•´í‚¹ ê´€ë ¨ - êµ¬ë¶„í•´ì„œ ì²˜ë¦¬
        if any(word in content for word in ['scam', 'fraud', 'hack', 'ì‚¬ê¸°', 'í•´í‚¹']):
            if 'decrease' in content or 'ê°ì†Œ' in content:
                return 'Â±0.3%'  # ë³´ì•ˆ ê°œì„ ì€ ê°„ì ‘ì  í˜¸ì¬
            else:
                return '-0.3~0.5%'  # íˆ¬ì ì‹¬ë¦¬ ìœ„ì¶•
        
        # í‚¤ì›Œë“œë³„ ì˜ˆìƒ ë³€ë™ë¥  (ë” í˜„ì‹¤ì ìœ¼ë¡œ)
        strong_bullish_keywords = {
            'etf approved': '+2~4%',  # ê¸°ì¡´ +5~10%ì—ì„œ í•˜í–¥
            'bought bitcoin': '+0.5~1.5%',  # ê¸°ì¡´ +2~5%ì—ì„œ í•˜í–¥
            'bitcoin purchase': '+0.5~1.5%',
            'adoption': '+1~2%',  # ê¸°ì¡´ +3~7%ì—ì„œ í•˜í–¥
            'all-time high': '+2~5%',  # ê¸°ì¡´ +5~15%ì—ì„œ í•˜í–¥
            'institutional': '+0.5~1%'  # ê¸°ì¡´ +2~4%ì—ì„œ í•˜í–¥
        }
        
        strong_bearish_keywords = {
            'ban': '-2~5%',  # ê¸°ì¡´ -5~10%ì—ì„œ í•˜í–¥
            'lawsuit': '-1~3%',  # ê¸°ì¡´ -3~7%ì—ì„œ í•˜í–¥
            'hack': '-2~4%',  # ê¸°ì¡´ -5~8%ì—ì„œ í•˜í–¥
            'crash': '-5~10%',  # ê¸°ì¡´ -10~20%ì—ì„œ í•˜í–¥
            'reject': '-1~2%',  # ê¸°ì¡´ -3~5%ì—ì„œ í•˜í–¥
            'crackdown': '-2~4%'  # ê¸°ì¡´ -5~10%ì—ì„œ í•˜í–¥
        }
        
        moderate_keywords = {
            'concern': 'Â±0.5~1%',  # ê¸°ì¡´ Â±1~3%ì—ì„œ í•˜í–¥
            'uncertainty': 'Â±1~2%',  # ê¸°ì¡´ Â±2~4%ì—ì„œ í•˜í–¥
            'volatility': 'Â±1~3%',  # ê¸°ì¡´ Â±3~5%ì—ì„œ í•˜í–¥
            'meeting': 'Â±0.3~0.5%',  # ê¸°ì¡´ Â±1~2%ì—ì„œ í•˜í–¥
            'discussion': 'Â±0.3~0.5%'  # ê¸°ì¡´ Â±1~2%ì—ì„œ í•˜í–¥
        }
        
        # ì˜ˆìƒ ë³€ë™ë¥  ê²°ì •
        for keyword, change in strong_bullish_keywords.items():
            if keyword in content:
                return change
        
        for keyword, change in strong_bearish_keywords.items():
            if keyword in content:
                return change
        
        for keyword, change in moderate_keywords.items():
            if keyword in content:
                return change
        
        # ê¸°ë³¸ê°’ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
        if 'í˜¸ì¬' in impact:
            return '+0.3~1%'  # ê¸°ì¡´ +1~3%ì—ì„œ í•˜í–¥
        elif 'ì•…ì¬' in impact:
            return '-0.3~1%'  # ê¸°ì¡´ -1~3%ì—ì„œ í•˜í–¥
        else:
            return 'Â±0.3%'  # ê¸°ì¡´ Â±1~2%ì—ì„œ í•˜í–¥
    
    async def monitor_reddit(self):
        """Reddit ëª¨ë‹ˆí„°ë§"""
        reddit_subreddits = [
            {'name': 'Bitcoin', 'threshold': 200, 'weight': 8},
            {'name': 'CryptoCurrency', 'threshold': 400, 'weight': 7},
            {'name': 'investing', 'threshold': 800, 'weight': 6},
            {'name': 'wallstreetbets', 'threshold': 2000, 'weight': 5},
            {'name': 'politics', 'threshold': 1000, 'weight': 6},  # ì‹ ê·œ ì¶”ê°€
            {'name': 'worldnews', 'threshold': 1500, 'weight': 6}  # ì‹ ê·œ ì¶”ê°€
        ]
        
        while True:
            try:
                successful_subs = 0
                
                for sub_info in reddit_subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{sub_info['name']}/hot.json?limit=20"
                        
                        async with self.session.get(url, headers={'User-Agent': 'Bitcoin Monitor Bot 1.0'}) as response:
                            if response.status == 200:
                                data = await response.json()
                                posts = data['data']['children']
                                successful_subs += 1
                                
                                relevant_posts = 0
                                for post in posts:
                                    post_data = post['data']
                                    
                                    if post_data['ups'] > sub_info['threshold']:
                                        article = {
                                            'title': post_data['title'],
                                            'title_ko': post_data['title'],  # Redditì€ ê¸°ë³¸ì ìœ¼ë¡œ ë²ˆì—­ ìƒëµ
                                            'description': post_data.get('selftext', '')[:200],
                                            'url': f"https://reddit.com{post_data['permalink']}",
                                            'source': f"Reddit r/{sub_info['name']}",
                                            'published_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                                            'upvotes': post_data['ups'],
                                            'weight': sub_info['weight']
                                        }
                                        
                                        # Redditë„ ì¤‘ìš”ë„ì— ë”°ë¼ ë²ˆì—­
                                        if self._is_critical_news(article) and self.openai_client and self._should_translate(article):
                                            article['title_ko'] = await self.translate_text(article['title'])
                                        
                                        if self._is_critical_news(article):
                                            if not self._is_duplicate_emergency(article):
                                                article['expected_change'] = self._estimate_price_impact(article)
                                                await self._trigger_emergency_alert(article)
                                            relevant_posts += 1
                                        elif self._is_important_news(article):
                                            await self._add_to_news_buffer(article)
                                            relevant_posts += 1
                                
                                if relevant_posts > 0:
                                    logger.info(f"ğŸ“± Reddit r/{sub_info['name']}: {relevant_posts}ê°œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ë°œê²¬")
                    
                    except Exception as e:
                        logger.warning(f"Reddit ì˜¤ë¥˜ {sub_info['name']}: {str(e)[:50]}")
                        continue
                
                logger.info(f"ğŸ“± Reddit ìŠ¤ìº” ì™„ë£Œ: {successful_subs}/{len(reddit_subreddits)} ì„œë¸Œë ˆë”§ ì„±ê³µ")
                await asyncio.sleep(600)  # 10ë¶„ë§ˆë‹¤ Reddit ì²´í¬
                
            except Exception as e:
                logger.error(f"Reddit ëª¨ë‹ˆí„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(900)
    
    async def aggressive_api_rotation(self):
        """ì ê·¹ì  API ìˆœí™˜ ì‚¬ìš© - ë” ìì£¼ í˜¸ì¶œ"""
        while True:
            try:
                self._reset_daily_usage()
                
                # NewsAPI (15ë¶„ë§ˆë‹¤ë¡œ ë‹¨ì¶•)
                if self.newsapi_key and self.api_usage['newsapi_today'] < self.api_limits['newsapi']:
                    try:
                        await self._call_newsapi()
                        self.api_usage['newsapi_today'] += 1
                        logger.info(f"âœ… NewsAPI í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['newsapi_today']}/{self.api_limits['newsapi']})")
                    except Exception as e:
                        logger.error(f"NewsAPI í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                
                await asyncio.sleep(900)  # 15ë¶„ ëŒ€ê¸° (ê¸°ì¡´ 30ë¶„ì—ì„œ ë‹¨ì¶•)
                
                # NewsData API (30ë¶„ë§ˆë‹¤ë¡œ ë‹¨ì¶•)
                if self.newsdata_key and self.api_usage['newsdata_today'] < self.api_limits['newsdata']:
                    try:
                        await self._call_newsdata()
                        self.api_usage['newsdata_today'] += 1
                        logger.info(f"âœ… NewsData API í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['newsdata_today']}/{self.api_limits['newsdata']})")
                    except Exception as e:
                        logger.error(f"NewsData API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                
                await asyncio.sleep(900)  # 15ë¶„ ëŒ€ê¸°
                
                # Alpha Vantage (í•˜ë£¨ 3íšŒë¡œ ì¦ê°€)
                if self.alpha_vantage_key and self.api_usage['alpha_vantage_today'] < self.api_limits['alpha_vantage']:
                    try:
                        await self._call_alpha_vantage()
                        self.api_usage['alpha_vantage_today'] += 1
                        logger.info(f"âœ… Alpha Vantage API í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['alpha_vantage_today']}/{self.api_limits['alpha_vantage']})")
                    except Exception as e:
                        logger.error(f"Alpha Vantage API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                
                await asyncio.sleep(1800)  # 30ë¶„ ëŒ€ê¸°
                
            except Exception as e:
                logger.error(f"API ìˆœí™˜ ì‚¬ìš© ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1800)
    
    async def _parse_rss_feed(self, feed_info: Dict) -> List[Dict]:
        """RSS í”¼ë“œ íŒŒì‹± - í–¥ìƒëœ ì˜¤ë¥˜ ì²˜ë¦¬"""
        articles = []
        try:
            async with self.session.get(
                feed_info['url'], 
                timeout=aiohttp.ClientTimeout(total=8),  # íƒ€ì„ì•„ì›ƒ ë” ë‹¨ì¶•
                headers={'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'}
            ) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # feedparserë¡œ íŒŒì‹±
                    feed = feedparser.parse(content)
                    
                    if feed.entries:
                        # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ê²°ì •
                        limit = min(20, max(5, feed_info['weight']))  # ê¸°ì‚¬ ìˆ˜ ì¦ê°€
                        
                        for entry in feed.entries[:limit]:
                            try:
                                # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                                pub_time = datetime.now().isoformat()
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    pub_time = datetime(*entry.published_parsed[:6]).isoformat()
                                elif hasattr(entry, 'published'):
                                    # ë¬¸ìì—´ ì‹œê°„ íŒŒì‹± ì‹œë„
                                    try:
                                        from dateutil import parser
                                        pub_time = parser.parse(entry.published).isoformat()
                                    except:
                                        pass
                                
                                article = {
                                    'title': entry.get('title', '').strip(),
                                    'description': entry.get('summary', '').strip()[:400],
                                    'url': entry.get('link', '').strip(),
                                    'source': feed_info['source'],
                                    'published_at': pub_time,
                                    'weight': feed_info['weight'],
                                    'category': feed_info['category']
                                }
                                
                                # ìœ íš¨í•œ ê¸°ì‚¬ë§Œ ì¶”ê°€
                                if article['title'] and article['url']:
                                    articles.append(article)
                                        
                            except Exception as e:
                                logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:50]}")
                                continue
                    
                    if articles:
                        logger.debug(f"âœ… {feed_info['source']}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    
                elif response.status == 403:
                    logger.warning(f"âš ï¸  {feed_info['source']}: ì ‘ê·¼ ê±°ë¶€ (403)")
                elif response.status == 404:
                    logger.warning(f"âš ï¸  {feed_info['source']}: í”¼ë“œ ì—†ìŒ (404)")
                elif response.status == 401:
                    logger.warning(f"âš ï¸  {feed_info['source']}: HTTP 401")
                else:
                    logger.warning(f"âš ï¸  {feed_info['source']}: HTTP {response.status}")
        
        except asyncio.TimeoutError:
            logger.debug(f"â° {feed_info['source']}: íƒ€ì„ì•„ì›ƒ")
        except aiohttp.ClientConnectorError:
            logger.debug(f"ğŸ”Œ {feed_info['source']}: ì—°ê²° ì‹¤íŒ¨")
        except Exception as e:
            logger.debug(f"âŒ {feed_info['source']}: {str(e)[:50]}")
        
        return articles
    
    async def _call_newsapi(self):
        """NewsAPI í˜¸ì¶œ - íŠ¸ëŸ¼í”„ ë° ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥"""
        try:
            # ê²€ìƒ‰ì–´ ëŒ€í­ ê°•í™”
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': '(bitcoin AND (bought OR purchased OR buys OR "buying bitcoin" OR acquisition)) OR (gamestop AND bitcoin) OR (tesla AND bitcoin) OR (microstrategy AND bitcoin) OR "whale alert" OR (trump AND (bitcoin OR crypto OR tariff OR policy OR china OR trade)) OR (fed AND (rate OR powell OR fomc)) OR (sec AND bitcoin) OR "bitcoin etf" OR (court AND bitcoin) OR ("us china" AND trade) OR ("trade war") OR ("xi jinping") OR ("federal reserve") OR ("interest rate") OR ("monetary policy") OR ("inflation data") OR ("economic data")',
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.newsapi_key,
                'pageSize': 25,  # ê¸°ì‚¬ ìˆ˜ ì¦ê°€
                'from': (datetime.now() - timedelta(hours=1)).isoformat()  # 1ì‹œê°„ ë‚´ë¡œ ë‹¨ì¶•
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('articles', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),  # ë²ˆì—­ì€ ë‚˜ì¤‘ì— ì„ íƒì ìœ¼ë¡œ
                            'description': article.get('description', ''),
                            'url': article.get('url', ''),
                            'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                            'published_at': article.get('publishedAt', ''),
                            'weight': 10,
                            'category': 'api'
                        }
                        
                        # ë²ˆì—­ í•„ìš”ì„± ì²´í¬
                        if self.openai_client and self._should_translate(formatted_article):
                            formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                        
                        if self._is_critical_news(formatted_article):
                            if not self._is_duplicate_emergency(formatted_article):
                                formatted_article['expected_change'] = self._estimate_price_impact(formatted_article)
                                await self._trigger_emergency_alert(formatted_article)
                            processed += 1
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
                            processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° NewsAPI: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"NewsAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsAPI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_newsdata(self):
        """NewsData API í˜¸ì¶œ - í‚¤ì›Œë“œ í™•ì¥"""
        try:
            url = "https://newsdata.io/api/1/news"
            params = {
                'apikey': self.newsdata_key,
                'q': 'bitcoin OR crypto OR "federal reserve" OR SEC OR gamestop OR tesla OR trump OR "us china trade" OR "trade war" OR tariff OR powell OR "interest rate"',
                'language': 'en',
                'category': 'business,politics,top',
                'size': 15  # ê¸°ì‚¬ ìˆ˜ ì¦ê°€
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('results', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('description', ''),
                            'url': article.get('link', ''),
                            'source': f"NewsData ({article.get('source_id', 'Unknown')})",
                            'published_at': article.get('pubDate', ''),
                            'weight': 9,
                            'category': 'api'
                        }
                        
                        # ë²ˆì—­ í•„ìš”ì„± ì²´í¬
                        if self.openai_client and self._should_translate(formatted_article):
                            formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                        
                        if self._is_critical_news(formatted_article):
                            if not self._is_duplicate_emergency(formatted_article):
                                formatted_article['expected_change'] = self._estimate_price_impact(formatted_article)
                                await self._trigger_emergency_alert(formatted_article)
                            processed += 1
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
                            processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° NewsData: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"NewsData API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsData API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_alpha_vantage(self):
        """Alpha Vantage API í˜¸ì¶œ - í‹°ì»¤ í™•ì¥"""
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': 'CRYPTO:BTC,COIN:MSTR,COIN:TSLA,COIN:GME,FOREX:USD,SPY,QQQ',  # í‹°ì»¤ í™•ì¥
                'topics': 'financial_markets,economy_monetary,technology,earnings,mergers_and_acquisitions,ipo',  # í† í”½ í™•ì¥
                'apikey': self.alpha_vantage_key,
                'sort': 'LATEST',
                'limit': 15  # ê¸°ì‚¬ ìˆ˜ ì¦ê°€
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('feed', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('summary', ''),
                            'url': article.get('url', ''),
                            'source': f"Alpha Vantage ({article.get('source', 'Unknown')})",
                            'published_at': article.get('time_published', ''),
                            'weight': 10,
                            'category': 'api',
                            'sentiment': article.get('overall_sentiment_label', 'Neutral')
                        }
                        
                        # ë²ˆì—­ í•„ìš”ì„± ì²´í¬
                        if self.openai_client and self._should_translate(formatted_article):
                            formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                        
                        if self._is_critical_news(formatted_article):
                            if not self._is_duplicate_emergency(formatted_article):
                                formatted_article['expected_change'] = self._estimate_price_impact(formatted_article)
                                await self._trigger_emergency_alert(formatted_article)
                            processed += 1
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
                            processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° Alpha Vantage: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"Alpha Vantage API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"Alpha Vantage API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    def _reset_daily_usage(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        today = datetime.now().date()
        if today > self.api_usage['last_reset']:
            old_usage = dict(self.api_usage)
            self.api_usage.update({
                'newsapi_today': 0,
                'newsdata_today': 0,
                'alpha_vantage_today': 0,
                'last_reset': today
            })
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ë„ ë¦¬ì…‹
            self.company_news_count = {}
            # ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            self.translation_count = 0
            self.last_translation_reset = datetime.now()
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ ì •ë¦¬
            self.news_first_seen = {}
            logger.info(f"ğŸ”„ API ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹: NewsAPI {old_usage['newsapi_today']}â†’0, NewsData {old_usage['newsdata_today']}â†’0, AlphaVantage {old_usage['alpha_vantage_today']}â†’0")
    
    def _is_critical_news(self, article: Dict) -> bool:
        """í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨ - ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì˜í–¥ë§Œ í•„í„°ë§"""
        # ì œëª©ê³¼ ì„¤ëª… ëª¨ë‘ ì²´í¬ (í•œê¸€ ì œëª©ë„ í¬í•¨)
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ë¨¼ì € ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ë¨¼ì € ì²´í¬ - ë” ì—„ê²©í•˜ê²Œ
        bitcoin_related = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸']
        crypto_general = ['crypto', 'cryptocurrency', 'ì•”í˜¸í™”í']
        
        has_bitcoin = any(keyword in content for keyword in bitcoin_related)
        has_crypto = any(keyword in content for keyword in crypto_general)
        
        # 1. ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰ì´ ìˆëŠ” ê²½ìš°ë§Œ ìš°ì„  ì²˜ë¦¬
        if has_bitcoin:
            # ë¹„íŠ¸ì½”ì¸ + ê¸°ì—… êµ¬ë§¤
            for company in self.important_companies:
                if company.lower() in content:
                    purchase_keywords = ['bought', 'buys', 'purchased', 'purchase', 'acquisition', 'êµ¬ë§¤', 'ë§¤ì…', 'íˆ¬ì']
                    if any(keyword in content for keyword in purchase_keywords):
                        if any(char in content for char in ['$', 'ë‹¬ëŸ¬', 'dollar', 'million', 'billion']):
                            logger.warning(f"ğŸš¨ ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤: {company} - {article.get('title', '')[:50]}...")
                            return True
            
            # ë¹„íŠ¸ì½”ì¸ + ETF
            if any(word in content for word in ['etf', 'etf approval', 'etf rejected', 'spot etf']):
                if article.get('weight', 0) >= 7:
                    logger.warning(f"ğŸš¨ ë¹„íŠ¸ì½”ì¸ ETF ë‰´ìŠ¤: {article.get('title', '')[:50]}...")
                    return True
            
            # ë¹„íŠ¸ì½”ì¸ + ê·œì œ
            if any(word in content for word in ['sec', 'regulation', 'ban', 'lawsuit', 'court', 'ê·œì œ', 'ê¸ˆì§€']):
                if article.get('weight', 0) >= 7:
                    logger.warning(f"ğŸš¨ ë¹„íŠ¸ì½”ì¸ ê·œì œ ë‰´ìŠ¤: {article.get('title', '')[:50]}...")
                    return True
        
        # 2. íŠ¸ëŸ¼í”„ ê´€ë ¨ - ë¹„íŠ¸ì½”ì¸/ê²½ì œ ê´€ë ¨ë§Œ
        trump_keywords = ['trump', 'donald trump', 'president trump', 'íŠ¸ëŸ¼í”„']
        if any(keyword in content for keyword in trump_keywords):
            # íŠ¸ëŸ¼í”„ + ë¹„íŠ¸ì½”ì¸/ì•”í˜¸í™”í/ê²½ì œ ê´€ë ¨ë§Œ
            trump_relevant = ['bitcoin', 'btc', 'crypto', 'cryptocurrency', 'ë¹„íŠ¸ì½”ì¸', 'ì•”í˜¸í™”í', 
                            'tariff', 'trade', 'china', 'fed', 'federal reserve', 'economy', 
                            'ê´€ì„¸', 'ë¬´ì—­', 'ì¤‘êµ­', 'ì—°ì¤€', 'ê²½ì œ', 'executive order', 'policy']
            if any(rel in content for rel in trump_relevant):
                if article.get('weight', 0) >= 7:
                    logger.warning(f"ğŸš¨ íŠ¸ëŸ¼í”„ ê²½ì œ/ì•”í˜¸í™”í ë‰´ìŠ¤: {article.get('title', '')[:50]}...")
                    return True
            else:
                # íŠ¸ëŸ¼í”„ ê´€ë ¨ì´ì§€ë§Œ ê²½ì œ/ì•”í˜¸í™”íì™€ ë¬´ê´€í•˜ë©´ ì œì™¸
                return False
        
        # 3. ë¯¸ì¤‘ ë¬´ì—­ - ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰ ë˜ëŠ” ê²½ì œ ì „ë°˜ ì˜í–¥
        trade_keywords = ['us china trade', 'china trade war', 'trade talks', 'xi jinping', 'ë¯¸ì¤‘ ë¬´ì—­', 'ì‹œì§„í•‘']
        if any(keyword in content for keyword in trade_keywords):
            # ê¸€ë¡œë²Œ ê²½ì œì— ì˜í–¥ì„ ì£¼ëŠ” ê·œëª¨ì˜ ë‰´ìŠ¤ë§Œ
            if any(word in content for word in ['billion', 'trillion', 'agreement', 'deal', 'í˜‘ì •', 'í•©ì˜']) and article.get('weight', 0) >= 7:
                logger.warning(f"ğŸš¨ ë¯¸ì¤‘ ë¬´ì—­ ì£¼ìš” ë‰´ìŠ¤: {article.get('title', '')[:50]}...")
                return True
            elif has_bitcoin or has_crypto:
                logger.warning(f"ğŸš¨ ë¯¸ì¤‘ ë¬´ì—­ + ì•”í˜¸í™”í: {article.get('title', '')[:50]}...")
                return True
            else:
                return False
        
        # 4. Fed/ê¸ˆë¦¬ ê´€ë ¨ - ì‹¤ì œ ê²°ì •ì´ë‚˜ ì¤‘ìš” ë°œì–¸ë§Œ
        fed_keywords = ['fed rate decision', 'powell says', 'fomc decides', 'interest rate hike', 'interest rate cut', 'ì—°ì¤€ ê¸ˆë¦¬']
        fed_important = ['rate decision', 'rate hike', 'rate cut', 'fomc meeting', 'powell speech', 'ê¸ˆë¦¬ ê²°ì •', 'ê¸ˆë¦¬ ì¸ìƒ', 'ê¸ˆë¦¬ ì¸í•˜']
        if any(keyword in content for keyword in fed_keywords) or any(keyword in content for keyword in fed_important):
            if article.get('weight', 0) >= 7:
                logger.warning(f"ğŸš¨ Fed ì¤‘ìš” ë‰´ìŠ¤: {article.get('title', '')[:50]}...")
                return True
        
        # 5. ê¸°íƒ€ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ - ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ìˆì„ ë•Œë§Œ
        if has_bitcoin:
            bitcoin_critical = [
                'bitcoin crash', 'bitcoin surge', 'bitcoin rally', 'bitcoin plunge',
                'whale alert', 'large bitcoin transfer', 'bitcoin moved',
                'exchange hacked', 'bitcoin stolen', 'security breach',
                'ë¹„íŠ¸ì½”ì¸ í­ë½', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±', 'ê³ ë˜ ì´ë™', 'ê±°ë˜ì†Œ í•´í‚¹'
            ]
            
            for keyword in bitcoin_critical:
                if keyword.lower() in content:
                    if article.get('weight', 0) >= 6:
                        negative_filters = ['fake', 'rumor', 'unconfirmed', 'alleged', 'speculation', 'ë£¨ë¨¸', 'ì¶”ì¸¡', 'ë¯¸í™•ì¸']
                        if not any(neg in content for neg in negative_filters):
                            logger.warning(f"ğŸš¨ ë¹„íŠ¸ì½”ì¸ í¬ë¦¬í‹°ì»¬: {article.get('title', '')[:50]}...")
                            return True
        
        # 6. ì•ŒíŠ¸ì½”ì¸ ë‰´ìŠ¤ëŠ” ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì˜í–¥ ìˆì„ ë•Œë§Œ
        altcoin_keywords = ['ethereum', 'eth', 'xrp', 'ripple', 'solana', 'sol', 'cardano', 'ada']
        if any(alt in content for alt in altcoin_keywords):
            # ì•ŒíŠ¸ì½”ì¸ ë‰´ìŠ¤ëŠ” ë¹„íŠ¸ì½”ì¸ê³¼ ì§ì ‘ ì—°ê´€ì„±ì´ ëª…ì‹œëœ ê²½ìš°ë§Œ
            if has_bitcoin and any(word in content for word in ['correlation', 'impact on bitcoin', 'bitcoin follows', 'ë¹„íŠ¸ì½”ì¸ ì˜í–¥']):
                if article.get('weight', 0) >= 8:  # ë” ë†’ì€ ê°€ì¤‘ì¹˜ ìš”êµ¬
                    logger.warning(f"ğŸš¨ ì•ŒíŠ¸ì½”ì¸â†’ë¹„íŠ¸ì½”ì¸ ì˜í–¥: {article.get('title', '')[:50]}...")
                    return True
            else:
                # ì•ŒíŠ¸ì½”ì¸ ë‹¨ë… ë‰´ìŠ¤ëŠ” ì œì™¸
                return False
        
        return False
    
    def _is_important_news(self, article: Dict) -> bool:
        """ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨ - ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ê°•í™”"""
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ì ìˆ˜ ì‹œìŠ¤í…œ
        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸']  # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰
        crypto_keywords = ['crypto', 'cryptocurrency', 'digital asset', 'blockchain', 'ì•”í˜¸í™”í', 'ë¸”ë¡ì²´ì¸']  # ì•”í˜¸í™”í ì¼ë°˜
        finance_keywords = ['fed', 'federal reserve', 'interest rate', 'inflation', 'sec', 'regulation', 'monetary policy', 'ì—°ì¤€', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ê·œì œ']
        political_keywords = ['trump', 'biden', 'congress', 'government', 'policy', 'administration', 'white house', 'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'ì •ë¶€', 'ì •ì±…', 'china', 'trade']
        market_keywords = ['market', 'trading', 'price', 'surge', 'crash', 'rally', 'dump', 'volatility', 'etf', 'ì‹œì¥', 'ê±°ë˜', 'ê°€ê²©', 'ê¸‰ë“±', 'í­ë½', 'ETF']
        company_keywords = self.important_companies
        
        bitcoin_score = sum(1 for word in bitcoin_keywords if word in content)
        crypto_score = sum(1 for word in crypto_keywords if word in content)
        finance_score = sum(1 for word in finance_keywords if word in content)
        political_score = sum(1 for word in political_keywords if word in content)
        market_score = sum(1 for word in market_keywords if word in content)
        company_score = sum(1 for word in company_keywords if word.lower() in content)
        
        total_score = bitcoin_score + crypto_score + finance_score + political_score + market_score + company_score
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ê´€ë ¨ì„± ìš°ì„  - ë” ì—„ê²©í•œ ì¡°ê±´
        conditions = [
            # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰ + ë‹¤ë¥¸ ìš”ì†Œ
            bitcoin_score >= 1 and (finance_score >= 1 or political_score >= 1 or company_score >= 1),
            
            # ë¹„íŠ¸ì½”ì¸ + ETF
            bitcoin_score >= 1 and 'etf' in content,
            
            # ê¸°ì—… + ë¹„íŠ¸ì½”ì¸ ì¡°í•©
            company_score >= 1 and bitcoin_score >= 1,
            
            # ì•”í˜¸í™”í ì „ë¬¸ ì†ŒìŠ¤ + ë¹„íŠ¸ì½”ì¸
            category == 'crypto' and bitcoin_score >= 1 and weight >= 8,
            
            # ê³ ê°€ì¤‘ì¹˜ ì†ŒìŠ¤ + ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰
            weight >= 9 and bitcoin_score >= 1,
            
            # Fed/ê¸ˆë¦¬ + ë†’ì€ ê°€ì¤‘ì¹˜ (ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰ ì—†ì–´ë„ ì¤‘ìš”)
            finance_score >= 2 and weight >= 8 and any(word in content for word in ['rate decision', 'fomc', 'powell', 'ê¸ˆë¦¬ ê²°ì •']),
            
            # íŠ¸ëŸ¼í”„ + ê²½ì œ/ë¬´ì—­ ê´€ë ¨ (ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰ ì—†ì–´ë„ ì‹œì¥ ì˜í–¥)
            political_score >= 1 and weight >= 8 and any(word in content for word in ['trump', 'íŠ¸ëŸ¼í”„']) and 
            any(word in content for word in ['tariff', 'trade', 'china', 'economy', 'ê´€ì„¸', 'ë¬´ì—­', 'ì¤‘êµ­', 'ê²½ì œ']),
            
            # API ë‰´ìŠ¤ + ë¹„íŠ¸ì½”ì¸/ì•”í˜¸í™”í
            category == 'api' and weight >= 9 and (bitcoin_score >= 1 or crypto_score >= 1),
        ]
        
        is_important = any(conditions)
        
        # ì•ŒíŠ¸ì½”ì¸ ë‹¨ë… ë‰´ìŠ¤ëŠ” ì¤‘ìš”ë„ ë‚®ì¶¤
        altcoin_keywords = ['ethereum', 'eth', 'xrp', 'ripple', 'solana', 'sol', 'cardano', 'ada']
        if any(alt in content for alt in altcoin_keywords) and bitcoin_score == 0:
            # ì•ŒíŠ¸ì½”ì¸ ë‰´ìŠ¤ëŠ” ETFë‚˜ ëŒ€ê·œëª¨ ì´ë²¤íŠ¸ê°€ ì•„ë‹ˆë©´ ì œì™¸
            if not any(word in content for word in ['etf', 'billion', 'major', 'breakthrough', 'ì‹­ì–µ', 'ëŒ€ê·œëª¨']):
                is_important = False
        
        if is_important:
            logger.debug(f"ğŸ“‹ ì¤‘ìš” ë‰´ìŠ¤: {article.get('source', '')[:15]} - BTC:{bitcoin_score},C:{crypto_score},F:{finance_score},P:{political_score},M:{market_score},Co:{company_score}")
        
        return is_important
    
    async def _trigger_emergency_alert(self, article: Dict):
        """ê¸´ê¸‰ ì•Œë¦¼ íŠ¸ë¦¬ê±° - ì²« ë°œê²¬ ì‹œê°„ ì¶”ì """
        try:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                logger.info(f"ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ëœ ê¸´ê¸‰ ë‰´ìŠ¤ ìŠ¤í‚µ: {article.get('title', '')[:30]}...")
                return
            
            # ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¡œ ê¸°ë¡
            self.processed_news_hashes.add(content_hash)
            
            # ì˜¤ë˜ëœ í•´ì‹œ ì •ë¦¬ (1000ê°œ ì´ˆê³¼ì‹œ)
            if len(self.processed_news_hashes) > 1000:
                self.processed_news_hashes = set(list(self.processed_news_hashes)[-500:])
            
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ ê¸°ë¡
            if content_hash not in self.news_first_seen:
                self.news_first_seen[content_hash] = datetime.now()
            
            event = {
                'type': 'critical_news',
                'title': article.get('title_ko', article.get('title', ''))[:100],
                'description': article.get('description', '')[:250],
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self._determine_impact(article),
                'expected_change': article.get('expected_change', 'Â±0.3%'),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown'),
                'published_at': article.get('published_at', ''),
                'first_seen': self.news_first_seen[content_hash]
            }
            
            # ë°ì´í„° ì»¬ë ‰í„°ì— ì „ë‹¬
            if hasattr(self, 'data_collector') and self.data_collector:
                self.data_collector.events_buffer.append(event)
            
            logger.critical(f"ğŸš¨ ê¸´ê¸‰ ë‰´ìŠ¤ ì•Œë¦¼: {article.get('source', '')} - {article.get('title_ko', article.get('title', ''))[:60]} (ì˜ˆìƒ: {event['expected_change']})")
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _add_to_news_buffer(self, article: Dict):
        """ë‰´ìŠ¤ ë²„í¼ì— ì¶”ê°€ - íšŒì‚¬ë³„ ì¹´ìš´íŠ¸ ì œí•œ"""
        try:
            # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
            new_title = article.get('title', '').lower()
            new_title_ko = article.get('title_ko', '').lower()
            new_source = article.get('source', '').lower()
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                logger.debug(f"ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ ìŠ¤í‚µ: {new_title[:30]}...")
                return
            
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ í™•ì¸
            for company in self.important_companies:
                if company.lower() in new_title or company.lower() in new_title_ko:
                    # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
                    bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                    if any(keyword in new_title or keyword in new_title_ko for keyword in bitcoin_keywords):
                        # í•´ë‹¹ íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ê°€ ì´ë¯¸ 1ê°œ ì´ìƒì¸ì§€ í™•ì¸
                        if self.company_news_count.get(company.lower(), 0) >= 1:
                            logger.debug(f"ğŸ”„ {company} ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì´ë¯¸ ìˆìŒ, ìŠ¤í‚µ: {new_title[:30]}...")
                            return
            
            # ë²„í¼ì— ìˆëŠ” ë‰´ìŠ¤ì™€ ì¤‘ë³µ ì²´í¬
            is_duplicate = False
            for existing in self.news_buffer:
                # ë™ì¼í•œ ë‰´ìŠ¤ ì²´í¬
                if self._is_similar_news(new_title, existing.get('title', '')):
                    is_duplicate = True
                    break
                
                # í•œê¸€ ì œëª©ë„ ì²´í¬
                if new_title_ko and existing.get('title_ko', ''):
                    if self._is_similar_news(new_title_ko, existing.get('title_ko', '')):
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                self.news_buffer.append(article)
                self.processed_news_hashes.add(content_hash)
                
                # íšŒì‚¬ë³„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                for company in self.important_companies:
                    if company.lower() in new_title or company.lower() in new_title_ko:
                        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                        if any(keyword in new_title or keyword in new_title_ko for keyword in bitcoin_keywords):
                            self.company_news_count[company.lower()] = self.company_news_count.get(company.lower(), 0) + 1
                            logger.debug(f"ğŸ“Š {company} ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì¹´ìš´íŠ¸: {self.company_news_count[company.lower()]}")
                
                # ë²„í¼ ê´€ë¦¬: ê°€ì¤‘ì¹˜, ì¹´í…Œê³ ë¦¬, ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 60ê°œë§Œ ìœ ì§€ (ê¸°ì¡´ 50ê°œì—ì„œ ì¦ê°€)
                if len(self.news_buffer) > 60:
                    def sort_key(x):
                        weight = x.get('weight', 0)
                        category_priority = {'crypto': 5, 'api': 4, 'politics': 3, 'finance': 2, 'news': 2, 'tech': 1}  # ì •ì¹˜ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                        cat_score = category_priority.get(x.get('category', ''), 0)
                        pub_time = x.get('published_at', '')
                        return (weight, cat_score, pub_time)
                    
                    self.news_buffer.sort(key=sort_key, reverse=True)
                    self.news_buffer = self.news_buffer[:60]
            else:
                logger.debug(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ì œì™¸: {new_title_ko[:30] if new_title_ko else new_title[:30]}...")
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def _determine_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ ì˜í–¥ë„ íŒë‹¨ - ë¹„íŠ¸ì½”ì¸ ì¤‘ì‹¬ìœ¼ë¡œ ì¬ì¡°ì •"""
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰ í™•ì¸
        has_bitcoin = any(word in content for word in ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸'])
        
        # íŠ¸ëŸ¼í”„ ê´€ë ¨ - ê²½ì œ/ì•”í˜¸í™”í ê´€ë ¨ë§Œ
        if 'trump' in content:
            if any(word in content for word in ['bitcoin', 'crypto', 'digital asset', 'ë¹„íŠ¸ì½”ì¸', 'ì•”í˜¸í™”í']):
                return "ğŸ“ˆ ì•½í•œ í˜¸ì¬"  # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰
            elif any(word in content for word in ['china', 'trade', 'tariff', 'ì¤‘êµ­', 'ë¬´ì—­', 'ê´€ì„¸']):
                return "ğŸ“Š ì‹œì¥ ê´€ì‹¬"  # ê°„ì ‘ ì˜í–¥
            elif any(word in content for word in ['economy', 'economic', 'ê²½ì œ']):
                return "âš ï¸ ê´€ë ¨ì„± ê²€í† "  # ê²½ì œ ê´€ë ¨
            else:
                return "âš ï¸ ë¹„íŠ¸ì½”ì¸ ë¬´ê´€"  # ì§ì ‘ ê´€ë ¨ ì—†ìŒ
        
        # ë¯¸ì¤‘ ë¬´ì—­/ê´€ê³„ - ê¸€ë¡œë²Œ ê²½ì œ ì˜í–¥ë§Œ
        if any(word in content for word in ['us china trade', 'trade war', 'china tariff', 'xi jinping']):
            if has_bitcoin:
                return "ğŸ“Š ì¤‘ê°„ ë³€ë™ì„±"
            elif any(word in content for word in ['billion', 'trillion', 'agreement', 'í˜‘ì •']):
                return "âš ï¸ ê°„ì ‘ ì˜í–¥"  # ëŒ€ê·œëª¨ ê²½ì œ ì´ë²¤íŠ¸
            else:
                return "âš ï¸ ì œí•œì  ì˜í–¥"
        
        # Fed/ê¸ˆë¦¬ ê´€ë ¨ - ë¹„íŠ¸ì½”ì¸ì— ì§ì ‘ ì˜í–¥
        if any(word in content for word in ['fed rate', 'powell', 'fomc', 'interest rate']):
            if any(word in content for word in ['hike', 'raise', 'increase']):
                return "ğŸ“‰ ì¤‘ê°„ ì•…ì¬"  # ê¸ˆë¦¬ ì¸ìƒ
            elif any(word in content for word in ['cut', 'lower', 'decrease']):
                return "ğŸ“ˆ ì¤‘ê°„ í˜¸ì¬"  # ê¸ˆë¦¬ ì¸í•˜
            else:
                return "âš ï¸ í†µí™” ì •ì±…"
        
        # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤
        if has_bitcoin:
            for company in self.important_companies:
                if company.lower() in content and any(word in content for word in ['bought', 'purchased', 'buys', 'bitcoin', 'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë§¤ì…']):
                    if any(word in content for word in ['billion', 'ì–µ ë‹¬ëŸ¬', 'ì‹­ì–µ']):
                        return "ğŸ“ˆ ì¤‘ê°„ í˜¸ì¬"
                    else:
                        return "ğŸ“ˆ ì•½í•œ í˜¸ì¬"
        
        # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„±ì´ ì—†ëŠ” ê²½ìš°
        if not has_bitcoin and not any(word in content for word in ['crypto', 'cryptocurrency', 'ì•”í˜¸í™”í']):
            # ì•ŒíŠ¸ì½”ì¸ ë‰´ìŠ¤
            if any(word in content for word in ['ethereum', 'eth', 'xrp', 'ripple']):
                return "âš ï¸ ì•ŒíŠ¸ì½”ì¸ (BTC ë¬´ê´€)"
            # ì¼ë°˜ ë‰´ìŠ¤
            else:
                return "âš ï¸ ë¹„íŠ¸ì½”ì¸ ë¬´ê´€"
        
        # ë¹„íŠ¸ì½”ì¸ ìš°ì„¸/ë„ë¯¸ë„ŒìŠ¤ ê´€ë ¨
        if any(word in content for word in ['dominance', 'ìš°ì„¸', 'ì ìœ ìœ¨']):
            return "âš ï¸ ì¤‘ë¦½ (ì´ë¯¸ ë°˜ì˜)"
        
        # ì‚¬ê¸°/í•´í‚¹ ê´€ë ¨ - ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ë§Œ
        if has_bitcoin and any(word in content for word in ['scam', 'fraud', 'hack', 'ì‚¬ê¸°', 'í•´í‚¹']):
            if any(word in content for word in ['decrease', 'down', 'ê°ì†Œ', 'ì¤„ì–´']):
                return "ğŸ“ˆ ì•½í•œ í˜¸ì¬"  # ë³´ì•ˆ ê°œì„ 
            else:
                return "ğŸ“‰ ì•½í•œ ì•…ì¬"  # ì‚¬ê¸° í”¼í•´
        
        # ê°•í•œ ì•…ì¬/í˜¸ì¬ (ë¹„íŠ¸ì½”ì¸ ê´€ë ¨)
        if has_bitcoin:
            strong_bearish = ['ban', 'banned', 'lawsuit', 'crash', 'crackdown', 'reject', 'rejected', 'hack', 'hacked', 'ê¸ˆì§€', 'ê·œì œ', 'ì†Œì†¡', 'í­ë½', 'í•´í‚¹']
            strong_bullish = ['approval', 'approved', 'adoption', 'breakthrough', 'all-time high', 'ath', 'ìŠ¹ì¸', 'ì±„íƒ', 'ì‹ ê³ ê°€']
            bearish = ['concern', 'worry', 'decline', 'fall', 'drop', 'uncertainty', 'regulation', 'fine', 'ìš°ë ¤', 'í•˜ë½', 'ë¶ˆí™•ì‹¤']
            bullish = ['growth', 'rise', 'increase', 'positive', 'rally', 'surge', 'investment', 'institutional', 'ìƒìŠ¹', 'ì¦ê°€', 'ê¸ì •ì ', 'íˆ¬ì']
            
            strong_bearish_count = sum(1 for word in strong_bearish if word in content)
            strong_bullish_count = sum(1 for word in strong_bullish if word in content)
            bearish_count = sum(1 for word in bearish if word in content)
            bullish_count = sum(1 for word in bullish if word in content)
            
            if strong_bearish_count >= 1:
                return "ğŸ“‰ ì¤‘ê°„ ì•…ì¬"
            elif strong_bullish_count >= 1:
                return "ğŸ“ˆ ì¤‘ê°„ í˜¸ì¬"
            elif bearish_count > bullish_count:
                return "ğŸ“‰ ì•½í•œ ì•…ì¬"
            elif bullish_count > bearish_count:
                return "ğŸ“ˆ ì•½í•œ í˜¸ì¬"
        
        # ê¸°ë³¸ê°’
        if has_bitcoin:
            return "âš ï¸ ì¤‘ë¦½"
        else:
            return "âš ï¸ ë¹„íŠ¸ì½”ì¸ ë¬´ê´€"
    
    async def get_recent_news(self, hours: int = 6) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° - íšŒì‚¬ë³„ ì¤‘ë³µ ì œê±° ê°•í™”"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_news = []
            seen_titles = set()  # ì¤‘ë³µ ì²´í¬ìš©
            company_count = {}  # íšŒì‚¬ë³„ ì¹´ìš´íŠ¸
            
            for article in self.news_buffer:
                try:
                    # ë°œí–‰ ì‹œê°„ ì²´í¬
                    if article.get('published_at'):
                        pub_time_str = article.get('published_at', '').replace('Z', '').replace('T', ' ')
                        # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                        try:
                            if 'T' in article.get('published_at', ''):
                                pub_time = datetime.fromisoformat(pub_time_str)
                            else:
                                from dateutil import parser
                                pub_time = parser.parse(article.get('published_at', ''))
                            
                            if pub_time > cutoff_time:
                                # ì¤‘ë³µ ì²´í¬
                                title_hash = self._generate_content_hash(article.get('title', ''), '')
                                if title_hash not in seen_titles:
                                    # íšŒì‚¬ë³„ ì¹´ìš´íŠ¸ í™•ì¸
                                    skip = False
                                    article_title = (article.get('title', '') + ' ' + article.get('title_ko', '')).lower()
                                    
                                    for company in self.important_companies:
                                        if company.lower() in article_title:
                                            bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                                            if any(keyword in article_title for keyword in bitcoin_keywords):
                                                if company_count.get(company.lower(), 0) >= 1:
                                                    skip = True
                                                    break
                                                else:
                                                    company_count[company.lower()] = company_count.get(company.lower(), 0) + 1
                                    
                                    if not skip:
                                        recent_news.append(article)
                                        seen_titles.add(title_hash)
                        except:
                            # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ì‹œ ìµœê·¼ ë‰´ìŠ¤ë¡œ ê°„ì£¼ (ì•ˆì „ì¥ì¹˜)
                            title_hash = self._generate_content_hash(article.get('title', ''), '')
                            if title_hash not in seen_titles:
                                recent_news.append(article)
                                seen_titles.add(title_hash)
                    else:
                        title_hash = self._generate_content_hash(article.get('title', ''), '')
                        if title_hash not in seen_titles:
                            recent_news.append(article)
                            seen_titles.add(title_hash)
                except:
                    pass
            
            # ì¶”ê°€ ì¤‘ë³µ ì œê±°: ìœ ì‚¬í•œ ì œëª© ì œê±°
            final_news = []
            for article in recent_news:
                is_similar = False
                for final_article in final_news:
                    if self._is_similar_news(article.get('title', ''), final_article.get('title', '')):
                        is_similar = True
                        break
                
                if not is_similar:
                    final_news.append(article)
            
            # ì •ë ¬ ê¸°ì¤€: ê°€ì¤‘ì¹˜ â†’ ì¹´í…Œê³ ë¦¬ â†’ ì‹œê°„
            def sort_key(x):
                weight = x.get('weight', 0)
                category_priority = {'crypto': 5, 'api': 4, 'politics': 3, 'finance': 2, 'news': 2, 'tech': 1}  # ì •ì¹˜ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                cat_score = category_priority.get(x.get('category', ''), 0)
                pub_time = x.get('published_at', '')
                return (weight, cat_score, pub_time)
            
            final_news.sort(key=sort_key, reverse=True)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ê· í˜• ì¡°ì • (ì •ì¹˜/ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì¶”ê°€)
            balanced_news = []
            crypto_count = 0
            politics_count = 0  # ì‹ ê·œ ì¶”ê°€
            other_count = 0
            
            for article in final_news:
                category = article.get('category', '')
                if category == 'crypto' and crypto_count < 8:
                    balanced_news.append(article)
                    crypto_count += 1
                elif category in ['politics', 'news'] and politics_count < 4:  # ì •ì¹˜/ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                    balanced_news.append(article)
                    politics_count += 1
                elif category not in ['crypto', 'politics', 'news'] and other_count < 3:
                    balanced_news.append(article)
                    other_count += 1
                elif len(balanced_news) < 12:  # ì´ 12ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€
                    balanced_news.append(article)
            
            final_result = balanced_news[:15]  # ìµœëŒ€ 15ê°œë¡œ ì¦ê°€
            
            logger.info(f"ğŸ“° ìµœê·¼ {hours}ì‹œê°„ ë‰´ìŠ¤ ë°˜í™˜: ì´ {len(final_result)}ê±´ (ì•”í˜¸í™”í: {crypto_count}, ì •ì¹˜: {politics_count}, ê¸°íƒ€: {other_count})")
            return final_result
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self.session:
                await self.session.close()
                logger.info("ğŸ”š ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

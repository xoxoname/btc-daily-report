import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from news_collector_core import NewsCollectorCore
from news_translator import NewsTranslator
from news_processor import NewsProcessor

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    """ğŸ”¥ğŸ”¥ í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - 3ê°œ ëª¨ë“ˆ í†µí•© ê´€ë¦¬"""
    
    def __init__(self, config):
        self.config = config
        
        # 3ê°œ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.core_collector = NewsCollectorCore(config)
        self.translator = NewsTranslator(config)
        self.processor = NewsProcessor(config)
        
        # ë©”ì¸ ë‰´ìŠ¤ ë²„í¼
        self.news_buffer = []
        self.events_buffer = []
        
        # ë°ì´í„° ì»¬ë ‰í„° ì°¸ì¡°
        self.data_collector = None
        
        logger.info(f"ğŸ”¥ğŸ”¥ í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“¡ RSS + API ìˆ˜ì§‘: NewsCollectorCore")
        logger.info(f"ğŸ§  ë²ˆì—­/ìš”ì•½: NewsTranslator (GPT ìš°ì„ , Claude ë°±ì—…)")
        logger.info(f"ğŸ“Š ë¶„ì„/ì²˜ë¦¬: NewsProcessor (ë¶„ë¥˜, ì¤‘ë³µì²´í¬, ì´ë²¤íŠ¸ìƒì„±)")
    
    def set_data_collector(self, data_collector):
        """ë°ì´í„° ì»¬ë ‰í„° ì„¤ì •"""
        self.data_collector = data_collector
    
    async def start_monitoring(self):
        """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        logger.info("ğŸ”¥ğŸ”¥ í†µí•© ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        # í•µì‹¬ ìˆ˜ì§‘ ì‘ì—…ë“¤
        tasks = [
            self.core_collector.start_monitoring(),  # RSS + API ìˆ˜ì§‘
            self.process_collected_news(),           # ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì²˜ë¦¬
            self.cleanup_old_data()                  # ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def process_collected_news(self):
        """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬"""
        while True:
            try:
                # í•µì‹¬ ìˆ˜ì§‘ê¸°ì—ì„œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                collected_news = self.core_collector.news_buffer.copy()
                
                if not collected_news:
                    await asyncio.sleep(10)
                    continue
                
                processed_count = 0
                critical_count = 0
                
                for article in collected_news:
                    try:
                        # ìµœì‹  ë‰´ìŠ¤ë§Œ ì²˜ë¦¬ (2ì‹œê°„ ì´ë‚´)
                        if not self._is_recent_news(article, hours=2):
                            continue
                        
                        # ë¹„íŠ¸ì½”ì¸ + ê±°ì‹œê²½ì œ ê´€ë ¨ì„± ì²´í¬
                        if not self.processor.is_bitcoin_or_macro_related(article):
                            continue
                        
                        # ê¸°ì—…ëª… ì¶”ì¶œ
                        company = self.processor.extract_company_from_content(
                            article.get('title', ''),
                            article.get('description', '')
                        )
                        if company:
                            article['company'] = company
                        
                        # í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ì²´í¬
                        if self.processor.is_critical_news(article):
                            # ë²ˆì—­ (í•„ìš”ì‹œì—ë§Œ)
                            if self.translator.should_translate_for_emergency_report(article):
                                article['title_ko'] = await self.translator.translate_text(article.get('title', ''))
                            else:
                                article['title_ko'] = article.get('title', '')
                            
                            # ìš”ì•½ (ì„ íƒì )
                            if self.translator.should_use_gpt_summary(article):
                                summary = await self.translator.summarize_article(
                                    article['title'],
                                    article.get('description', '')
                                )
                                if summary:
                                    article['summary'] = summary
                            
                            # ì¤‘ë³µ ì²´í¬ í›„ ì´ë²¤íŠ¸ ìƒì„±
                            if not self.processor.is_duplicate_emergency(article):
                                article['expected_change'] = self.processor.estimate_price_impact(article)
                                event = self.processor.create_emergency_event(article, self.translator)
                                
                                if event and self.data_collector:
                                    self.data_collector.events_buffer.append(event)
                                    critical_count += 1
                        
                        # ì¤‘ìš” ë‰´ìŠ¤ëŠ” ë²„í¼ì— ì¶”ê°€
                        elif self.processor.is_important_news(article):
                            self.news_buffer.append(article)
                        
                        processed_count += 1
                        
                    except Exception as e:
                        logger.error(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                # ì²˜ë¦¬ëœ ë‰´ìŠ¤ëŠ” í•µì‹¬ ìˆ˜ì§‘ê¸° ë²„í¼ì—ì„œ ì œê±°
                self.core_collector.news_buffer = []
                
                if processed_count > 0:
                    logger.info(f"ğŸ“° ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬ (í¬ë¦¬í‹°ì»¬: {critical_count}ê°œ)")
                
                # ë²„í¼ í¬ê¸° ê´€ë¦¬
                if len(self.news_buffer) > 100:
                    self.news_buffer = self.news_buffer[-100:]
                
                await asyncio.sleep(15)  # 15ì´ˆë§ˆë‹¤ ì²˜ë¦¬
                
            except Exception as e:
                logger.error(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)
    
    async def cleanup_old_data(self):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ - 30ë¶„ë§ˆë‹¤"""
        while True:
            try:
                # ë‰´ìŠ¤ ì²˜ë¦¬ê¸°ì˜ ì •ë¦¬ ì‘ì—… ì‹¤í–‰
                self.processor.cleanup_old_data()
                
                # ë©”ì¸ ë²„í¼ ì •ë¦¬
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=12)
                
                # ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œê±°
                old_buffer_size = len(self.news_buffer)
                self.news_buffer = [
                    article for article in self.news_buffer
                    if self._get_article_time(article) > cutoff_time
                ]
                
                removed_count = old_buffer_size - len(self.news_buffer)
                if removed_count > 0:
                    logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ë‰´ìŠ¤ ì •ë¦¬: {removed_count}ê°œ ì œê±°")
                
                await asyncio.sleep(1800)  # 30ë¶„ë§ˆë‹¤
                
            except Exception as e:
                logger.error(f"âŒ ë°ì´í„° ì •ë¦¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
    
    def _is_recent_news(self, article: Dict, hours: int = 2) -> bool:
        """ë‰´ìŠ¤ê°€ ìµœê·¼ ê²ƒì¸ì§€ í™•ì¸"""
        try:
            pub_time_str = article.get('published_at', '')
            if not pub_time_str:
                return True
            
            try:
                if 'T' in pub_time_str:
                    pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                else:
                    from dateutil import parser
                    pub_time = parser.parse(pub_time_str)
                
                if pub_time.tzinfo is None:
                    import pytz
                    pub_time = pytz.UTC.localize(pub_time)
                
                time_diff = datetime.now(pytz.UTC) - pub_time
                return time_diff.total_seconds() < (hours * 3600)
            except:
                return True
        except:
            return True
    
    def _get_article_time(self, article: Dict) -> datetime:
        """ê¸°ì‚¬ ì‹œê°„ ì¶”ì¶œ"""
        try:
            pub_time_str = article.get('published_at', '')
            if pub_time_str:
                if 'T' in pub_time_str:
                    return datetime.fromisoformat(pub_time_str.replace('Z', ''))
                else:
                    from dateutil import parser
                    return parser.parse(pub_time_str)
        except:
            pass
        
        return datetime.now()
    
    async def get_recent_news(self, hours: int = 12) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # í•µì‹¬ ìˆ˜ì§‘ê¸°ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            core_news = await self.core_collector.get_recent_news(hours)
            
            # ë©”ì¸ ë²„í¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            cutoff_time = datetime.now() - timedelta(hours=hours)
            buffer_news = [
                article for article in self.news_buffer
                if self._get_article_time(article) > cutoff_time
            ]
            
            # í•©ì¹˜ê³  ì •ë ¬
            all_news = core_news + buffer_news
            
            # ì¤‘ë³µ ì œê±°
            seen_hashes = set()
            unique_news = []
            
            for article in all_news:
                content_hash = self.processor.generate_content_hash(
                    article.get('title', ''),
                    article.get('description', '')
                )
                if content_hash not in seen_hashes:
                    unique_news.append(article)
                    seen_hashes.add(content_hash)
            
            # ê°€ì¤‘ì¹˜ì™€ ì‹œê°„ìœ¼ë¡œ ì •ë ¬
            unique_news.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
            
            logger.info(f"ğŸ“° ìµœê·¼ {hours}ì‹œê°„ ë‰´ìŠ¤: {len(unique_news)}ê°œ")
            
            return unique_news[:25]
            
        except Exception as e:
            logger.error(f"âŒ ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_translation_stats(self) -> Dict:
        """ë²ˆì—­ í†µê³„ ë°˜í™˜"""
        return self.translator.get_translation_stats()
    
    def update_news_stats(self, news_type: str, translation_type: str = None):
        """ë‰´ìŠ¤ í†µê³„ ì—…ë°ì´íŠ¸ (í˜¸í™˜ì„±ì„ ìœ„í•´)"""
        pass
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            await self.core_collector.close()
            
            # í†µê³„ ì¶œë ¥
            stats = self.get_translation_stats()
            logger.info("ğŸ”š í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ")
            logger.info(f"ğŸ§  ìµœì¢… GPT ë²ˆì—­: {stats['gpt_translations']}, Claude ë²ˆì—­: {stats['claude_translations']}")
            logger.info(f"ğŸ“ ìµœì¢… GPT ìš”ì•½: {stats['summaries']}")
            logger.info(f"âš ï¸ Claude ì—ëŸ¬: {stats['claude_errors']}íšŒ")
            logger.info(f"ğŸ’° ë²ˆì—­ ì •ì±…: í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì „ì†¡ ì‹œì—ë§Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

import aiohttp
import asyncio
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional, Set
import pytz
from bs4 import BeautifulSoup
import feedparser
import openai
import os
import hashlib
import re

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    def __init__(self, config):
        self.config = config
        self.session = None
        self.news_buffer = []
        self.emergency_alerts_sent = {}  # ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€ìš©
        self.processed_news_hashes = set()  # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ ì €ì¥
        self.news_title_cache = {}  # ì œëª©ë³„ ìºì‹œ
        self.company_news_count = {}  # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸
        
        # ë²ˆì—­ ìºì‹œ ë° rate limit ê´€ë¦¬ - í•œë„ ì¦ê°€
        self.translation_cache = {}  # ë²ˆì—­ ìºì‹œ
        self.translation_count = 0  # ë²ˆì—­ íšŸìˆ˜ ì¶”ì 
        self.last_translation_reset = datetime.now()
        self.max_translations_per_30min = 200  # 30ë¶„ë‹¹ ìµœëŒ€ ë²ˆì—­ ìˆ˜ (ê¸°ì¡´ 50/ì‹œê°„ â†’ 200/30ë¶„)
        self.translation_reset_interval = 1800  # 30ë¶„ (ê¸°ì¡´ 3600ì´ˆ â†’ 1800ì´ˆ)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë²ˆì—­ìš©)
        self.openai_client = None
        if hasattr(config, 'OPENAI_API_KEY') and config.OPENAI_API_KEY:
            self.openai_client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        
        # ëª¨ë“  API í‚¤ë“¤
        self.newsapi_key = getattr(config, 'NEWSAPI_KEY', None)
        self.newsdata_key = getattr(config, 'NEWSDATA_KEY', None)
        self.alpha_vantage_key = getattr(config, 'ALPHA_VANTAGE_KEY', None)
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ (ì¦‰ì‹œ ì•Œë¦¼ìš©) - ê°•í™”
        self.critical_keywords = [
            # ì •ë¶€/ì •ì¹˜ ê´€ë ¨ - íŠ¸ëŸ¼í”„ ì¶”ê°€
            'trump bitcoin', 'trump crypto', 'trump ban', 'trump announces', 'trump says bitcoin',
            'trump tariff', 'trump executive order', 'trump policy', 'trump federal',
            'íŠ¸ëŸ¼í”„ ë¹„íŠ¸ì½”ì¸', 'íŠ¸ëŸ¼í”„ ì•”í˜¸í™”í', 'íŠ¸ëŸ¼í”„ ê·œì œ', 'íŠ¸ëŸ¼í”„ ê´€ì„¸', 'íŠ¸ëŸ¼í”„ ì •ì±…',
            # ì—°ì¤€/ê¸ˆë¦¬ ê´€ë ¨
            'fed rate decision', 'fed raises', 'fed cuts', 'powell says', 'fomc decides', 'fed meeting',
            'interest rate hike', 'interest rate cut', 'monetary policy',
            'ì—°ì¤€ ê¸ˆë¦¬', 'FOMC ê²°ì •', 'íŒŒì›” ë°œì–¸', 'ê¸ˆë¦¬ ì¸ìƒ', 'ê¸ˆë¦¬ ì¸í•˜',
            # SEC ê´€ë ¨
            'sec lawsuit bitcoin', 'sec sues', 'sec enforcement', 'sec charges bitcoin',
            'sec approves', 'sec rejects', 'sec bitcoin etf',
            'SEC ì†Œì†¡', 'SEC ê·œì œ', 'SEC ë¹„íŠ¸ì½”ì¸', 'SEC ìŠ¹ì¸', 'SEC ê±°ë¶€',
            # ê·œì œ/ê¸ˆì§€ ê´€ë ¨
            'china bans bitcoin', 'china crypto ban', 'government bans crypto', 'regulatory ban',
            'court blocks', 'federal court', 'supreme court crypto',
            'ì¤‘êµ­ ë¹„íŠ¸ì½”ì¸ ê¸ˆì§€', 'ì •ë¶€ ê·œì œ', 'ì•”í˜¸í™”í ê¸ˆì§€', 'ë²•ì› íŒê²°',
            # ì‹œì¥ ê¸‰ë³€ë™
            'bitcoin crash', 'crypto crash', 'market crash', 'flash crash', 'bitcoin plunge',
            'bitcoin surge', 'bitcoin rally', 'bitcoin breaks',
            'ë¹„íŠ¸ì½”ì¸ í­ë½', 'ì•”í˜¸í™”í ê¸‰ë½', 'ì‹œì¥ ë¶•ê´´', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±',
            # ETF ê´€ë ¨
            'bitcoin etf approved', 'bitcoin etf rejected', 'etf decision', 'etf filing',
            'ETF ìŠ¹ì¸', 'ETF ê±°ë¶€', 'ETF ê²°ì •',
            # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤
            'bought bitcoin', 'buys bitcoin', 'purchased bitcoin', 'bitcoin purchase', 'bitcoin acquisition',
            'tesla bitcoin', 'microstrategy bitcoin', 'square bitcoin', 'paypal bitcoin',
            'gamestop bitcoin', 'gme bitcoin', '$gme bitcoin',
            'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë¹„íŠ¸ì½”ì¸ ë§¤ì…', 'ë¹„íŠ¸ì½”ì¸ íˆ¬ì', 'ë¹„íŠ¸ì½”ì¸ ë³´ìœ ',
            # ëŒ€ëŸ‰ ê±°ë˜/ì´ë™
            'whale alert', 'large bitcoin transfer', 'bitcoin moved', 'btc transferred',
            'exchange inflow', 'exchange outflow',
            'ê³ ë˜ ì´ë™', 'ëŒ€ëŸ‰ ì´ì²´', 'ë¹„íŠ¸ì½”ì¸ ì´ë™', 'ê±°ë˜ì†Œ ìœ ì…', 'ê±°ë˜ì†Œ ìœ ì¶œ',
            # í•´í‚¹/ë³´ì•ˆ
            'exchange hacked', 'bitcoin stolen', 'crypto hack', 'security breach',
            'ê±°ë˜ì†Œ í•´í‚¹', 'ë¹„íŠ¸ì½”ì¸ ë„ë‚œ', 'ë³´ì•ˆ ì‚¬ê³ '
        ]
        
        # ì œì™¸ í‚¤ì›Œë“œ (ë¹„íŠ¸ì½”ì¸ê³¼ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ê²ƒë“¤)
        self.exclude_keywords = [
            'gold price', 'gold rises', 'gold falls', 'gold market',
            'oil price', 'oil market', 'commodity',
            'stock market', 'nasdaq', 's&p 500', 'dow jones',
            'ê¸ˆ ê°€ê²©', 'ê¸ˆê°’', 'ì›ìœ ', 'ì£¼ì‹ì‹œì¥',
            'mining at home', 'ì§‘ì—ì„œ ì±„êµ´', 'how to mine',
            'crypto news today', 'ì˜¤ëŠ˜ì˜ ì•”í˜¸í™”í ì†Œì‹',
            'price prediction', 'ê°€ê²© ì˜ˆì¸¡'
        ]
        
        # ì¤‘ìš” ê¸°ì—… ë¦¬ìŠ¤íŠ¸
        self.important_companies = [
            'tesla', 'microstrategy', 'square', 'block', 'paypal', 'mastercard', 'visa',
            'apple', 'google', 'amazon', 'meta', 'facebook', 'microsoft', 'netflix',
            'gamestop', 'gme', 'amc', 'blackrock', 'fidelity', 'jpmorgan', 'goldman',
            'samsung', 'lg', 'sk', 'kakao', 'naver', 'ì‚¼ì„±', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„',
            'metaplanet', 'ë©”íƒ€í”Œë˜ë‹›'
        ]
        
        # RSS í”¼ë“œ
        self.rss_feeds = [
            # ì•”í˜¸í™”í ì „ë¬¸ (ìµœìš°ì„ )
            {'url': 'https://cointelegraph.com/rss', 'source': 'Cointelegraph', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'source': 'CoinDesk', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://decrypt.co/feed', 'source': 'Decrypt', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://bitcoinmagazine.com/.rss/full/', 'source': 'Bitcoin Magazine', 'weight': 9, 'category': 'crypto'},
            
            # ìƒˆë¡œìš´ ì•”í˜¸í™”í ì†ŒìŠ¤
            {'url': 'https://ambcrypto.com/feed/', 'source': 'AMBCrypto', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://cryptopotato.com/feed/', 'source': 'CryptoPotato', 'weight': 8, 'category': 'crypto'},
            
            # ì¼ë°˜ ê¸ˆìœµ
            {'url': 'https://www.marketwatch.com/rss/topstories', 'source': 'MarketWatch', 'weight': 8, 'category': 'finance'},
            {'url': 'https://seekingalpha.com/feed.xml', 'source': 'Seeking Alpha', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.feedburner.com/InvestingcomAnalysis', 'source': 'Investing.com', 'weight': 8, 'category': 'finance'},
            {'url': 'https://www.fool.com/feeds/index.aspx', 'source': 'Motley Fool', 'weight': 7, 'category': 'finance'},
            
            # ì¼ë°˜ ë‰´ìŠ¤ (í™•ì‹¤í•œ ê²ƒë“¤)
            {'url': 'https://rss.cnn.com/rss/edition.rss', 'source': 'CNN World', 'weight': 8, 'category': 'news'},
            {'url': 'http://feeds.bbci.co.uk/news/business/rss.xml', 'source': 'BBC Business', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.npr.org/1001/rss.xml', 'source': 'NPR News', 'weight': 7, 'category': 'news'},
            {'url': 'https://feeds.washingtonpost.com/rss/business', 'source': 'Washington Post Business', 'weight': 7, 'category': 'finance'},
            
            # í…Œí¬/ë¹„ì¦ˆë‹ˆìŠ¤
            {'url': 'https://techcrunch.com/feed/', 'source': 'TechCrunch', 'weight': 7, 'category': 'tech'},
            {'url': 'https://www.wired.com/feed/rss', 'source': 'Wired', 'weight': 6, 'category': 'tech'},
            {'url': 'https://feeds.feedburner.com/venturebeat/SZYF', 'source': 'VentureBeat', 'weight': 7, 'category': 'tech'},
            
            # ì¶”ê°€ ì‹ ë¢°í• ë§Œí•œ ê¸ˆìœµ ì†ŒìŠ¤
            {'url': 'https://feeds.bloomberg.com/markets/news.rss', 'source': 'Bloomberg Markets', 'weight': 9, 'category': 'finance'},
        ]
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.api_usage = {
            'newsapi_today': 0,
            'newsdata_today': 0,
            'alpha_vantage_today': 0,
            'last_reset': datetime.now().date()
        }
        
        # API ì¼ì¼ í•œë„
        self.api_limits = {
            'newsapi': 15,
            'newsdata': 8,
            'alpha_vantage': 1
        }
        
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ - API í‚¤ ìƒíƒœ: NewsAPI={bool(self.newsapi_key)}, NewsData={bool(self.newsdata_key)}, AlphaVantage={bool(self.alpha_vantage_key)}")
    
    def _reset_translation_count_if_needed(self):
        """í•„ìš”ì‹œ ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ - 30ë¶„ë§ˆë‹¤"""
        now = datetime.now()
        if (now - self.last_translation_reset).total_seconds() > self.translation_reset_interval:
            old_count = self.translation_count
            self.translation_count = 0
            self.last_translation_reset = now
            logger.info(f"ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {old_count} â†’ 0 (30ë¶„ ê²½ê³¼)")
    
    def _should_translate(self, article: Dict) -> bool:
        """ë‰´ìŠ¤ë¥¼ ë²ˆì—­í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜"""
        # ì´ë¯¸ í•œê¸€ ì œëª©ì´ ìˆìœ¼ë©´ ë²ˆì—­ ë¶ˆí•„ìš”
        if article.get('title_ko') and article['title_ko'] != article.get('title', ''):
            return False
        
        # ë²ˆì—­ ìš°ì„ ìˆœìœ„ ê²°ì •
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # 1ìˆœìœ„: í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ëŠ” í•­ìƒ ë²ˆì—­
        if self._is_critical_news(article):
            return True
        
        # 2ìˆœìœ„: ì¤‘ìš” ë‰´ìŠ¤ + ë†’ì€ ê°€ì¤‘ì¹˜
        if self._is_important_news(article) and weight >= 8:
            return True
        
        # 3ìˆœìœ„: ì•”í˜¸í™”í ì¹´í…Œê³ ë¦¬ + ì¤‘ìš” ë‰´ìŠ¤
        if category == 'crypto' and self._is_important_news(article):
            return True
        
        # 4ìˆœìœ„: API ë‰´ìŠ¤ (NewsAPI, NewsData ë“±)
        if category == 'api' and weight >= 9:
            return True
        
        # ë‚˜ë¨¸ì§€ëŠ” ë²ˆì—­ í•˜ì§€ ì•ŠìŒ
        return False
    
    async def translate_text(self, text: str, max_length: int = 100) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ (Rate limit ì²˜ë¦¬ í¬í•¨)"""
        if not self.openai_client:
            return text
        
        # ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ì²´í¬
        self._reset_translation_count_if_needed()
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]
        
        # Rate limit ì²´í¬
        if self.translation_count >= self.max_translations_per_30min:
            logger.warning(f"ë²ˆì—­ í•œë„ ì´ˆê³¼: {self.translation_count}/{self.max_translations_per_30min} (30ë¶„)")
            return text[:max_length] + "..." if len(text) > max_length else text
        
        try:
            # ê¸¸ì´ ì œí•œ
            if len(text) > max_length:
                text = text[:max_length] + "..."
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a professional translator. Translate the following text to Korean concisely and accurately. Keep it under 80 characters."},
                    {"role": "user", "content": text}
                ],
                max_tokens=150,
                temperature=0.3
            )
            
            translated = response.choices[0].message.content.strip()
            # ë²ˆì—­ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(translated) > 80:
                translated = translated[:77] + "..."
            
            # ìºì‹œ ì €ì¥ ë° ì¹´ìš´íŠ¸ ì¦ê°€
            self.translation_cache[cache_key] = translated
            self.translation_count += 1
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.translation_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ 500ê°œ ì œê±°
                keys_to_remove = list(self.translation_cache.keys())[:500]
                for key in keys_to_remove:
                    del self.translation_cache[key]
            
            return translated
            
        except openai.RateLimitError as e:
            logger.warning(f"OpenAI Rate limit ì˜¤ë¥˜: {str(e)}")
            self.translation_count = self.max_translations_per_30min  # ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•Šë„ë¡
            return text[:80] + "..." if len(text) > 80 else text
        except Exception as e:
            logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:50]}")
            return text[:80] + "..." if len(text) > 80 else text
    
    def _generate_content_hash(self, title: str, description: str = "") -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì˜ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©) - ê°•í™”ëœ ë²„ì „"""
        # ì œëª©ì—ì„œ ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ì—¬ ìœ ì‚¬í•œ ë‰´ìŠ¤ ê°ì§€
        clean_title = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title.lower())
        clean_title = re.sub(r'\s+', ' ', clean_title).strip()
        
        # íšŒì‚¬ëª…ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        companies = []
        keywords = []
        
        for company in self.important_companies:
            if company.lower() in clean_title.lower():
                companies.append(company.lower())
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        key_terms = ['bitcoin', 'btc', 'purchase', 'bought', 'buys', 'acquisition', 'êµ¬ë§¤', 'ë§¤ì…', 'first', 'ì²«']
        for term in key_terms:
            if term in clean_title.lower():
                keywords.append(term)
        
        # íšŒì‚¬ëª… + í•µì‹¬ í‚¤ì›Œë“œë¡œ í•´ì‹œ ìƒì„±
        if companies and keywords:
            # íšŒì‚¬ë³„ë¡œ í•˜ë‚˜ì˜ í•´ì‹œë§Œ ìƒì„± (ìˆ«ì ë¬´ì‹œ)
            hash_content = f"{','.join(sorted(set(companies)))}_{','.join(sorted(set(keywords)))}"
        else:
            # ì¼ë°˜ ë‰´ìŠ¤ëŠ” ì „ì²´ ë‚´ìš©ìœ¼ë¡œ í•´ì‹œ
            hash_content = clean_title
        
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def _is_duplicate_emergency(self, article: Dict, time_window: int = 120) -> bool:
        """ê¸´ê¸‰ ì•Œë¦¼ì´ ì¤‘ë³µì¸ì§€ í™•ì¸ (120ë¶„ ì´ë‚´ ìœ ì‚¬ ë‚´ìš©)"""
        try:
            current_time = datetime.now()
            content_hash = self._generate_content_hash(
                article.get('title', ''), 
                article.get('description', '')
            )
            
            # ì‹œê°„ì´ ì§€ë‚œ ì•Œë¦¼ ì œê±°
            cutoff_time = current_time - timedelta(minutes=time_window)
            self.emergency_alerts_sent = {
                k: v for k, v in self.emergency_alerts_sent.items()
                if v > cutoff_time
            }
            
            # ì¤‘ë³µ ì²´í¬
            if content_hash in self.emergency_alerts_sent:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€: {article.get('title', '')[:50]}...")
                return True
            
            # ìƒˆë¡œìš´ ì•Œë¦¼ ê¸°ë¡
            self.emergency_alerts_sent[content_hash] = current_time
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _is_similar_news(self, title1: str, title2: str) -> bool:
        """ë‘ ë‰´ìŠ¤ ì œëª©ì´ ìœ ì‚¬í•œì§€ í™•ì¸ - ë” ì—„ê²©í•œ ê¸°ì¤€"""
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean1 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title1.lower())
        clean2 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title2.lower())
        
        clean1 = re.sub(r'\s+', ' ', clean1).strip()
        clean2 = re.sub(r'\s+', ' ', clean2).strip()
        
        # íŠ¹ì • íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ ë‰´ìŠ¤ì¸ì§€ ì²´í¬
        for company in self.important_companies:
            company_lower = company.lower()
            if company_lower in clean1 and company_lower in clean2:
                # ê°™ì€ íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ë©´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
                bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                if any(keyword in clean1 for keyword in bitcoin_keywords) and \
                   any(keyword in clean2 for keyword in bitcoin_keywords):
                    return True
        
        # ë‹¨ì–´ ì§‘í•© ë¹„êµ
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        if not words1 or not words2:
            return False
        
        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        if union == 0:
            return False
        
        similarity = intersection / union
        
        # 65% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        return similarity > 0.65
    
    async def start_monitoring(self):
        """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=15),
                connector=aiohttp.TCPConnector(limit=100, limit_per_host=30)
            )
        
        logger.info("ğŸ” ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - RSS ì¤‘ì‹¬ + ìŠ¤ë§ˆíŠ¸ API ì‚¬ìš©")
        logger.info(f"ğŸ“Š ë²ˆì—­ ì„¤ì •: 30ë¶„ë‹¹ ìµœëŒ€ {self.max_translations_per_30min}ê°œ")
        
        # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        self.company_news_count = {}
        
        tasks = [
            self.monitor_rss_feeds(),      # ë©”ì¸: RSS (45ì´ˆë§ˆë‹¤)
            self.monitor_reddit(),         # ë³´ì¡°: Reddit (10ë¶„ë§ˆë‹¤)
            self.smart_api_rotation()      # ì œí•œì : 3ê°œ API ìˆœí™˜ ì‚¬ìš©
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_rss_feeds(self):
        """RSS í”¼ë“œ ëª¨ë‹ˆí„°ë§ - ë©”ì¸ ì†ŒìŠ¤"""
        while True:
            try:
                # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì†ŒìŠ¤ë¶€í„° ì²˜ë¦¬
                sorted_feeds = sorted(self.rss_feeds, key=lambda x: x['weight'], reverse=True)
                successful_feeds = 0
                
                for feed_info in sorted_feeds:
                    try:
                        articles = await self._parse_rss_feed(feed_info)
                        
                        if articles:  # ì„±ê³µì ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¨ ê²½ìš°
                            successful_feeds += 1
                            
                            for article in articles:
                                # ë²ˆì—­ í•„ìš” ì—¬ë¶€ ì²´í¬
                                if self.openai_client and self._should_translate(article):
                                    article['title_ko'] = await self.translate_text(article['title'])
                                else:
                                    article['title_ko'] = article.get('title', '')
                                
                                # ê°€ì¤‘ì¹˜ 8 ì´ìƒì€ í¬ë¦¬í‹°ì»¬ ì²´í¬
                                if feed_info['weight'] >= 8:
                                    if self._is_critical_news(article):
                                        # ì¤‘ë³µ ì²´í¬ í›„ ì•Œë¦¼
                                        if not self._is_duplicate_emergency(article):
                                            await self._trigger_emergency_alert(article)
                                
                                # ëª¨ë“  RSSëŠ” ì¤‘ìš” ë‰´ìŠ¤ ì²´í¬
                                if self._is_important_news(article):
                                    await self._add_to_news_buffer(article)
                    
                    except Exception as e:
                        logger.warning(f"RSS í”¼ë“œ ì¼ì‹œ ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:100]}")
                        continue
                
                logger.info(f"ğŸ“° RSS ìŠ¤ìº” ì™„ë£Œ: {successful_feeds}/{len(sorted_feeds)} í”¼ë“œ ì„±ê³µ (ë²ˆì—­: {self.translation_count}/{self.max_translations_per_30min})")
                await asyncio.sleep(45)  # 45ì´ˆë§ˆë‹¤ ì „ì²´ RSS ì²´í¬
                
            except Exception as e:
                logger.error(f"RSS ëª¨ë‹ˆí„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(180)
    
    async def monitor_reddit(self):
        """Reddit ëª¨ë‹ˆí„°ë§"""
        reddit_subreddits = [
            {'name': 'Bitcoin', 'threshold': 200, 'weight': 8},
            {'name': 'CryptoCurrency', 'threshold': 400, 'weight': 7},
            {'name': 'investing', 'threshold': 800, 'weight': 6},
            {'name': 'wallstreetbets', 'threshold': 2000, 'weight': 5}
        ]
        
        while True:
            try:
                successful_subs = 0
                
                for sub_info in reddit_subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{sub_info['name']}/hot.json?limit=20"
                        
                        async with self.session.get(url, headers={'User-Agent': 'Bitcoin Monitor Bot 1.0'}) as response:
                            if response.status == 200:
                                data = await response.json()
                                posts = data['data']['children']
                                successful_subs += 1
                                
                                relevant_posts = 0
                                for post in posts:
                                    post_data = post['data']
                                    
                                    if post_data['ups'] > sub_info['threshold']:
                                        article = {
                                            'title': post_data['title'],
                                            'title_ko': post_data['title'],  # Redditì€ ê¸°ë³¸ì ìœ¼ë¡œ ë²ˆì—­ ìƒëµ
                                            'description': post_data.get('selftext', '')[:200],
                                            'url': f"https://reddit.com{post_data['permalink']}",
                                            'source': f"Reddit r/{sub_info['name']}",
                                            'published_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                                            'upvotes': post_data['ups'],
                                            'weight': sub_info['weight']
                                        }
                                        
                                        # Redditë„ ì¤‘ìš”ë„ì— ë”°ë¼ ë²ˆì—­
                                        if self._is_critical_news(article) and self.openai_client and self._should_translate(article):
                                            article['title_ko'] = await self.translate_text(article['title'])
                                        
                                        if self._is_critical_news(article):
                                            if not self._is_duplicate_emergency(article):
                                                await self._trigger_emergency_alert(article)
                                            relevant_posts += 1
                                        elif self._is_important_news(article):
                                            await self._add_to_news_buffer(article)
                                            relevant_posts += 1
                                
                                if relevant_posts > 0:
                                    logger.info(f"ğŸ“± Reddit r/{sub_info['name']}: {relevant_posts}ê°œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ë°œê²¬")
                    
                    except Exception as e:
                        logger.warning(f"Reddit ì˜¤ë¥˜ {sub_info['name']}: {str(e)[:50]}")
                        continue
                
                logger.info(f"ğŸ“± Reddit ìŠ¤ìº” ì™„ë£Œ: {successful_subs}/{len(reddit_subreddits)} ì„œë¸Œë ˆë”§ ì„±ê³µ")
                await asyncio.sleep(600)  # 10ë¶„ë§ˆë‹¤ Reddit ì²´í¬
                
            except Exception as e:
                logger.error(f"Reddit ëª¨ë‹ˆí„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(900)
    
    async def smart_api_rotation(self):
        """3ê°œ API ìŠ¤ë§ˆíŠ¸ ìˆœí™˜ ì‚¬ìš©"""
        while True:
            try:
                self._reset_daily_usage()
                
                # NewsAPI (30ë¶„ë§ˆë‹¤)
                if self.newsapi_key and self.api_usage['newsapi_today'] < self.api_limits['newsapi']:
                    try:
                        await self._call_newsapi()
                        self.api_usage['newsapi_today'] += 1
                        logger.info(f"âœ… NewsAPI í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['newsapi_today']}/{self.api_limits['newsapi']})")
                    except Exception as e:
                        logger.error(f"NewsAPI í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                
                await asyncio.sleep(1800)  # 30ë¶„ ëŒ€ê¸°
                
                # NewsData API (1ì‹œê°„ë§ˆë‹¤)
                if self.newsdata_key and self.api_usage['newsdata_today'] < self.api_limits['newsdata']:
                    try:
                        await self._call_newsdata()
                        self.api_usage['newsdata_today'] += 1
                        logger.info(f"âœ… NewsData API í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['newsdata_today']}/{self.api_limits['newsdata']})")
                    except Exception as e:
                        logger.error(f"NewsData API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                
                await asyncio.sleep(1800)  # 30ë¶„ ëŒ€ê¸°
                
                # Alpha Vantage (í•˜ë£¨ 1íšŒ)
                if self.alpha_vantage_key and self.api_usage['alpha_vantage_today'] < self.api_limits['alpha_vantage']:
                    try:
                        await self._call_alpha_vantage()
                        self.api_usage['alpha_vantage_today'] += 1
                        logger.info(f"âœ… Alpha Vantage API í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['alpha_vantage_today']}/{self.api_limits['alpha_vantage']})")
                    except Exception as e:
                        logger.error(f"Alpha Vantage API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)[:100]}")
                
                await asyncio.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸°
                
            except Exception as e:
                logger.error(f"API ìˆœí™˜ ì‚¬ìš© ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
    
    async def _parse_rss_feed(self, feed_info: Dict) -> List[Dict]:
        """RSS í”¼ë“œ íŒŒì‹± - í–¥ìƒëœ ì˜¤ë¥˜ ì²˜ë¦¬"""
        articles = []
        try:
            async with self.session.get(
                feed_info['url'], 
                timeout=aiohttp.ClientTimeout(total=10),
                headers={'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'}
            ) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # feedparserë¡œ íŒŒì‹±
                    feed = feedparser.parse(content)
                    
                    if feed.entries:
                        # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ê²°ì •
                        limit = min(15, max(5, feed_info['weight']))
                        
                        for entry in feed.entries[:limit]:
                            try:
                                # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                                pub_time = datetime.now().isoformat()
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    pub_time = datetime(*entry.published_parsed[:6]).isoformat()
                                elif hasattr(entry, 'published'):
                                    # ë¬¸ìì—´ ì‹œê°„ íŒŒì‹± ì‹œë„
                                    try:
                                        from dateutil import parser
                                        pub_time = parser.parse(entry.published).isoformat()
                                    except:
                                        pass
                                
                                article = {
                                    'title': entry.get('title', '').strip(),
                                    'description': entry.get('summary', '').strip()[:400],
                                    'url': entry.get('link', '').strip(),
                                    'source': feed_info['source'],
                                    'published_at': pub_time,
                                    'weight': feed_info['weight'],
                                    'category': feed_info['category']
                                }
                                
                                # ìœ íš¨í•œ ê¸°ì‚¬ë§Œ ì¶”ê°€
                                if article['title'] and article['url']:
                                    # ìµœê·¼ 6ì‹œê°„ ë‚´ ê¸°ì‚¬ë§Œ
                                    try:
                                        article_time = datetime.fromisoformat(pub_time.replace('Z', ''))
                                        if datetime.now() - article_time < timedelta(hours=6):
                                            articles.append(article)
                                    except:
                                        articles.append(article)  # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨
                                        
                            except Exception as e:
                                logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:50]}")
                                continue
                    
                    if articles:
                        logger.debug(f"âœ… {feed_info['source']}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    
                elif response.status == 403:
                    logger.warning(f"âš ï¸  {feed_info['source']}: ì ‘ê·¼ ê±°ë¶€ (403)")
                elif response.status == 404:
                    logger.warning(f"âš ï¸  {feed_info['source']}: í”¼ë“œ ì—†ìŒ (404)")
                elif response.status == 401:
                    logger.warning(f"âš ï¸  {feed_info['source']}: HTTP 401")
                else:
                    logger.warning(f"âš ï¸  {feed_info['source']}: HTTP {response.status}")
        
        except asyncio.TimeoutError:
            logger.debug(f"â° {feed_info['source']}: íƒ€ì„ì•„ì›ƒ")
        except aiohttp.ClientConnectorError:
            logger.debug(f"ğŸ”Œ {feed_info['source']}: ì—°ê²° ì‹¤íŒ¨")
        except Exception as e:
            logger.debug(f"âŒ {feed_info['source']}: {str(e)[:50]}")
        
        return articles
    
    async def _call_newsapi(self):
        """NewsAPI í˜¸ì¶œ - íŠ¸ëŸ¼í”„ ë° ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€"""
        try:
            # ê²€ìƒ‰ì–´ ê°•í™”
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': '(bitcoin AND (bought OR purchased OR buys OR "buying bitcoin" OR acquisition)) OR (gamestop AND bitcoin) OR (tesla AND bitcoin) OR (microstrategy AND bitcoin) OR "whale alert" OR (trump AND (bitcoin OR crypto OR tariff OR policy)) OR (fed AND rate) OR (sec AND bitcoin) OR "bitcoin etf" OR (court AND bitcoin)',
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.newsapi_key,
                'pageSize': 20,
                'from': (datetime.now() - timedelta(hours=2)).isoformat()
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('articles', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),  # ë²ˆì—­ì€ ë‚˜ì¤‘ì— ì„ íƒì ìœ¼ë¡œ
                            'description': article.get('description', ''),
                            'url': article.get('url', ''),
                            'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                            'published_at': article.get('publishedAt', ''),
                            'weight': 10,
                            'category': 'api'
                        }
                        
                        # ë²ˆì—­ í•„ìš”ì„± ì²´í¬
                        if self.openai_client and self._should_translate(formatted_article):
                            formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                        
                        if self._is_critical_news(formatted_article):
                            if not self._is_duplicate_emergency(formatted_article):
                                await self._trigger_emergency_alert(formatted_article)
                            processed += 1
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
                            processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° NewsAPI: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"NewsAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsAPI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_newsdata(self):
        """NewsData API í˜¸ì¶œ"""
        try:
            url = "https://newsdata.io/api/1/news"
            params = {
                'apikey': self.newsdata_key,
                'q': 'bitcoin OR crypto OR "federal reserve" OR SEC OR gamestop OR tesla OR trump',
                'language': 'en',
                'category': 'business,politics,top',
                'size': 10
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('results', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('description', ''),
                            'url': article.get('link', ''),
                            'source': f"NewsData ({article.get('source_id', 'Unknown')})",
                            'published_at': article.get('pubDate', ''),
                            'weight': 9,
                            'category': 'api'
                        }
                        
                        # ë²ˆì—­ í•„ìš”ì„± ì²´í¬
                        if self.openai_client and self._should_translate(formatted_article):
                            formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                        
                        if self._is_critical_news(formatted_article):
                            if not self._is_duplicate_emergency(formatted_article):
                                await self._trigger_emergency_alert(formatted_article)
                            processed += 1
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
                            processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° NewsData: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"NewsData API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsData API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_alpha_vantage(self):
        """Alpha Vantage API í˜¸ì¶œ"""
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': 'CRYPTO:BTC,COIN:MSTR,COIN:TSLA,COIN:GME',
                'topics': 'financial_markets,economy_monetary,technology',
                'apikey': self.alpha_vantage_key,
                'sort': 'LATEST',
                'limit': 10
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('feed', [])
                    
                    processed = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('summary', ''),
                            'url': article.get('url', ''),
                            'source': f"Alpha Vantage ({article.get('source', 'Unknown')})",
                            'published_at': article.get('time_published', ''),
                            'weight': 10,
                            'category': 'api',
                            'sentiment': article.get('overall_sentiment_label', 'Neutral')
                        }
                        
                        # ë²ˆì—­ í•„ìš”ì„± ì²´í¬
                        if self.openai_client and self._should_translate(formatted_article):
                            formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                        
                        if self._is_critical_news(formatted_article):
                            if not self._is_duplicate_emergency(formatted_article):
                                await self._trigger_emergency_alert(formatted_article)
                            processed += 1
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
                            processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ“° Alpha Vantage: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬")
                else:
                    logger.warning(f"Alpha Vantage API ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"Alpha Vantage API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    def _reset_daily_usage(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        today = datetime.now().date()
        if today > self.api_usage['last_reset']:
            old_usage = dict(self.api_usage)
            self.api_usage.update({
                'newsapi_today': 0,
                'newsdata_today': 0,
                'alpha_vantage_today': 0,
                'last_reset': today
            })
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ë„ ë¦¬ì…‹
            self.company_news_count = {}
            # ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            self.translation_count = 0
            self.last_translation_reset = datetime.now()
            logger.info(f"ğŸ”„ API ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹: NewsAPI {old_usage['newsapi_today']}â†’0, NewsData {old_usage['newsdata_today']}â†’0")
    
    def _is_critical_news(self, article: Dict) -> bool:
        """í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨ - ë” ì •í™•í•œ í•„í„°ë§"""
        # ì œëª©ê³¼ ì„¤ëª… ëª¨ë‘ ì²´í¬ (í•œê¸€ ì œëª©ë„ í¬í•¨)
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ë¨¼ì € ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ì²´í¬
        bitcoin_related = ['bitcoin', 'btc', 'crypto', 'ë¹„íŠ¸ì½”ì¸', 'ì•”í˜¸í™”í']
        if not any(keyword in content for keyword in bitcoin_related):
            # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ í¬ë¦¬í‹°ì»¬ ì•„ë‹˜
            return False
        
        # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ ê°ì§€
        for company in self.important_companies:
            if company.lower() in content:
                # ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬
                purchase_keywords = ['bought', 'buys', 'purchased', 'bitcoin purchase', 'bitcoin acquisition',
                                   'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë¹„íŠ¸ì½”ì¸ ë§¤ì…', 'ë¹„íŠ¸ì½”ì¸ íˆ¬ì', 'bitcoin', 'btc']
                if any(keyword in content for keyword in purchase_keywords):
                    # ê¸ˆì•¡ì´ í¬í•¨ëœ ê²½ìš° ë” ë†’ì€ ì‹ ë¢°ë„
                    if any(char in content for char in ['$', 'ë‹¬ëŸ¬', 'dollar', 'million', 'billion']):
                        logger.warning(f"ğŸš¨ ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ ê°ì§€: {company} - {article.get('title', '')[:50]}...")
                        return True
        
        # ê¸°ì¡´ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.critical_keywords:
            if keyword.lower() in content:
                # ì‹ ë¢°í•  ë§Œí•œ ì†ŒìŠ¤ì—ì„œë§Œ (ê°€ì¤‘ì¹˜ 7 ì´ìƒ)
                if article.get('weight', 0) >= 7:
                    # ì¶”ê°€ ê²€ì¦: ë¶€ì •ì  í‚¤ì›Œë“œ ì œì™¸
                    negative_filters = ['fake', 'rumor', 'unconfirmed', 'alleged', 'speculation', 'ë£¨ë¨¸', 'ì¶”ì¸¡', 'ë¯¸í™•ì¸']
                    if not any(neg in content for neg in negative_filters):
                        logger.warning(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ê°ì§€: {article.get('source', '')[:20]} - {article.get('title_ko', article.get('title', ''))[:50]}...")
                        return True
        
        return False
    
    def _is_important_news(self, article: Dict) -> bool:
        """ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨ - í–¥ìƒëœ ë¡œì§"""
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ì ìˆ˜ ì‹œìŠ¤í…œ
        crypto_keywords = ['bitcoin', 'btc', 'crypto', 'cryptocurrency', 'digital asset', 'blockchain', 'ë¹„íŠ¸ì½”ì¸', 'ì•”í˜¸í™”í', 'ë¸”ë¡ì²´ì¸']
        finance_keywords = ['fed', 'federal reserve', 'interest rate', 'inflation', 'sec', 'regulation', 'monetary policy', 'ì—°ì¤€', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ê·œì œ']
        political_keywords = ['trump', 'biden', 'congress', 'government', 'policy', 'administration', 'white house', 'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'ì •ë¶€', 'ì •ì±…']
        market_keywords = ['market', 'trading', 'price', 'surge', 'crash', 'rally', 'dump', 'volatility', 'etf', 'ì‹œì¥', 'ê±°ë˜', 'ê°€ê²©', 'ê¸‰ë“±', 'í­ë½', 'ETF']
        company_keywords = self.important_companies
        
        crypto_score = sum(1 for word in crypto_keywords if word in content)
        finance_score = sum(1 for word in finance_keywords if word in content)
        political_score = sum(1 for word in political_keywords if word in content)
        market_score = sum(1 for word in market_keywords if word in content)
        company_score = sum(1 for word in company_keywords if word.lower() in content)
        
        total_score = crypto_score + finance_score + political_score + market_score + company_score
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # íŒë‹¨ ì¡°ê±´ë“¤
        conditions = [
            crypto_score >= 2,  # ì•”í˜¸í™”í í‚¤ì›Œë“œ 2ê°œ ì´ìƒ
            crypto_score >= 1 and (finance_score >= 1 or political_score >= 1),  # ì•”í˜¸í™”í + ê¸ˆìœµ/ì •ì¹˜
            crypto_score >= 1 and company_score >= 1,  # ì•”í˜¸í™”í + ê¸°ì—…
            weight >= 9 and total_score >= 2,  # ê³ ê°€ì¤‘ì¹˜ ì†ŒìŠ¤ + ê´€ë ¨ í‚¤ì›Œë“œ
            category == 'crypto' and market_score >= 1,  # ì•”í˜¸í™”í ì†ŒìŠ¤ + ì‹œì¥ í‚¤ì›Œë“œ
            crypto_score >= 1 and 'etf' in content,  # ETF ê´€ë ¨
            finance_score >= 2 and weight >= 8,  # ê¸ˆìœµ í‚¤ì›Œë“œ + ì‹ ë¢°í• ë§Œí•œ ì†ŒìŠ¤
            company_score >= 1 and ('bitcoin' in content or 'btc' in content),  # ê¸°ì—… + ë¹„íŠ¸ì½”ì¸
        ]
        
        is_important = any(conditions)
        
        if is_important:
            logger.debug(f"ğŸ“‹ ì¤‘ìš” ë‰´ìŠ¤: {article.get('source', '')[:15]} - ì ìˆ˜(C:{crypto_score},F:{finance_score},P:{political_score},M:{market_score},Co:{company_score})")
        
        return is_important
    
    async def _trigger_emergency_alert(self, article: Dict):
        """ê¸´ê¸‰ ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        try:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                logger.info(f"ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ëœ ê¸´ê¸‰ ë‰´ìŠ¤ ìŠ¤í‚µ: {article.get('title', '')[:30]}...")
                return
            
            # ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¡œ ê¸°ë¡
            self.processed_news_hashes.add(content_hash)
            
            # ì˜¤ë˜ëœ í•´ì‹œ ì •ë¦¬ (1000ê°œ ì´ˆê³¼ì‹œ)
            if len(self.processed_news_hashes) > 1000:
                self.processed_news_hashes = set(list(self.processed_news_hashes)[-500:])
            
            event = {
                'type': 'critical_news',
                'title': article.get('title_ko', article.get('title', ''))[:100],
                'description': article.get('description', '')[:250],
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self._determine_impact(article),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown'),
                'published_at': article.get('published_at', '')
            }
            
            # ë°ì´í„° ì»¬ë ‰í„°ì— ì „ë‹¬
            if hasattr(self, 'data_collector') and self.data_collector:
                self.data_collector.events_buffer.append(event)
            
            logger.critical(f"ğŸš¨ ê¸´ê¸‰ ë‰´ìŠ¤ ì•Œë¦¼: {article.get('source', '')} - {article.get('title_ko', article.get('title', ''))[:60]}")
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _add_to_news_buffer(self, article: Dict):
        """ë‰´ìŠ¤ ë²„í¼ì— ì¶”ê°€ - íšŒì‚¬ë³„ ì¹´ìš´íŠ¸ ì œí•œ"""
        try:
            # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
            new_title = article.get('title', '').lower()
            new_title_ko = article.get('title_ko', '').lower()
            new_source = article.get('source', '').lower()
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                logger.debug(f"ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ ìŠ¤í‚µ: {new_title[:30]}...")
                return
            
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ í™•ì¸
            for company in self.important_companies:
                if company.lower() in new_title or company.lower() in new_title_ko:
                    # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
                    bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                    if any(keyword in new_title or keyword in new_title_ko for keyword in bitcoin_keywords):
                        # í•´ë‹¹ íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ê°€ ì´ë¯¸ 1ê°œ ì´ìƒì¸ì§€ í™•ì¸
                        if self.company_news_count.get(company.lower(), 0) >= 1:
                            logger.debug(f"ğŸ”„ {company} ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì´ë¯¸ ìˆìŒ, ìŠ¤í‚µ: {new_title[:30]}...")
                            return
            
            # ë²„í¼ì— ìˆëŠ” ë‰´ìŠ¤ì™€ ì¤‘ë³µ ì²´í¬
            is_duplicate = False
            for existing in self.news_buffer:
                # ë™ì¼í•œ ë‰´ìŠ¤ ì²´í¬
                if self._is_similar_news(new_title, existing.get('title', '')):
                    is_duplicate = True
                    break
                
                # í•œê¸€ ì œëª©ë„ ì²´í¬
                if new_title_ko and existing.get('title_ko', ''):
                    if self._is_similar_news(new_title_ko, existing.get('title_ko', '')):
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                self.news_buffer.append(article)
                self.processed_news_hashes.add(content_hash)
                
                # íšŒì‚¬ë³„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                for company in self.important_companies:
                    if company.lower() in new_title or company.lower() in new_title_ko:
                        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                        if any(keyword in new_title or keyword in new_title_ko for keyword in bitcoin_keywords):
                            self.company_news_count[company.lower()] = self.company_news_count.get(company.lower(), 0) + 1
                            logger.debug(f"ğŸ“Š {company} ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì¹´ìš´íŠ¸: {self.company_news_count[company.lower()]}")
                
                # ë²„í¼ ê´€ë¦¬: ê°€ì¤‘ì¹˜, ì¹´í…Œê³ ë¦¬, ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 50ê°œë§Œ ìœ ì§€
                if len(self.news_buffer) > 50:
                    def sort_key(x):
                        weight = x.get('weight', 0)
                        category_priority = {'crypto': 4, 'api': 3, 'finance': 2, 'news': 1, 'tech': 1}
                        cat_score = category_priority.get(x.get('category', ''), 0)
                        pub_time = x.get('published_at', '')
                        return (weight, cat_score, pub_time)
                    
                    self.news_buffer.sort(key=sort_key, reverse=True)
                    self.news_buffer = self.news_buffer[:50]
            else:
                logger.debug(f"ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ì œì™¸: {new_title_ko[:30] if new_title_ko else new_title[:30]}...")
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def _determine_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ ì˜í–¥ë„ íŒë‹¨ - ë” ì„¸ë°€í•œ ë¶„ì„"""
        content = (article.get('title', '') + ' ' + article.get('description', '') + ' ' + article.get('title_ko', '')).lower()
        
        # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ëŠ” ê°•í•œ í˜¸ì¬
        for company in self.important_companies:
            if company.lower() in content and any(word in content for word in ['bought', 'purchased', 'buys', 'bitcoin', 'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë§¤ì…']):
                return "â•ê°•í•œ í˜¸ì¬"
        
        # íŠ¸ëŸ¼í”„ ê´€ë ¨
        if 'trump' in content:
            if any(word in content for word in ['tariff', 'ban', 'restrict', 'court blocks', 'ê´€ì„¸', 'ê¸ˆì§€']):
                return "â–ì•…ì¬ ì˜ˆìƒ"  # íŠ¸ëŸ¼í”„ ì •ì±… ì°¨ë‹¨ì€ ì¼ë°˜ì ìœ¼ë¡œ ì‹œì¥ì— ë¶€ì •ì 
            elif any(word in content for word in ['approve', 'support', 'bitcoin reserve', 'ì§€ì§€', 'ìŠ¹ì¸']):
                return "â•í˜¸ì¬ ì˜ˆìƒ"
        
        # ê°•í•œ ì•…ì¬ (ì¦‰ì‹œ ë§¤ë„ ì‹ í˜¸)
        strong_bearish = ['ban', 'banned', 'lawsuit', 'crash', 'crackdown', 'reject', 'rejected', 'hack', 'hacked', 'ê¸ˆì§€', 'ê·œì œ', 'ì†Œì†¡', 'í­ë½', 'í•´í‚¹']
        # ê°•í•œ í˜¸ì¬ (ì¦‰ì‹œ ë§¤ìˆ˜ ì‹ í˜¸)
        strong_bullish = ['approval', 'approved', 'adoption', 'breakthrough', 'all-time high', 'ath', 'pump', 'ìŠ¹ì¸', 'ì±„íƒ', 'ì‹ ê³ ê°€', 'bought bitcoin', 'purchased bitcoin']
        # ì¼ë°˜ ì•…ì¬
        bearish = ['concern', 'worry', 'decline', 'fall', 'drop', 'uncertainty', 'regulation', 'fine', 'ìš°ë ¤', 'í•˜ë½', 'ë¶ˆí™•ì‹¤']
        # ì¼ë°˜ í˜¸ì¬
        bullish = ['growth', 'rise', 'increase', 'positive', 'rally', 'surge', 'investment', 'institutional', 'ìƒìŠ¹', 'ì¦ê°€', 'ê¸ì •ì ', 'íˆ¬ì']
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        strong_bearish_count = sum(2 for word in strong_bearish if word in content)  # ê°€ì¤‘ì¹˜ 2
        strong_bullish_count = sum(2 for word in strong_bullish if word in content)  # ê°€ì¤‘ì¹˜ 2
        bearish_count = sum(1 for word in bearish if word in content)
        bullish_count = sum(1 for word in bullish if word in content)
        
        bearish_total = strong_bearish_count + bearish_count
        bullish_total = strong_bullish_count + bullish_count
        
        # ì„¼í‹°ë¨¼íŠ¸ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° (Alpha Vantage)
        sentiment = article.get('sentiment', '').lower()
        if 'bearish' in sentiment:
            bearish_total += 1
        elif 'bullish' in sentiment:
            bullish_total += 1
        
        # ìµœì¢… íŒë‹¨
        if strong_bearish_count > 0:
            return "â–ê°•í•œ ì•…ì¬"
        elif strong_bullish_count > 0:
            return "â•ê°•í•œ í˜¸ì¬"
        elif bearish_total > bullish_total + 1:  # ëª…í™•í•œ ì°¨ì´
            return "â–ì•…ì¬ ì˜ˆìƒ"
        elif bullish_total > bearish_total + 1:  # ëª…í™•í•œ ì°¨ì´
            return "â•í˜¸ì¬ ì˜ˆìƒ"
        else:
            return "ì¤‘ë¦½"
    
    async def get_recent_news(self, hours: int = 6) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° - íšŒì‚¬ë³„ ì¤‘ë³µ ì œê±° ê°•í™”"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_news = []
            seen_titles = set()  # ì¤‘ë³µ ì²´í¬ìš©
            company_count = {}  # íšŒì‚¬ë³„ ì¹´ìš´íŠ¸
            
            for article in self.news_buffer:
                try:
                    # ë°œí–‰ ì‹œê°„ ì²´í¬
                    if article.get('published_at'):
                        pub_time_str = article.get('published_at', '').replace('Z', '').replace('T', ' ')
                        # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                        try:
                            if 'T' in article.get('published_at', ''):
                                pub_time = datetime.fromisoformat(pub_time_str)
                            else:
                                from dateutil import parser
                                pub_time = parser.parse(article.get('published_at', ''))
                            
                            if pub_time > cutoff_time:
                                # ì¤‘ë³µ ì²´í¬
                                title_hash = self._generate_content_hash(article.get('title', ''), '')
                                if title_hash not in seen_titles:
                                    # íšŒì‚¬ë³„ ì¹´ìš´íŠ¸ í™•ì¸
                                    skip = False
                                    article_title = (article.get('title', '') + ' ' + article.get('title_ko', '')).lower()
                                    
                                    for company in self.important_companies:
                                        if company.lower() in article_title:
                                            bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'purchase', 'bought', 'êµ¬ë§¤', 'ë§¤ì…']
                                            if any(keyword in article_title for keyword in bitcoin_keywords):
                                                if company_count.get(company.lower(), 0) >= 1:
                                                    skip = True
                                                    break
                                                else:
                                                    company_count[company.lower()] = company_count.get(company.lower(), 0) + 1
                                    
                                    if not skip:
                                        recent_news.append(article)
                                        seen_titles.add(title_hash)
                        except:
                            # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ì‹œ ìµœê·¼ ë‰´ìŠ¤ë¡œ ê°„ì£¼ (ì•ˆì „ì¥ì¹˜)
                            title_hash = self._generate_content_hash(article.get('title', ''), '')
                            if title_hash not in seen_titles:
                                recent_news.append(article)
                                seen_titles.add(title_hash)
                    else:
                        title_hash = self._generate_content_hash(article.get('title', ''), '')
                        if title_hash not in seen_titles:
                            recent_news.append(article)
                            seen_titles.add(title_hash)
                except:
                    pass
            
            # ì¶”ê°€ ì¤‘ë³µ ì œê±°: ìœ ì‚¬í•œ ì œëª© ì œê±°
            final_news = []
            for article in recent_news:
                is_similar = False
                for final_article in final_news:
                    if self._is_similar_news(article.get('title', ''), final_article.get('title', '')):
                        is_similar = True
                        break
                
                if not is_similar:
                    final_news.append(article)
            
            # ì •ë ¬ ê¸°ì¤€: ê°€ì¤‘ì¹˜ â†’ ì¹´í…Œê³ ë¦¬ â†’ ì‹œê°„
            def sort_key(x):
                weight = x.get('weight', 0)
                category_priority = {'crypto': 4, 'api': 3, 'finance': 2, 'news': 1, 'tech': 1}
                cat_score = category_priority.get(x.get('category', ''), 0)
                pub_time = x.get('published_at', '')
                return (weight, cat_score, pub_time)
            
            final_news.sort(key=sort_key, reverse=True)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ê· í˜• ì¡°ì • (ì•”í˜¸í™”í ë‰´ìŠ¤ ìš°ì„ , í•˜ì§€ë§Œ ë‹¤ì–‘ì„± ìœ ì§€)
            balanced_news = []
            crypto_count = 0
            other_count = 0
            
            for article in final_news:
                category = article.get('category', '')
                if category == 'crypto' and crypto_count < 8:
                    balanced_news.append(article)
                    crypto_count += 1
                elif category != 'crypto' and other_count < 4:
                    balanced_news.append(article)
                    other_count += 1
                elif len(balanced_news) < 10:  # ì´ 10ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€
                    balanced_news.append(article)
            
            final_result = balanced_news[:12]  # ìµœëŒ€ 12ê°œ
            
            logger.info(f"ğŸ“° ìµœê·¼ {hours}ì‹œê°„ ë‰´ìŠ¤ ë°˜í™˜: ì´ {len(final_result)}ê±´ (ì•”í˜¸í™”í: {crypto_count}, ê¸°íƒ€: {other_count})")
            return final_result
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self.session:
                await self.session.close()
                logger.info("ğŸ”š ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

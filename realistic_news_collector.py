import aiohttp
import asyncio
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional, Set
import pytz
from bs4 import BeautifulSoup
import feedparser
import openai
import os
import hashlib
import re
import json

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    def __init__(self, config):
        self.config = config
        self.session = None
        self.news_buffer = []
        self.emergency_alerts_sent = {}  # ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€ìš©
        self.processed_news_hashes = set()  # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ ì €ì¥
        self.news_title_cache = {}  # ì œëª©ë³„ ìºì‹œ
        self.company_news_count = {}  # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸
        self.news_first_seen = {}  # ë‰´ìŠ¤ ìµœì´ˆ ë°œê²¬ ì‹œê°„
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.persistence_file = 'news_duplicates.json'
        
        # ì „ì†¡ëœ ë‰´ìŠ¤ ì œëª© ìºì‹œ (ì¤‘ë³µ ë°©ì§€ ê°•í™”) - ì´ˆê¸°í™”
        self.sent_news_titles = {}
        
        # ğŸ”¥ğŸ”¥ Claude API ìš°ì„  ì‚¬ìš©, GPTëŠ” ë°±ì—…ìš©
        self.translation_cache = {}  # ë²ˆì—­ ìºì‹œ
        self.claude_translation_count = 0  # Claude ë²ˆì—­ íšŸìˆ˜
        self.gpt_translation_count = 0  # GPT ë²ˆì—­ íšŸìˆ˜ 
        self.last_translation_reset = datetime.now()
        self.max_claude_translations_per_15min = 100  # ClaudeëŠ” ë” ë§ì´ ì‚¬ìš© ê°€ëŠ¥
        self.max_gpt_translations_per_15min = 10  # GPTëŠ” ë°±ì—…ìš©ìœ¼ë¡œë§Œ
        self.translation_reset_interval = 900  # 15ë¶„
        
        # Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.anthropic_client = None
        if hasattr(config, 'ANTHROPIC_API_KEY') and config.ANTHROPIC_API_KEY:
            try:
                import anthropic
                self.anthropic_client = anthropic.AsyncAnthropic(api_key=config.ANTHROPIC_API_KEY)
                logger.info("âœ… Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                logger.warning("âŒ anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: pip install anthropic")
            except Exception as e:
                logger.warning(f"Claude API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë°±ì—…ìš©)
        self.openai_client = None
        if hasattr(config, 'OPENAI_API_KEY') and config.OPENAI_API_KEY:
            self.openai_client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        
        # GPT ìš”ì•½ ì‚¬ìš©ëŸ‰ ì œí•œ ì¶”ê°€
        self.summary_count = 0
        self.max_summaries_per_15min = 30  # 15ê°œì—ì„œ 30ê°œë¡œ ì¦ê°€
        self.last_summary_reset = datetime.now()
        
        # ëª¨ë“  API í‚¤ë“¤
        self.newsapi_key = getattr(config, 'NEWSAPI_KEY', None)
        self.newsdata_key = getattr(config, 'NEWSDATA_KEY', None)
        self.alpha_vantage_key = getattr(config, 'ALPHA_VANTAGE_KEY', None)
        
        # ğŸ”¥ğŸ”¥ ê°•í™”ëœ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ (ë” í¬ê´„ì ì´ê³  ë¯¼ê°í•˜ê²Œ)
        self.critical_keywords = [
            # ë¹„íŠ¸ì½”ì¸ ETF ê´€ë ¨ (ìµœìš°ì„ )
            'bitcoin etf approved', 'bitcoin etf rejected', 'spot bitcoin etf', 'etf decision',
            'blackrock bitcoin etf', 'fidelity bitcoin etf', 'ark bitcoin etf', 'grayscale bitcoin etf',
            'SEC ë¹„íŠ¸ì½”ì¸ ETF', 'ETF ìŠ¹ì¸', 'ETF ê±°ë¶€', 'SEC approves bitcoin', 'SEC rejects bitcoin',
            
            # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ (ì§ì ‘ì )
            'tesla bought bitcoin', 'microstrategy bought bitcoin', 'bought bitcoin', 'buys bitcoin',
            'gamestop bitcoin purchase', 'metaplanet bitcoin', 'corporate bitcoin purchase',
            'bitcoin acquisition', 'adds bitcoin', 'bitcoin investment', 'purchases bitcoin',
            'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë¹„íŠ¸ì½”ì¸ ë§¤ì…', 'BTC êµ¬ë§¤', 'BTC ë§¤ì…', 'bitcoin holdings',
            
            # êµ­ê°€/ì€í–‰ ì±„íƒ
            'central bank bitcoin', 'russia bitcoin', 'sberbank bitcoin', 'bitcoin bonds',
            'government bitcoin', 'country adopts bitcoin', 'bitcoin legal tender',
            'ì¤‘ì•™ì€í–‰ ë¹„íŠ¸ì½”ì¸', 'ëŸ¬ì‹œì•„ ë¹„íŠ¸ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸ ì±„ê¶Œ', 'el salvador bitcoin',
            'putin bitcoin', 'russia legalize bitcoin', 'china bitcoin ban lifted',
            
            # ë¹„íŠ¸ì½”ì¸ ê·œì œ (ì§ì ‘ì )
            'sec bitcoin lawsuit', 'bitcoin ban', 'bitcoin regulation', 'bitcoin lawsuit',
            'china bans bitcoin', 'government bans bitcoin', 'court bitcoin', 'biden bitcoin',
            'regulatory approval bitcoin', 'regulatory rejection bitcoin', 'trump bitcoin',
            'SEC ë¹„íŠ¸ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸ ê¸ˆì§€', 'ë¹„íŠ¸ì½”ì¸ ê·œì œ', 'coinbase lawsuit',
            
            # ë¹„íŠ¸ì½”ì¸ ì‹œì¥ ê¸‰ë³€ë™
            'bitcoin crash', 'bitcoin surge', 'bitcoin breaks', 'bitcoin plunge',
            'bitcoin all time high', 'bitcoin ath', 'bitcoin tumbles', 'bitcoin soars',
            'ë¹„íŠ¸ì½”ì¸ í­ë½', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë½', 'bitcoin reaches',
            'bitcoin hits', 'bitcoin falls below', 'bitcoin crosses',
            
            # ëŒ€ëŸ‰ ë¹„íŠ¸ì½”ì¸ ì´ë™
            'whale alert bitcoin', 'large bitcoin transfer', 'bitcoin moved exchange',
            'massive bitcoin', 'billion bitcoin', 'btc whale', 'bitcoin outflow',
            'ê³ ë˜ ë¹„íŠ¸ì½”ì¸', 'ëŒ€ëŸ‰ ë¹„íŠ¸ì½”ì¸', 'BTC ì´ë™', 'satoshi nakamoto',
            
            # ë¹„íŠ¸ì½”ì¸ í•´í‚¹/ë³´ì•ˆ
            'bitcoin stolen', 'bitcoin hack', 'exchange hacked bitcoin',
            'bitcoin security breach', 'btc stolen', 'binance hack', 'coinbase hack',
            'ë¹„íŠ¸ì½”ì¸ ë„ë‚œ', 'ë¹„íŠ¸ì½”ì¸ í•´í‚¹', 'ê±°ë˜ì†Œ í•´í‚¹', 'mt gox',
            
            # Fed ê¸ˆë¦¬ ê²°ì • (ë¹„íŠ¸ì½”ì¸ ì˜í–¥) - ê°•í™”
            'fed rate decision', 'fomc decision', 'powell speech', 'interest rate decision',
            'federal reserve meeting', 'fed minutes', 'inflation report', 'cpi data',
            'ì—°ì¤€ ê¸ˆë¦¬', 'ê¸°ì¤€ê¸ˆë¦¬', 'í†µí™”ì •ì±…', 'jobless claims', 'unemployment rate',
            
            # ê±°ì‹œê²½ì œ ì˜í–¥ (ê°•í™”)
            'us economic policy', 'treasury secretary', 'inflation data', 'cpi report',
            'unemployment rate', 'gdp growth', 'recession fears', 'economic stimulus',
            'quantitative easing', 'dollar strength', 'dollar weakness', 'dxy index',
            'ë‹¬ëŸ¬ ê°•ì„¸', 'ë‹¬ëŸ¬ ì•½ì„¸', 'ì¸í”Œë ˆì´ì…˜', 'ê²½ê¸°ì¹¨ì²´', 'china economic data',
            
            # ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ (ê°•í™”)
            'ukraine war', 'russia sanctions', 'north korea sanctions', 'iran sanctions',
            'china us tensions', 'taiwan conflict', 'middle east conflict', 'israel iran',
            'energy crisis', 'oil price surge', 'natural gas crisis', 'europe energy',
            'ì§€ì •í•™ì  ë¦¬ìŠ¤í¬', 'ì œì¬', 'ë¶„ìŸ', 'gaza conflict', 'russia ukraine',
            
            # ë¯¸êµ­ ê´€ì„¸ ë° ë¬´ì—­ (ê°•í™”)
            'trump tariffs', 'china tariffs', 'trade war', 'trade deal', 'trade agreement',
            'customs duties', 'import tariffs', 'export restrictions', 'trade negotiations',
            'trade talks deadline', 'tariff exemption', 'tariff extension', 'wto ruling',
            'ê´€ì„¸', 'ë¬´ì—­í˜‘ìƒ', 'ë¬´ì—­ì „ìŸ', 'ë¬´ì—­í•©ì˜', 'usmca agreement',
            
            # ì•”í˜¸í™”í ê±°ë˜ì†Œ/ì¸í”„ë¼
            'coinbase public', 'binance regulation', 'kraken ipo', 'crypto exchange hack',
            'tether audit', 'usdc regulation', 'defi hack', 'crypto mining ban',
            'ì•”í˜¸í™”í ê±°ë˜ì†Œ', 'í…Œë”', 'CBDC', 'digital dollar',
            
            # ê¸°ê´€ íˆ¬ìì ì§„ì…
            'institutional adoption', 'pension fund bitcoin', 'insurance company bitcoin',
            'bank crypto custody', 'goldman sachs bitcoin', 'jpmorgan bitcoin',
            'ê¸°ê´€ íˆ¬ìì', 'ì—°ê¸°ê¸ˆ', 'ë³´í—˜ì‚¬', 'sovereign wealth fund',
            
            # ê¸°ìˆ ì  ì´ìŠˆ
            'bitcoin mining', 'bitcoin halving', 'lightning network', 'bitcoin fork',
            'bitcoin upgrade', 'taproot activation', 'mining difficulty', 'hash rate',
            'ë¹„íŠ¸ì½”ì¸ ë°˜ê°ê¸°', 'ì±„êµ´', 'í•´ì‹œë ˆì´íŠ¸', 'proof of work'
        ]
        
        # ì œì™¸ í‚¤ì›Œë“œ (ë¹„íŠ¸ì½”ì¸ê³¼ ë¬´ê´€í•œ ê²ƒë“¤) - ê°•í™”
        self.exclude_keywords = [
            'how to mine', 'ì§‘ì—ì„œ ì±„êµ´', 'mining at home', 'mining tutorial',
            'price prediction tutorial', 'ê°€ê²© ì˜ˆì¸¡ ë°©ë²•', 'technical analysis tutorial',
            'altcoin only', 'ethereum only', 'ripple only', 'cardano only', 'solana only', 
            'dogecoin only', 'shiba only', 'nft only', 'web3 only', 'metaverse only',
            'defi only', 'gamefi only', 'celebrity news', 'entertainment only',
            'sports only', 'weather', 'local news', 'obituary', 'wedding',
            'movie review', 'book review', 'restaurant review', 'travel guide'
        ]
        
        # ì¤‘ìš” ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (ë¹„íŠ¸ì½”ì¸ ë³´ìœ /ê´€ë ¨) - í™•ì¥
        self.important_companies = [
            'tesla', 'microstrategy', 'square', 'block', 'paypal', 'mastercard', 'visa',
            'gamestop', 'gme', 'blackrock', 'fidelity', 'ark invest', 'grayscale',
            'coinbase', 'binance', 'kraken', 'bitget', 'okx', 'bybit',
            'metaplanet', 'ë©”íƒ€í”Œë˜ë‹›', 'í…ŒìŠ¬ë¼', 'ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€',
            'sberbank', 'ìŠ¤ë² ë¥´ë°©í¬', 'jpmorgan', 'goldman sachs', 'morgan stanley',
            'nvidia', 'amd', 'intel', 'apple', 'microsoft', 'amazon',
            'ì‚¼ì„±', 'samsung', 'lg', 'sk', 'hyundai'
        ]
        
        # ğŸ”¥ğŸ”¥ í˜„ì‹¤ì ì¸ ê³¼ê±° ë‰´ìŠ¤ ì˜í–¥ íŒ¨í„´ (ë” ì •êµí•˜ê²Œ - ì‹¤ì œ ì‹œì¥ ë°˜ì‘ ê¸°ë°˜)
        self.historical_patterns = {
            # ETF ê´€ë ¨ (ê°€ì¥ í° ì˜í–¥)
            'etf_approval': {'avg_impact': 3.5, 'duration_hours': 24, 'confidence': 0.95},
            'etf_rejection': {'avg_impact': -2.8, 'duration_hours': 12, 'confidence': 0.9},
            'etf_filing': {'avg_impact': 0.8, 'duration_hours': 6, 'confidence': 0.7},
            
            # ê¸°ì—… êµ¬ë§¤ (ê·œëª¨ë³„)
            'tesla_purchase': {'avg_impact': 2.2, 'duration_hours': 18, 'confidence': 0.9},
            'microstrategy_purchase': {'avg_impact': 0.7, 'duration_hours': 8, 'confidence': 0.85},
            'large_corp_purchase': {'avg_impact': 1.2, 'duration_hours': 12, 'confidence': 0.8},
            'small_corp_purchase': {'avg_impact': 0.3, 'duration_hours': 4, 'confidence': 0.6},
            
            # ê·œì œ ê´€ë ¨
            'sec_lawsuit': {'avg_impact': -1.5, 'duration_hours': 8, 'confidence': 0.8},
            'china_ban': {'avg_impact': -4.2, 'duration_hours': 24, 'confidence': 0.85},
            'regulatory_clarity': {'avg_impact': 1.8, 'duration_hours': 12, 'confidence': 0.75},
            
            # ê±°ì‹œê²½ì œ (Fed ê´€ë ¨)
            'fed_rate_hike': {'avg_impact': -1.2, 'duration_hours': 6, 'confidence': 0.7},
            'fed_rate_cut': {'avg_impact': 1.5, 'duration_hours': 8, 'confidence': 0.75},
            'fed_dovish': {'avg_impact': 0.8, 'duration_hours': 4, 'confidence': 0.6},
            'fed_hawkish': {'avg_impact': -0.6, 'duration_hours': 4, 'confidence': 0.6},
            
            # ì¸í”Œë ˆì´ì…˜/ê²½ì œì§€í‘œ
            'high_inflation': {'avg_impact': 1.2, 'duration_hours': 6, 'confidence': 0.65},
            'low_inflation': {'avg_impact': -0.4, 'duration_hours': 4, 'confidence': 0.55},
            'recession_fears': {'avg_impact': 0.8, 'duration_hours': 8, 'confidence': 0.6},
            'strong_jobs': {'avg_impact': -0.3, 'duration_hours': 3, 'confidence': 0.5},
            
            # ì§€ì •í•™ì  ë¦¬ìŠ¤í¬
            'war_escalation': {'avg_impact': 1.5, 'duration_hours': 12, 'confidence': 0.7},
            'peace_talks': {'avg_impact': -0.5, 'duration_hours': 6, 'confidence': 0.55},
            'sanctions': {'avg_impact': 0.8, 'duration_hours': 8, 'confidence': 0.6},
            
            # ë¬´ì—­/ê´€ì„¸
            'new_tariffs': {'avg_impact': -0.8, 'duration_hours': 6, 'confidence': 0.65},
            'trade_deal': {'avg_impact': 0.6, 'duration_hours': 8, 'confidence': 0.7},
            
            # ê¸°ìˆ ì /ë³´ì•ˆ ì´ìŠˆ
            'major_hack': {'avg_impact': -2.2, 'duration_hours': 8, 'confidence': 0.8},
            'minor_hack': {'avg_impact': -0.4, 'duration_hours': 3, 'confidence': 0.6},
            'upgrade_news': {'avg_impact': 0.3, 'duration_hours': 4, 'confidence': 0.5},
            
            # ì±„êµ´/ì¸í”„ë¼
            'mining_ban': {'avg_impact': -1.8, 'duration_hours': 12, 'confidence': 0.75},
            'mining_support': {'avg_impact': 0.5, 'duration_hours': 6, 'confidence': 0.6},
            'halving_approach': {'avg_impact': 0.4, 'duration_hours': 8, 'confidence': 0.65},
            
            # ê¸°ê´€/ì€í–‰ ê´€ë ¨
            'bank_adoption': {'avg_impact': 1.0, 'duration_hours': 10, 'confidence': 0.75},
            'bank_restriction': {'avg_impact': -0.8, 'duration_hours': 6, 'confidence': 0.7},
            'pension_entry': {'avg_impact': 0.8, 'duration_hours': 8, 'confidence': 0.7}
        }
        
        # RSS í”¼ë“œ - ë” ë§ì€ ì†ŒìŠ¤ ì¶”ê°€
        self.rss_feeds = [
            # ì•”í˜¸í™”í ì „ë¬¸ (ìµœìš°ì„ )
            {'url': 'https://cointelegraph.com/rss', 'source': 'Cointelegraph', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'source': 'CoinDesk', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://decrypt.co/feed', 'source': 'Decrypt', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://bitcoinmagazine.com/.rss/full/', 'source': 'Bitcoin Magazine', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://cryptopotato.com/feed/', 'source': 'CryptoPotato', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://u.today/rss', 'source': 'U.Today', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://ambcrypto.com/feed/', 'source': 'AMBCrypto', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://cryptonews.com/news/feed/', 'source': 'Cryptonews', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://www.watcher.guru/news/feed', 'source': 'Watcher.Guru', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://cryptoslate.com/feed/', 'source': 'CryptoSlate', 'weight': 8, 'category': 'crypto'},
            
            # ê¸ˆìœµ (Fed/ê·œì œ ê´€ë ¨) - í™•ì¥
            {'url': 'https://feeds.bloomberg.com/markets/news.rss', 'source': 'Bloomberg Markets', 'weight': 9, 'category': 'finance'},
            {'url': 'https://feeds.bloomberg.com/economics/news.rss', 'source': 'Bloomberg Economics', 'weight': 9, 'category': 'finance'},
            {'url': 'https://www.marketwatch.com/rss/topstories', 'source': 'MarketWatch', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.reuters.com/reuters/businessNews', 'source': 'Reuters Business', 'weight': 8, 'category': 'news'},
            {'url': 'https://feeds.cnbc.com/cnbc/ID/100003114/device/rss/rss.html', 'source': 'CNBC Markets', 'weight': 8, 'category': 'finance'},
            {'url': 'https://www.ft.com/rss/home/us', 'source': 'Financial Times', 'weight': 9, 'category': 'finance'},
            
            # ê¸°ìˆ  ë‰´ìŠ¤
            {'url': 'https://techcrunch.com/feed/', 'source': 'TechCrunch', 'weight': 7, 'category': 'tech'},
            {'url': 'https://www.wired.com/feed/rss', 'source': 'Wired', 'weight': 7, 'category': 'tech'}
        ]
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.api_usage = {
            'newsapi_today': 0,
            'newsdata_today': 0,
            'alpha_vantage_today': 0,
            'last_reset': datetime.now().date()
        }
        
        # API ì¼ì¼ í•œë„ ì¦ê°€
        self.api_limits = {
            'newsapi': 50,  # 20 â†’ 50
            'newsdata': 25,  # 10 â†’ 25
            'alpha_vantage': 5   # 2 â†’ 5
        }
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ
        self._load_duplicate_data()
        
        logger.info(f"ğŸ”¥ğŸ”¥ Claude ìš°ì„  ë²ˆì—­ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ¤– Claude API: {'í™œì„±í™”' if self.anthropic_client else 'ë¹„í™œì„±í™”'} (15ë¶„ë‹¹ {self.max_claude_translations_per_15min}ê°œ)")
        logger.info(f"ğŸ§  GPT API: {'í™œì„±í™”' if self.openai_client else 'ë¹„í™œì„±í™”'} (ë°±ì—…ìš© 15ë¶„ë‹¹ {self.max_gpt_translations_per_15min}ê°œ)")
        logger.info(f"ğŸ“Š ì„¤ì •: RSS 5ì´ˆ ì²´í¬ (ë¹ ë¥¸ ê°ì§€), ìš”ì•½ 15ë¶„ë‹¹ {self.max_summaries_per_15min}ê°œ")
        logger.info(f"ğŸ¯ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ: {len(self.critical_keywords)}ê°œ (ëŒ€í­ í™•ì¥)")
        logger.info(f"ğŸ¢ ì¶”ì  ê¸°ì—…: {len(self.important_companies)}ê°œ")
        logger.info(f"ğŸ“ˆ ê°€ê²© íŒ¨í„´: {len(self.historical_patterns)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        logger.info(f"ğŸ“¡ RSS ì†ŒìŠ¤: {len(self.rss_feeds)}ê°œ (í™•ì¥)")
        logger.info(f"ğŸ’¾ ì¤‘ë³µ ë°©ì§€: ì²˜ë¦¬ëœ ë‰´ìŠ¤ {len(self.processed_news_hashes)}ê°œ")
    
    def _load_duplicate_data(self):
        """ì¤‘ë³µ ë°©ì§€ ë°ì´í„° íŒŒì¼ì—ì„œ ë¡œë“œ"""
        try:
            if os.path.exists(self.persistence_file):
                with open(self.persistence_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ ë¡œë“œ
                self.processed_news_hashes = set(data.get('processed_news_hashes', []))
                
                # ê¸´ê¸‰ ì•Œë¦¼ ë°ì´í„° ë¡œë“œ (ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜)
                emergency_data = data.get('emergency_alerts_sent', {})
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=12)  # 12ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ ìœ ì§€
                
                for hash_key, time_str in emergency_data.items():
                    try:
                        alert_time = datetime.fromisoformat(time_str)
                        if alert_time > cutoff_time:  # 12ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ ìœ ì§€
                            self.emergency_alerts_sent[hash_key] = alert_time
                    except:
                        continue
                
                # ë‰´ìŠ¤ ì œëª© ìºì‹œ ë¡œë“œ
                title_data = data.get('sent_news_titles', {})
                cutoff_time = current_time - timedelta(hours=3)  # 3ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ ìœ ì§€
                
                for title_hash, time_str in title_data.items():
                    try:
                        sent_time = datetime.fromisoformat(time_str)
                        if sent_time > cutoff_time:  # 3ì‹œê°„ ì´ë‚´ ë°ì´í„°ë§Œ ìœ ì§€
                            self.sent_news_titles[title_hash] = sent_time
                    except:
                        continue
                
                # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 3000ê°œ)
                if len(self.processed_news_hashes) > 3000:
                    self.processed_news_hashes = set(list(self.processed_news_hashes)[-1500:])
                
                logger.info(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì²˜ë¦¬ëœ ë‰´ìŠ¤ {len(self.processed_news_hashes)}ê°œ")
                
        except Exception as e:
            logger.warning(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            self.processed_news_hashes = set()
            self.emergency_alerts_sent = {}
            self.sent_news_titles = {}
    
    def _save_duplicate_data(self):
        """ì¤‘ë³µ ë°©ì§€ ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            emergency_data = {}
            for hash_key, alert_time in self.emergency_alerts_sent.items():
                emergency_data[hash_key] = alert_time.isoformat()
            
            title_data = {}
            for title_hash, sent_time in self.sent_news_titles.items():
                title_data[title_hash] = sent_time.isoformat()
            
            data = {
                'processed_news_hashes': list(self.processed_news_hashes),
                'emergency_alerts_sent': emergency_data,
                'sent_news_titles': title_data,
                'last_updated': datetime.now().isoformat()
            }
            
            with open(self.persistence_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(self.processed_news_hashes)}ê°œ í•´ì‹œ")
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _reset_translation_count_if_needed(self):
        """í•„ìš”ì‹œ ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        now = datetime.now()
        if (now - self.last_translation_reset).total_seconds() > self.translation_reset_interval:
            old_claude_count = self.claude_translation_count
            old_gpt_count = self.gpt_translation_count
            self.claude_translation_count = 0
            self.gpt_translation_count = 0
            self.last_translation_reset = now
            if old_claude_count > 0 or old_gpt_count > 0:
                logger.info(f"ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹: Claude {old_claude_count} â†’ 0, GPT {old_gpt_count} â†’ 0")
    
    def _reset_summary_count_if_needed(self):
        """í•„ìš”ì‹œ ìš”ì•½ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        now = datetime.now()
        if (now - self.last_summary_reset).total_seconds() > self.translation_reset_interval:
            old_count = self.summary_count
            self.summary_count = 0
            self.last_summary_reset = now
            if old_count > 0:
                logger.info(f"ìš”ì•½ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {old_count} â†’ 0 (15ë¶„ë‹¹ {self.max_summaries_per_15min}ê°œ ì œí•œ)")
    
    def _should_translate(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ ë²ˆì—­ ëŒ€ìƒì„ ì¢€ ë” ê´€ëŒ€í•˜ê²Œ - ClaudeëŠ” ë” ë§ì´ ì‚¬ìš© ê°€ëŠ¥"""
        # ì´ë¯¸ í•œê¸€ ì œëª©ì´ ìˆìœ¼ë©´ ë²ˆì—­ ë¶ˆí•„ìš”
        if article.get('title_ko') and article['title_ko'] != article.get('title', ''):
            return False
        
        # ğŸ”¥ğŸ”¥ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ëŠ” ë²ˆì—­ (weight >= 8ë¡œ ë‚®ì¶¤)
        weight = article.get('weight', 0)
        if weight < 8:
            return False
        
        # ğŸ”¥ğŸ”¥ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ì´ë©´ì„œ ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ë§Œ
        if not self._is_critical_news(article):
            return False
        
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ ë˜ëŠ” ì¤‘ìš” ê²½ì œ í‚¤ì›Œë“œê°€ ìˆì–´ì•¼ í•¨
        important_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'fed', 'tariff', 'inflation', 'etf', 
                             'tesla', 'microstrategy', 'sec', 'regulation']
        if not any(keyword in content for keyword in important_keywords):
            return False
        
        return True
    
    def _should_use_gpt_summary(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ GPT ìš”ì•½ ì‚¬ìš© ì—¬ë¶€ ê²°ì • - ë” ê´€ëŒ€í•˜ê²Œ"""
        # ìš”ì•½ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ì²´í¬
        self._reset_summary_count_if_needed()
        
        # Rate limit ì²´í¬
        if self.summary_count >= self.max_summaries_per_15min:
            return False
        
        # weight >= 9ì´ë©´ì„œ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ë§Œ
        if article.get('weight', 0) < 9:
            return False
        
        if not self._is_critical_news_enhanced(article):
            return False
        
        # descriptionì´ ì¶©ë¶„íˆ ê¸¸ì–´ì•¼ í•¨ (ìš”ì•½í•  ê°€ì¹˜ê°€ ìˆì–´ì•¼ í•¨)
        description = article.get('description', '')
        if len(description) < 200:  # 300ìì—ì„œ 200ìë¡œ ë‚®ì¶¤
            return False
        
        return True
    
    async def translate_text_with_claude(self, text: str, max_length: int = 400) -> str:
        """ğŸ”¥ğŸ”¥ Claude APIë¥¼ ì‚¬ìš©í•œ ë²ˆì—­"""
        if not self.anthropic_client:
            return text
        
        # ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ì²´í¬
        self._reset_translation_count_if_needed()
        
        # ìºì‹œ í™•ì¸
        cache_key = f"claude_{hashlib.md5(text.encode()).hexdigest()}"
        if cache_key in self.translation_cache:
            logger.debug(f"ğŸ”„ Claude ë²ˆì—­ ìºì‹œ íˆíŠ¸")
            return self.translation_cache[cache_key]
        
        # Claude Rate limit ì²´í¬
        if self.claude_translation_count >= self.max_claude_translations_per_15min:
            logger.warning(f"Claude ë²ˆì—­ í•œë„ ì´ˆê³¼: {self.claude_translation_count}/{self.max_claude_translations_per_15min}")
            return text
        
        try:
            response = await self.anthropic_client.messages.create(
                model="claude-3-5-haiku-20241022",  # ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
                max_tokens=200,
                messages=[{
                    "role": "user", 
                    "content": f"""ë‹¤ìŒ ì˜ë¬¸ ë‰´ìŠ¤ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì „ë¬¸ ìš©ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë²ˆì—­í•˜ì„¸ìš”:

- Bitcoin/BTC â†’ ë¹„íŠ¸ì½”ì¸
- ETF â†’ ETF
- Tesla â†’ í…ŒìŠ¬ë¼
- MicroStrategy â†’ ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€
- SEC â†’ SEC
- Fed/Federal Reserve â†’ ì—°ì¤€
- Trump â†’ íŠ¸ëŸ¼í”„
- China â†’ ì¤‘êµ­
- Russia â†’ ëŸ¬ì‹œì•„
- tariffs â†’ ê´€ì„¸

ìµœëŒ€ {max_length}ì ì´ë‚´ë¡œ ë²ˆì—­í•˜ë˜, ì˜ë¯¸ê°€ ëª…í™•í•˜ê²Œ ì „ë‹¬ë˜ë„ë¡ í•´ì£¼ì„¸ìš”.

ì œëª©: {text}"""
                }]
            )
            
            translated = response.content[0].text.strip()
            
            # ê¸¸ì´ ì²´í¬
            if len(translated) > max_length:
                sentences = translated.split('.')
                result = ""
                for sentence in sentences:
                    if len(result + sentence + ".") <= max_length - 3:
                        result += sentence + "."
                    else:
                        break
                translated = result.strip()
                if not translated:
                    translated = translated[:max_length-3] + "..."
            
            # ìºì‹œ ì €ì¥ ë° ì¹´ìš´íŠ¸ ì¦ê°€
            self.translation_cache[cache_key] = translated
            self.claude_translation_count += 1
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.translation_cache) > 500:
                keys_to_remove = list(self.translation_cache.keys())[:250]
                for key in keys_to_remove:
                    del self.translation_cache[key]
            
            logger.info(f"ğŸ¤– Claude ë²ˆì—­ ì™„ë£Œ ({self.claude_translation_count}/{self.max_claude_translations_per_15min})")
            return translated
            
        except Exception as e:
            logger.warning(f"Claude ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:50]} - GPT ë°±ì—… ì‹œë„")
            return await self.translate_text_with_gpt(text, max_length)
    
    async def translate_text_with_gpt(self, text: str, max_length: int = 400) -> str:
        """ğŸ”¥ğŸ”¥ GPT APIë¥¼ ì‚¬ìš©í•œ ë°±ì—… ë²ˆì—­"""
        if not self.openai_client:
            return text
        
        # GPT Rate limit ì²´í¬
        if self.gpt_translation_count >= self.max_gpt_translations_per_15min:
            logger.warning(f"GPT ë²ˆì—­ í•œë„ ì´ˆê³¼: {self.gpt_translation_count}/{self.max_gpt_translations_per_15min}")
            return text
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë¹„íŠ¸ì½”ì¸ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì˜ë¬¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ìµœëŒ€ {max_length}ì):\n\n{text}"}
                ],
                max_tokens=150,
                temperature=0.2
            )
            
            translated = response.choices[0].message.content.strip()
            
            # ê¸¸ì´ ì²´í¬
            if len(translated) > max_length:
                translated = translated[:max_length-3] + "..."
            
            self.gpt_translation_count += 1
            logger.info(f"ğŸ§  GPT ë°±ì—… ë²ˆì—­ ì™„ë£Œ ({self.gpt_translation_count}/{self.max_gpt_translations_per_15min})")
            return translated
            
        except Exception as e:
            logger.warning(f"GPT ë²ˆì—­ë„ ì‹¤íŒ¨: {str(e)[:50]}")
            return text
    
    async def translate_text(self, text: str, max_length: int = 400) -> str:
        """ğŸ”¥ğŸ”¥ í†µí•© ë²ˆì—­ í•¨ìˆ˜ - Claude ìš°ì„ , GPT ë°±ì—…"""
        if self.anthropic_client:
            return await self.translate_text_with_claude(text, max_length)
        elif self.openai_client:
            return await self.translate_text_with_gpt(text, max_length)
        else:
            return text
    
    def _generate_content_hash(self, title: str, description: str = "") -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì˜ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©) - ë” ì—„ê²©í•˜ê²Œ"""
        # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        content = f"{title} {description[:200]}".lower()
        
        # ìˆ«ì ì •ê·œí™” (580,955 -> 580955)
        content = re.sub(r'[\d,]+', lambda m: m.group(0).replace(',', ''), content)
        
        # íšŒì‚¬ëª… ì •ê·œí™”
        companies_found = []
        for company in self.important_companies:
            if company.lower() in content:
                companies_found.append(company.lower())
        
        # ì•¡ì…˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        action_keywords = []
        actions = ['bought', 'purchased', 'acquired', 'adds', 'buys', 'sells', 'sold', 
                  'announced', 'launches', 'approves', 'rejects', 'bans', 'raises', 'cuts']
        for action in actions:
            if action in content:
                action_keywords.append(action)
        
        # BTC ìˆ˜ëŸ‰ ì¶”ì¶œ
        btc_amounts = re.findall(r'(\d+(?:,\d+)*)\s*(?:btc|bitcoin)', content)
        
        # ê³ ìœ  ì‹ë³„ì ìƒì„±
        unique_parts = []
        if companies_found:
            unique_parts.append('_'.join(sorted(companies_found)))
        if action_keywords:
            unique_parts.append('_'.join(sorted(action_keywords)))
        if btc_amounts:
            unique_parts.append('_'.join(btc_amounts))
        
        # í•´ì‹œ ìƒì„±
        if unique_parts:
            hash_content = '|'.join(unique_parts)
        else:
            # í•µì‹¬ ë‹¨ì–´ë§Œ ì¶”ì¶œ
            words = re.findall(r'\b[a-z]{4,}\b', content)
            important_words = [w for w in words if w not in ['that', 'this', 'with', 'from', 'have', 'been', 'their', 'about']]
            hash_content = ' '.join(sorted(important_words[:10]))
        
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def _is_duplicate_emergency(self, article: Dict, time_window: int = 30) -> bool:
        """ê¸´ê¸‰ ì•Œë¦¼ì´ ì¤‘ë³µì¸ì§€ í™•ì¸ (30ë¶„ ì´ë‚´ë¡œ ë‹¨ì¶•)"""
        try:
            current_time = datetime.now()
            content_hash = self._generate_content_hash(
                article.get('title', ''), 
                article.get('description', '')
            )
            
            # ì‹œê°„ì´ ì§€ë‚œ ì•Œë¦¼ ì œê±°
            cutoff_time = current_time - timedelta(minutes=time_window)
            self.emergency_alerts_sent = {
                k: v for k, v in self.emergency_alerts_sent.items()
                if v > cutoff_time
            }
            
            # ì¤‘ë³µ ì²´í¬
            if content_hash in self.emergency_alerts_sent:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€: {article.get('title', '')[:50]}...")
                return True
            
            # ìƒˆë¡œìš´ ì•Œë¦¼ ê¸°ë¡
            self.emergency_alerts_sent[content_hash] = current_time
            
            # íŒŒì¼ì— ì €ì¥
            self._save_duplicate_data()
            
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _is_recent_news(self, article: Dict, hours: int = 2) -> bool:
        """ë‰´ìŠ¤ê°€ ìµœê·¼ ê²ƒì¸ì§€ í™•ì¸ - 2ì‹œê°„ ì´ë‚´ë¡œ ë‹¨ì¶•"""
        try:
            pub_time_str = article.get('published_at', '')
            if not pub_time_str:
                return True
            
            try:
                if 'T' in pub_time_str:
                    pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                else:
                    from dateutil import parser
                    pub_time = parser.parse(pub_time_str)
                
                if pub_time.tzinfo is None:
                    pub_time = pytz.UTC.localize(pub_time)
                
                time_diff = datetime.now(pytz.UTC) - pub_time
                return time_diff.total_seconds() < (hours * 3600)
            except:
                return True
        except:
            return True
    
    async def start_monitoring(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - ë” ë¹ ë¥¸ ê°ì§€"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=15),
                connector=aiohttp.TCPConnector(limit=150, limit_per_host=50)
            )
        
        logger.info("ğŸ”¥ğŸ”¥ Claude ìš°ì„  ë²ˆì—­ ë¹„íŠ¸ì½”ì¸ + ê±°ì‹œê²½ì œ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        logger.info(f"ğŸ¤– Claude API: {'í™œì„±í™”' if self.anthropic_client else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ§  GPT API: {'í™œì„±í™” (ë°±ì—…)' if self.openai_client else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ“Š RSS ì²´í¬: 5ì´ˆë§ˆë‹¤ (ë¹ ë¥¸ ê°ì§€)")
        logger.info(f"ğŸ¯ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ: {len(self.critical_keywords)}ê°œ")
        logger.info(f"ğŸ¢ ì¶”ì  ê¸°ì—…: {len(self.important_companies)}ê°œ")
        logger.info(f"ğŸ“¡ RSS ì†ŒìŠ¤: {len(self.rss_feeds)}ê°œ")
        
        # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
        self.company_news_count = {}
        
        tasks = [
            self.monitor_rss_feeds_enhanced(),      # RSS (5ì´ˆë§ˆë‹¤) - ë” ë¹ ë¥´ê²Œ
            self.monitor_reddit_enhanced(),         # Reddit (5ë¶„ë§ˆë‹¤) - ê°•í™”
            self.aggressive_api_rotation_enhanced() # API ìˆœí™˜ ì‚¬ìš© - ê°•í™”
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_rss_feeds_enhanced(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ RSS í”¼ë“œ ëª¨ë‹ˆí„°ë§ - 5ì´ˆë§ˆë‹¤ (ë” ë¹ ë¥´ê²Œ)"""
        while True:
            try:
                # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì†ŒìŠ¤ë¶€í„° ì²˜ë¦¬
                sorted_feeds = sorted(self.rss_feeds, key=lambda x: x['weight'], reverse=True)
                successful_feeds = 0
                processed_articles = 0
                critical_found = 0
                translated_count = 0
                
                for feed_info in sorted_feeds:
                    try:
                        articles = await self._parse_rss_feed_enhanced(feed_info)
                        
                        if articles:
                            successful_feeds += 1
                            
                            for article in articles:
                                # ìµœì‹  ë‰´ìŠ¤ë§Œ ì²˜ë¦¬ (2ì‹œê°„ ì´ë‚´ë¡œ ë‹¨ì¶•)
                                if not self._is_recent_news(article, hours=2):
                                    continue
                                
                                # ë¹„íŠ¸ì½”ì¸ + ê±°ì‹œê²½ì œ ê´€ë ¨ì„± ì²´í¬ (ê°•í™”)
                                if not self._is_bitcoin_or_macro_related_enhanced(article):
                                    continue
                                
                                # ê¸°ì—…ëª… ì¶”ì¶œ
                                company = self._extract_company_from_content(
                                    article.get('title', ''),
                                    article.get('description', '')
                                )
                                if company:
                                    article['company'] = company
                                
                                # ğŸ”¥ğŸ”¥ ë²ˆì—­ - Claude ìš°ì„  ì‚¬ìš©
                                if self._should_translate(article):
                                    article['title_ko'] = await self.translate_text(article['title'])
                                    translated_count += 1
                                else:
                                    article['title_ko'] = article.get('title', '')
                                
                                # ğŸ”¥ğŸ”¥ ê°•í™”ëœ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ì²´í¬
                                if self._is_critical_news_enhanced(article):
                                    # ìš”ì•½ (ì„ íƒì )
                                    if self._should_use_gpt_summary(article):
                                        summary = await self.summarize_article_enhanced(
                                            article['title'],
                                            article.get('description', '')
                                        )
                                        if summary:
                                            article['summary'] = summary
                                    
                                    if not self._is_duplicate_emergency(article):
                                        article['expected_change'] = self._estimate_price_impact_enhanced(article)
                                        await self._trigger_emergency_alert_enhanced(article)
                                        processed_articles += 1
                                        critical_found += 1
                                
                                # ì¤‘ìš” ë‰´ìŠ¤ëŠ” ë²„í¼ì— ì¶”ê°€
                                elif self._is_important_news_enhanced(article):
                                    await self._add_to_news_buffer_enhanced(article)
                                    processed_articles += 1
                    
                    except Exception as e:
                        logger.warning(f"RSS í”¼ë“œ ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:50]}")
                        continue
                
                if processed_articles > 0:
                    logger.info(f"ğŸ”¥ RSS ìŠ¤ìº” ì™„ë£Œ: {successful_feeds}ê°œ í”¼ë“œ, {processed_articles}ê°œ ê´€ë ¨ ë‰´ìŠ¤ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ, ë²ˆì—­: {translated_count}ê°œ)")
                
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ (ë” ë¹ˆë²ˆí•˜ê²Œ)
                
            except Exception as e:
                logger.error(f"RSS ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)
    
    def _is_bitcoin_or_macro_related_enhanced(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ê´€ë ¨ì„± + ê±°ì‹œê²½ì œ ì˜í–¥ ì²´í¬"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ë¨¼ì € ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ğŸ”¥ 1. ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰ (ê°€ì¥ ìš°ì„ )
        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'bitcoins']
        has_bitcoin = any(keyword in content for keyword in bitcoin_keywords)
        
        if has_bitcoin:
            return True
        
        # ğŸ”¥ 2. ì•”í˜¸í™”í ì¼ë°˜ + ì¤‘ìš” ë‚´ìš©
        crypto_keywords = ['crypto', 'cryptocurrency', 'ì•”í˜¸í™”í', 'cryptocurrencies', 'digital currency']
        has_crypto = any(keyword in content for keyword in crypto_keywords)
        
        if has_crypto:
            # ETF, SEC, ê·œì œ ë“± ì¤‘ìš” í‚¤ì›Œë“œì™€ í•¨ê»˜ ë‚˜ì˜¤ë©´ í¬í•¨
            important_terms = ['etf', 'sec', 'regulation', 'ban', 'approval', 'court', 'lawsuit', 
                             'bonds', 'russia', 'sberbank', 'institutional', 'adoption']
            if any(term in content for term in important_terms):
                return True
        
        # ğŸ”¥ 3. Fed ê¸ˆë¦¬ ê²°ì • (ë¹„íŠ¸ì½”ì¸ ì–¸ê¸‰ ì—†ì–´ë„ ì¤‘ìš”)
        fed_keywords = ['fed rate decision', 'fomc decides', 'powell announces', 'federal reserve decision',
                       'interest rate decision', 'fed chair', 'fed meeting', 'monetary policy']
        if any(keyword in content for keyword in fed_keywords):
            return True
        
        # ğŸ”¥ 4. ì¤‘ìš” ê²½ì œ ì§€í‘œ (ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì— ì§ì ‘ ì˜í–¥)
        economic_keywords = ['inflation data', 'cpi report', 'unemployment rate', 'jobs report',
                           'gdp growth', 'pce index', 'retail sales', 'manufacturing pmi']
        if any(keyword in content for keyword in economic_keywords):
            return True
        
        # ğŸ”¥ 5. ë¯¸êµ­ ê´€ì„¸ ë° ë¬´ì—­ (ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì— ì˜í–¥)
        trade_keywords = ['trump tariffs', 'china tariffs', 'trade war escalation', 'trade deal signed',
                         'trade agreement', 'trade negotiations breakthrough', 'wto ruling']
        if any(keyword in content for keyword in trade_keywords):
            return True
        
        # ğŸ”¥ 6. ë‹¬ëŸ¬ ê°•ì„¸/ì•½ì„¸ (ë¹„íŠ¸ì½”ì¸ê³¼ ì—­ìƒê´€)
        dollar_keywords = ['dollar strength surge', 'dollar weakness', 'dxy breaks', 'dollar index hits',
                          'usd strengthens', 'usd weakens']
        if any(keyword in content for keyword in dollar_keywords):
            return True
        
        # ğŸ”¥ 7. ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ (ì•ˆì „ìì‚° ìˆ˜ìš”)
        geopolitical_keywords = ['ukraine war escalation', 'russia sanctions expanded', 'china us tensions',
                               'middle east conflict', 'iran israel', 'energy crisis', 'oil price surge']
        if any(keyword in content for keyword in geopolitical_keywords):
            return True
        
        # ğŸ”¥ 8. ì£¼ìš” ê¸°ì—… ê´€ë ¨ (ë¹„íŠ¸ì½”ì¸ ë³´ìœ  ê¸°ì—…ë“¤)
        for company in self.important_companies:
            if company.lower() in content:
                # ê¸°ì—…ì´ ì–¸ê¸‰ë˜ê³  ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í•¨ê»˜ ë‚˜ì˜¤ë©´ í¬í•¨
                relevant_terms = ['earnings', 'acquisition', 'investment', 'purchase', 'announces',
                                'reports', 'launches', 'partnership', 'regulation', 'lawsuit']
                if any(term in content for term in relevant_terms):
                    return True
        
        # ğŸ”¥ 9. ì¤‘ì•™ì€í–‰ ì •ì±… (ê¸€ë¡œë²Œ ì˜í–¥)
        central_bank_keywords = ['ecb rate decision', 'bank of japan policy', 'people bank of china',
                               'boe rate decision', 'rba decision', 'snb policy']
        if any(keyword in content for keyword in central_bank_keywords):
            return True
        
        return False
    
    def _is_critical_news_enhanced(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨ - ë” ë¯¼ê°í•˜ê²Œ"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ + ê±°ì‹œê²½ì œ ê´€ë ¨ì„± ì²´í¬
        if not self._is_bitcoin_or_macro_related_enhanced(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ğŸ”¥ğŸ”¥ ê°€ì¤‘ì¹˜ ì²´í¬ë¥¼ ë‚®ì¶¤ (7 ì´ìƒë§Œ â†’ 6 ì´ìƒìœ¼ë¡œ)
        if article.get('weight', 0) < 6:
            return False
        
        # ğŸ”¥ğŸ”¥ ê°•í™”ëœ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.critical_keywords:
            if keyword.lower() in content:
                # ë¶€ì •ì  í•„í„° (ë£¨ë¨¸, ì¶”ì¸¡ ë“±)
                negative_filters = ['rumor', 'speculation', 'unconfirmed', 'fake', 'false', 
                                  'ë£¨ë¨¸', 'ì¶”ì¸¡', 'ë¯¸í™•ì¸', 'alleged', 'reportedly']
                if any(neg in content for neg in negative_filters):
                    continue
                
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ê°ì§€: '{keyword}' - {article.get('title', '')[:50]}...")
                return True
        
        # ğŸ”¥ğŸ”¥ ì¶”ê°€ í¬ë¦¬í‹°ì»¬ íŒ¨í„´ (ë” ë¯¼ê°í•˜ê²Œ)
        critical_patterns = [
            # ë¹„íŠ¸ì½”ì¸ ì§ì ‘
            ('bitcoin', 'etf', 'approved'),
            ('bitcoin', 'etf', 'rejected'),  
            ('bitcoin', 'billion', 'bought'),
            ('bitcoin', 'sec', 'lawsuit'),
            ('bitcoin', 'ban', 'china'),
            ('bitcoin', 'all', 'time', 'high'),
            ('bitcoin', 'crash', 'below'),
            
            # ê¸°ì—… êµ¬ë§¤
            ('tesla', 'bitcoin', 'purchase'),
            ('microstrategy', 'bitcoin', 'buy'),
            ('blackrock', 'bitcoin', 'fund'),
            
            # Fed ê´€ë ¨
            ('fed', 'rate', 'decision'),
            ('powell', 'announces', 'rate'),
            ('fomc', 'decides', 'policy'),
            
            # ê²½ì œ ì§€í‘œ
            ('inflation', 'surges', 'above'),
            ('unemployment', 'drops', 'below'),
            ('gdp', 'growth', 'exceeds'),
            ('cpi', 'data', 'shows'),
            
            # ë¬´ì—­/ì§€ì •í•™
            ('trump', 'announces', 'tariffs'),
            ('china', 'trade', 'deal'),
            ('ukraine', 'war', 'escalates'),
            ('russia', 'sanctions', 'expanded'),
            
            # ê¸°íƒ€ ì¤‘ìš”
            ('dollar', 'index', 'breaks'),
            ('oil', 'price', 'surges'),
            ('gold', 'hits', 'record')
        ]
        
        for pattern in critical_patterns:
            if all(word in content for word in pattern):
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ íŒ¨í„´ ê°ì§€: {pattern} - {article.get('title', '')[:50]}...")
                return True
        
        return False
    
    def _is_important_news_enhanced(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ + ê±°ì‹œê²½ì œ ê´€ë ¨ì„± ì²´í¬
        if not self._is_bitcoin_or_macro_related_enhanced(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ê°€ì¤‘ì¹˜ì™€ ì¹´í…Œê³ ë¦¬ ì²´í¬
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # ğŸ”¥ğŸ”¥ ì¡°ê±´ë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
        conditions = [
            # ì•”í˜¸í™”í ì „ë¬¸ ì†ŒìŠ¤ (ê°€ì¤‘ì¹˜ ë‚®ì¶¤)
            category == 'crypto' and weight >= 6,
            
            # ê¸ˆìœµ ì†ŒìŠ¤ + ë¹„íŠ¸ì½”ì¸ ë˜ëŠ” ì¤‘ìš” í‚¤ì›Œë“œ
            category == 'finance' and weight >= 6 and (
                any(word in content for word in ['bitcoin', 'btc', 'crypto']) or
                any(word in content for word in ['fed', 'rate', 'inflation', 'sec', 'tariffs', 'trade'])
            ),
            
            # API ë‰´ìŠ¤ (ê°€ì¤‘ì¹˜ ë‚®ì¶¤)
            category == 'api' and weight >= 7,
            
            # ê¸°ì—… + ë¹„íŠ¸ì½”ì¸/ì•”í˜¸í™”í
            any(company.lower() in content for company in self.important_companies) and 
            any(word in content for word in ['bitcoin', 'btc', 'crypto', 'digital', 'blockchain']),
            
            # ê±°ì‹œê²½ì œ ì¤‘ìš” ë‰´ìŠ¤
            any(word in content for word in ['fed rate decision', 'inflation data', 'cpi report', 
                                           'unemployment rate', 'gdp growth', 'trade deal']) and weight >= 6,
            
            # ì§€ì •í•™ì /ë¬´ì—­ ë‰´ìŠ¤
            any(word in content for word in ['trump tariffs', 'china trade', 'ukraine war', 
                                           'russia sanctions', 'middle east']) and weight >= 6,
            
            # ì¤‘ì•™ì€í–‰ ì •ì±…
            any(word in content for word in ['central bank', 'monetary policy', 'ecb decision', 
                                           'boj policy']) and weight >= 6
        ]
        
        return any(conditions)
    
    def _estimate_price_impact_enhanced(self, article: Dict) -> str:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ í˜„ì‹¤ì  ê°€ê²© ì˜í–¥ ì¶”ì •"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ğŸ”¥ 1. ê³¼ê±° íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ (ìš°ì„ )
        pattern_match = self._match_historical_pattern_enhanced(content)
        if pattern_match:
            pattern_data = self.historical_patterns[pattern_match]
            impact = pattern_data['avg_impact']
            confidence = pattern_data['confidence']
            duration = pattern_data['duration_hours']
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ ë²”ìœ„ ì¡°ì •
            if confidence >= 0.9:
                range_modifier = 0.2
            elif confidence >= 0.75:
                range_modifier = 0.3
            else:
                range_modifier = 0.4
            
            if impact > 0:
                min_impact = impact * (1 - range_modifier)
                max_impact = impact * (1 + range_modifier)
                direction = "ğŸ“ˆ ìƒìŠ¹"
                emoji = "ğŸš€" if impact >= 2.0 else "ğŸ“ˆ"
            else:
                min_impact = abs(impact) * (1 - range_modifier)
                max_impact = abs(impact) * (1 + range_modifier)
                direction = "ğŸ“‰ í•˜ë½"
                emoji = "ğŸ”»" if abs(impact) >= 2.0 else "ğŸ“‰"
            
            return f"{emoji} {direction} {min_impact:.1f}~{max_impact:.1f}% ({duration}ì‹œê°„ ë‚´)"
        
        # ğŸ”¥ 2. í‚¤ì›Œë“œ ê¸°ë°˜ ì„¸ë°€í•œ ë¶„ì„
        return self._estimate_price_impact_by_keywords(content)
    
    def _match_historical_pattern_enhanced(self, content: str) -> Optional[str]:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ê³¼ê±° íŒ¨í„´ ë§¤ì¹­"""
        patterns = {
            # ETF ê´€ë ¨
            'etf_approval': ['bitcoin', 'etf', 'approved', 'sec'],
            'etf_rejection': ['bitcoin', 'etf', 'rejected', 'denied'],
            'etf_filing': ['bitcoin', 'etf', 'filing', 'application'],
            
            # ê¸°ì—… êµ¬ë§¤ (ê·œëª¨ë³„)
            'tesla_purchase': ['tesla', 'bitcoin', 'bought', 'purchase'],
            'microstrategy_purchase': ['microstrategy', 'bitcoin', 'acquired', 'buy'],
            'large_corp_purchase': ['billion', 'bitcoin', 'purchase', 'acquired'],
            'small_corp_purchase': ['million', 'bitcoin', 'bought', 'adds'],
            
            # ê·œì œ
            'sec_lawsuit': ['sec', 'lawsuit', 'bitcoin', 'crypto'],
            'china_ban': ['china', 'ban', 'bitcoin', 'cryptocurrency'],
            'regulatory_clarity': ['regulatory', 'clarity', 'bitcoin', 'approved'],
            
            # Fed ê´€ë ¨ (ì„¸ë¶„í™”)
            'fed_rate_hike': ['fed', 'raises', 'rate', 'hike'],
            'fed_rate_cut': ['fed', 'cuts', 'rate', 'lower'],
            'fed_dovish': ['powell', 'dovish', 'accommodative', 'supportive'],
            'fed_hawkish': ['powell', 'hawkish', 'aggressive', 'tightening'],
            
            # ê²½ì œ ì§€í‘œ
            'high_inflation': ['inflation', 'cpi', 'above', 'exceeds'],
            'low_inflation': ['inflation', 'cpi', 'below', 'falls'],
            'recession_fears': ['recession', 'fears', 'slowdown', 'contraction'],
            'strong_jobs': ['jobs', 'unemployment', 'strong', 'beats'],
            
            # ì§€ì •í•™
            'war_escalation': ['ukraine', 'war', 'escalation', 'conflict'],
            'peace_talks': ['peace', 'talks', 'ceasefire', 'negotiations'],
            'sanctions': ['sanctions', 'russia', 'expanded', 'additional'],
            
            # ë¬´ì—­
            'new_tariffs': ['trump', 'tariffs', 'china', 'new'],
            'trade_deal': ['trade', 'deal', 'agreement', 'signed'],
            
            # ë³´ì•ˆ/ê¸°ìˆ 
            'major_hack': ['billion', 'hack', 'stolen', 'breach'],
            'minor_hack': ['million', 'hack', 'stolen', 'compromise'],
            'upgrade_news': ['bitcoin', 'upgrade', 'improvement', 'taproot'],
            
            # ì±„êµ´
            'mining_ban': ['mining', 'ban', 'china', 'prohibited'],
            'mining_support': ['mining', 'support', 'renewable', 'green'],
            'halving_approach': ['halving', 'approach', 'countdown', 'event'],
            
            # ê¸°ê´€
            'bank_adoption': ['bank', 'adopt', 'bitcoin', 'custody'],
            'bank_restriction': ['bank', 'restrict', 'bitcoin', 'prohibited'],
            'pension_entry': ['pension', 'fund', 'bitcoin', 'allocation']
        }
        
        # ë” ì •í™•í•œ ë§¤ì¹­ (ìµœì†Œ 3ê°œ í‚¤ì›Œë“œ ì¼ì¹˜)
        for pattern_name, keywords in patterns.items():
            matches = sum(1 for keyword in keywords if keyword in content)
            if matches >= 3:  # ìµœì†Œ 3ê°œ í‚¤ì›Œë“œ ë§¤ì¹­
                logger.info(f"ğŸ¯ íŒ¨í„´ ë§¤ì¹­: {pattern_name} ({matches}/{len(keywords)} í‚¤ì›Œë“œ)")
                return pattern_name
        
        return None
    
    def _estimate_price_impact_by_keywords(self, content: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°€ê²© ì˜í–¥ ì¶”ì •"""
        # ETF ê´€ë ¨ (ê°€ì¥ ë†’ì€ ì˜í–¥)
        if any(word in content for word in ['etf approved', 'etf approval', 'sec approves bitcoin']):
            return 'ğŸš€ ìƒìŠ¹ 2.5~4.0% (24ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['etf rejected', 'etf denial', 'sec rejects bitcoin']):
            return 'ğŸ”» í•˜ë½ 2.0~3.5% (12ì‹œê°„ ë‚´)'
        
        # Fed ê´€ë ¨
        elif any(word in content for word in ['fed raises rates', 'rate hike', 'hawkish fed']):
            return 'ğŸ“‰ í•˜ë½ 0.8~1.5% (6ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['fed cuts rates', 'rate cut', 'dovish fed']):
            return 'ğŸ“ˆ ìƒìŠ¹ 1.0~2.0% (8ì‹œê°„ ë‚´)'
        
        # ì¸í”Œë ˆì´ì…˜
        elif any(word in content for word in ['inflation above', 'cpi exceeds', 'high inflation']):
            return 'ğŸ“ˆ ìƒìŠ¹ 0.8~1.8% (6ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['inflation below', 'cpi falls', 'low inflation']):
            return 'ğŸ“‰ í•˜ë½ 0.3~0.8% (4ì‹œê°„ ë‚´)'
        
        # ê¸°ì—… êµ¬ë§¤
        elif any(word in content for word in ['tesla bought bitcoin', 'tesla bitcoin purchase']):
            return 'ğŸš€ ìƒìŠ¹ 1.5~3.0% (18ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['microstrategy bought bitcoin', 'saylor bitcoin']):
            return 'ğŸ“ˆ ìƒìŠ¹ 0.5~1.2% (8ì‹œê°„ ë‚´)'
        
        # ê·œì œ
        elif any(word in content for word in ['china bans bitcoin', 'bitcoin banned']):
            return 'ğŸ”» í•˜ë½ 3.0~5.0% (24ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['regulatory clarity', 'bitcoin approved']):
            return 'ğŸ“ˆ ìƒìŠ¹ 1.2~2.5% (12ì‹œê°„ ë‚´)'
        
        # ì§€ì •í•™
        elif any(word in content for word in ['war escalation', 'conflict escalates']):
            return 'ğŸ“ˆ ìƒìŠ¹ 0.8~2.0% (12ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['peace talks', 'ceasefire']):
            return 'ğŸ“‰ í•˜ë½ 0.2~0.8% (6ì‹œê°„ ë‚´)'
        
        # ë¬´ì—­
        elif any(word in content for word in ['new tariffs', 'trade war']):
            return 'ğŸ“‰ í•˜ë½ 0.5~1.2% (6ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['trade deal', 'trade agreement']):
            return 'ğŸ“ˆ ìƒìŠ¹ 0.4~1.0% (8ì‹œê°„ ë‚´)'
        
        # í•´í‚¹/ë³´ì•ˆ
        elif any(word in content for word in ['billion stolen', 'major hack']):
            return 'ğŸ”» í•˜ë½ 1.5~3.0% (8ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['million stolen', 'minor hack']):
            return 'ğŸ“‰ í•˜ë½ 0.3~0.8% (4ì‹œê°„ ë‚´)'
        
        # ê¸°ë³¸ê°’ (ë³´ìˆ˜ì )
        return 'âš¡ ë³€ë™ Â±0.2~0.8% (ë‹¨ê¸°)'
    
    async def summarize_article_enhanced(self, title: str, description: str, max_length: int = 200) -> str:
        """ğŸ”¥ğŸ”¥ ê°œì„ ëœ ìš”ì•½ - ê¸°ë³¸ ìš”ì•½ ìš°ì„ , GPTëŠ” ë°±ì—…"""
        
        # ğŸ”¥ğŸ”¥ ë¨¼ì € ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ì‹œë„
        basic_summary = self._generate_basic_summary_enhanced(title, description)
        if basic_summary and len(basic_summary.strip()) > 50:
            logger.debug(f"ğŸ”„ ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©")
            return basic_summary
        
        # GPT ìš”ì•½ì´ ì •ë§ í•„ìš”í•œ ê²½ìš°ë§Œ
        if not self.openai_client or not description:
            return basic_summary or "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤. íˆ¬ììë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤."
        
        if len(description) <= 200:
            return basic_summary or self._generate_basic_summary_enhanced(title, description)
        
        # ìš”ì•½ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ì²´í¬
        self._reset_summary_count_if_needed()
        
        # Rate limit ì²´í¬
        if self.summary_count >= self.max_summaries_per_15min:
            logger.warning(f"ìš”ì•½ í•œë„ ì´ˆê³¼: {self.summary_count}/{self.max_summaries_per_15min} - ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©")
            return basic_summary or "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤. íˆ¬ììë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤."
        
        try:
            news_type = self._classify_news_for_summary_enhanced(title, description)
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"ë¹„íŠ¸ì½”ì¸ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n\n1ë¬¸ì¥: í•µì‹¬ ì‚¬ì‹¤\n2ë¬¸ì¥: ì¤‘ìš”ì„±\n3ë¬¸ì¥: ì‹œì¥ ì˜í–¥\n\në‰´ìŠ¤ íƒ€ì…: {news_type}"},
                    {"role": "user", "content": f"3ë¬¸ì¥ ìš”ì•½ (ìµœëŒ€ {max_length}ì):\n\nì œëª©: {title}\n\në‚´ìš©: {description[:800]}"}
                ],
                max_tokens=250,
                temperature=0.2
            )
            
            summary = response.choices[0].message.content.strip()
            
            if len(summary) > max_length:
                sentences = summary.split('.')
                result = ""
                for sentence in sentences[:3]:
                    if len(result + sentence + ".") <= max_length - 3:
                        result += sentence + "."
                    else:
                        break
                summary = result.strip() or summary[:max_length-3] + "..."
            
            self.summary_count += 1
            logger.info(f"ğŸ“ GPT ìš”ì•½ ì™„ë£Œ ({self.summary_count}/{self.max_summaries_per_15min})")
            
            return summary
            
        except Exception as e:
            logger.warning(f"GPT ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]} - ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©")
            return basic_summary or "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤. íˆ¬ììë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤."
    
    def _classify_news_for_summary_enhanced(self, title: str, description: str) -> str:
        """ê°•í™”ëœ ë‰´ìŠ¤ íƒ€ì… ë¶„ë¥˜"""
        content = (title + " " + description).lower()
        
        if any(word in content for word in ['etf approved', 'etf rejected', 'etf filing']):
            return 'etf'
        elif any(word in content for word in ['fed rate', 'fomc', 'powell', 'interest rate']):
            return 'fed_policy'
        elif any(word in content for word in ['inflation', 'cpi', 'pce', 'unemployment']):
            return 'economic_data'
        elif any(company in content for company in ['tesla', 'microstrategy', 'blackrock']):
            return 'corporate_action'
        elif any(word in content for word in ['sec', 'regulation', 'ban', 'lawsuit']):
            return 'regulation'
        elif any(word in content for word in ['tariff', 'trade war', 'trade deal']):
            return 'trade_policy'
        elif any(word in content for word in ['hack', 'stolen', 'breach', 'security']):
            return 'security_incident'
        elif any(word in content for word in ['war', 'conflict', 'sanctions', 'geopolitical']):
            return 'geopolitical'
        else:
            return 'general'
    
    def _generate_basic_summary_enhanced(self, title: str, description: str) -> str:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ê¸°ë³¸ ìš”ì•½ ìƒì„± - GPT ëŒ€ì‹  ì‚¬ìš©í•  ê³ í’ˆì§ˆ ìš”ì•½"""
        try:
            content = (title + " " + description).lower()
            summary_parts = []
            
            # êµ¬ì¡°í™” ìƒí’ˆ íŠ¹ë³„ ì²˜ë¦¬
            if any(word in content for word in ['structured', 'bonds', 'linked', 'exposure']):
                if 'sberbank' in content:
                    summary_parts.append("ëŸ¬ì‹œì•„ ìµœëŒ€ ì€í–‰ ìŠ¤ë² ë¥´ë°©í¬ê°€ ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì— ì—°ë™ëœ êµ¬ì¡°í™” ì±„ê¶Œì„ ì¶œì‹œí–ˆë‹¤.")
                    summary_parts.append("ì´ëŠ” ì§ì ‘ì ì¸ ë¹„íŠ¸ì½”ì¸ ë§¤ìˆ˜ê°€ ì•„ë‹Œ ê°€ê²© ì¶”ì  ìƒí’ˆìœ¼ë¡œ, ì‹¤ì œ BTC ìˆ˜ìš” ì°½ì¶œ íš¨ê³¼ëŠ” ì œí•œì ì´ë‹¤.")
                    summary_parts.append("ëŸ¬ì‹œì•„ ì œì¬ ìƒí™©ê³¼ OTC ê±°ë˜ë¡œ ì¸í•´ ê¸€ë¡œë²Œ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì¦‰ê°ì  ì˜í–¥ì€ ë¯¸ë¯¸í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.")
                else:
                    summary_parts.append("ìƒˆë¡œìš´ ë¹„íŠ¸ì½”ì¸ ì—°ê³„ êµ¬ì¡°í™” ìƒí’ˆì´ ì¶œì‹œë˜ì—ˆë‹¤.")
                    summary_parts.append("ì§ì ‘ì ì¸ ë¹„íŠ¸ì½”ì¸ ìˆ˜ìš”ë³´ë‹¤ëŠ” ê°„ì ‘ì  ë…¸ì¶œ ì œê³µì— ì¤‘ì ì„ ë‘” ìƒí’ˆìœ¼ë¡œ í‰ê°€ëœë‹¤.")
                    summary_parts.append("ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì‹¤ì§ˆì  ì˜í–¥ì€ ì œí•œì ì¼ ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤.")
                
                return " ".join(summary_parts)
            
            # ê¸°ì—…ëª…ê³¼ í–‰ë™ ë§¤ì¹­
            companies_in_title = []
            for company in ['tesla', 'microstrategy', 'blackrock', 'gamestop']:
                if company in content:
                    companies_in_title.append(company)
            
            if companies_in_title:
                company = companies_in_title[0]
                
                # ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€ ì²˜ë¦¬
                if company == 'microstrategy':
                    if 'bought' in content or 'purchase' in content:
                        btc_amounts = re.findall(r'(\d+(?:,\d+)*)\s*(?:btc|bitcoin)', content)
                        if btc_amounts:
                            summary_parts.append(f"ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€ê°€ ë¹„íŠ¸ì½”ì¸ {btc_amounts[0]}ê°œë¥¼ ì§ì ‘ ë§¤ì…í–ˆë‹¤.")
                        else:
                            summary_parts.append("ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€ê°€ ë¹„íŠ¸ì½”ì¸ì„ ì¶”ê°€ ë§¤ì…í–ˆë‹¤.")
                        
                        summary_parts.append("ì´ëŠ” ì‹¤ì œ BTC ìˆ˜ìš” ì¦ê°€ë¥¼ ì˜ë¯¸í•˜ë©°, ê¸°ì—… ì¬ë¬´ ì „ëµì˜ ì¼í™˜ìœ¼ë¡œ ì‹œì¥ì— ê¸ì •ì  ì‹ í˜¸ë¥¼ ë³´ë‚¸ë‹¤.")
                        summary_parts.append("ëŒ€í˜• ê¸°ì—…ì˜ ì§€ì†ì ì¸ ë¹„íŠ¸ì½”ì¸ ë§¤ì…ì€ ì‹œì¥ ì‹ ë¢°ë„ í–¥ìƒì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.")
                
                # í…ŒìŠ¬ë¼ ì²˜ë¦¬
                elif company == 'tesla':
                    if 'bought' in content or 'purchase' in content:
                        summary_parts.append("í…ŒìŠ¬ë¼ê°€ ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ë§¤ì…ì„ ì¬ê°œí–ˆë‹¤.")
                        summary_parts.append("ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ ì˜í–¥ë ¥ê³¼ í•¨ê»˜ ì‹œì¥ì— ìƒë‹¹í•œ ê´€ì‹¬ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.")
                        summary_parts.append("ê¸°ì—…ì˜ ë¹„íŠ¸ì½”ì¸ ì±„íƒ í™•ì‚°ì— ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ì „ë§ì´ë‹¤.")
                
                # ë¸”ë™ë¡ ì²˜ë¦¬
                elif company == 'blackrock':
                    if 'etf' in content:
                        if 'approved' in content:
                            summary_parts.append("ì„¸ê³„ ìµœëŒ€ ìì‚°ìš´ìš©ì‚¬ ë¸”ë™ë¡ì˜ ë¹„íŠ¸ì½”ì¸ ETFê°€ ìŠ¹ì¸ë˜ì—ˆë‹¤.")
                            summary_parts.append("ì´ëŠ” ê¸°ê´€ ìê¸ˆì˜ ëŒ€ê·œëª¨ ìœ ì… ê°€ëŠ¥ì„±ì„ ì—´ì–´ì£¼ëŠ” íšê¸°ì  ì‚¬ê±´ì´ë‹¤.")
                            summary_parts.append("ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì˜ ì œë„í™”ì™€ ì£¼ë¥˜ ì±„íƒì— ì¤‘ìš”í•œ ì´ì •í‘œê°€ ë  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.")
                        else:
                            summary_parts.append("ë¸”ë™ë¡ì˜ ë¹„íŠ¸ì½”ì¸ ETF ê´€ë ¨ ì¤‘ìš”í•œ ë°œí‘œê°€ ìˆì—ˆë‹¤.")
                            summary_parts.append("ì„¸ê³„ ìµœëŒ€ ìì‚°ìš´ìš©ì‚¬ì˜ ì›€ì§ì„ì´ ì‹œì¥ì— ì£¼ëª©ë°›ê³  ìˆë‹¤.")
                            summary_parts.append("ê¸°ê´€ íˆ¬ììë“¤ì˜ ë¹„íŠ¸ì½”ì¸ ê´€ì‹¬ë„ê°€ ë†’ì•„ì§€ê³  ìˆìŒì„ ì‹œì‚¬í•œë‹¤.")
            
            # ê±°ì‹œê²½ì œ íŒ¨í„´ ì²˜ë¦¬ (ìƒˆë¡œ ì¶”ê°€)
            if not summary_parts:
                # ê´€ì„¸ ê´€ë ¨
                if any(word in content for word in ['trump', 'tariffs', 'trade war']):
                    summary_parts.append("ë¯¸êµ­ì˜ ìƒˆë¡œìš´ ê´€ì„¸ ì •ì±…ì´ ë°œí‘œë˜ì—ˆë‹¤.")
                    summary_parts.append("ë¬´ì—­ ë¶„ìŸ ìš°ë ¤ë¡œ ì¸í•´ ë‹¨ê¸°ì ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ìì‚°ì— ë¶€ë‹´ì´ ë  ìˆ˜ ìˆë‹¤.")
                    summary_parts.append("í•˜ì§€ë§Œ ë‹¬ëŸ¬ ì•½ì„¸ ìš”ì¸ì´ ë¹„íŠ¸ì½”ì¸ì—ëŠ” ì¤‘ì¥ê¸°ì ìœ¼ë¡œ ìœ ë¦¬í•  ê²ƒìœ¼ë¡œ ë¶„ì„ëœë‹¤.")
                
                # ì¸í”Œë ˆì´ì…˜ ê´€ë ¨
                elif any(word in content for word in ['inflation', 'cpi']):
                    summary_parts.append("ìµœì‹  ì¸í”Œë ˆì´ì…˜ ë°ì´í„°ê°€ ë°œí‘œë˜ì—ˆë‹¤.")
                    summary_parts.append("ì¸í”Œë ˆì´ì…˜ í—¤ì§€ ìì‚°ìœ¼ë¡œì„œ ë¹„íŠ¸ì½”ì¸ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆë‹¤.")
                    summary_parts.append("ì‹¤ë¬¼ ìì‚° ëŒ€ë¹„ ìš°ì›”í•œ ì„±ê³¼ë¥¼ ë³´ì´ë©° íˆ¬ììë“¤ì˜ ì£¼ëª©ì„ ë°›ê³  ìˆë‹¤.")
                
                # ETF ê´€ë ¨
                elif 'etf' in content:
                    if 'approved' in content or 'approval' in content:
                        summary_parts.append("ë¹„íŠ¸ì½”ì¸ í˜„ë¬¼ ETF ìŠ¹ì¸ ì†Œì‹ì´ ì „í•´ì¡Œë‹¤.")
                        summary_parts.append("ETF ìŠ¹ì¸ì€ ê¸°ê´€ íˆ¬ììë“¤ì˜ ëŒ€ê·œëª¨ ìê¸ˆ ìœ ì…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì¤‘ìš”í•œ ì´ì •í‘œë‹¤.")
                        summary_parts.append("ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì˜ ì„±ìˆ™ë„ì™€ ì œë„ì  ì¸ì •ì„ ë³´ì—¬ì£¼ëŠ” ìƒì§•ì  ì‚¬ê±´ìœ¼ë¡œ í‰ê°€ëœë‹¤.")
                    elif 'rejected' in content or 'delay' in content:
                        summary_parts.append("ë¹„íŠ¸ì½”ì¸ ETF ìŠ¹ì¸ì´ ì§€ì—°ë˜ê±°ë‚˜ ê±°ë¶€ë˜ì—ˆë‹¤.")
                        summary_parts.append("ë‹¨ê¸°ì  ì‹¤ë§ê°ì€ ìˆìœ¼ë‚˜, ì§€ì†ì ì¸ ì‹ ì²­ì€ ê²°êµ­ ìŠ¹ì¸ ê°€ëŠ¥ì„±ì„ ë†’ì´ê³  ìˆë‹¤.")
                        summary_parts.append("ì‹œì¥ì€ ì´ë¯¸ ETF ìŠ¹ì¸ì„ ê¸°ì •ì‚¬ì‹¤ë¡œ ë°›ì•„ë“¤ì´ê³  ìˆì–´ ì¥ê¸° ì „ë§ì€ ê¸ì •ì ì´ë‹¤.")
                
                # Fed ê¸ˆë¦¬ ê´€ë ¨
                elif 'fed' in content or 'rate' in content:
                    if 'cut' in content or 'lower' in content:
                        summary_parts.append("ì—°ì¤€ì˜ ê¸ˆë¦¬ ì¸í•˜ ê²°ì •ì´ ë°œí‘œë˜ì—ˆë‹¤.")
                        summary_parts.append("ê¸ˆë¦¬ ì¸í•˜ëŠ” ìœ ë™ì„± ì¦ê°€ë¥¼ í†µí•´ ë¹„íŠ¸ì½”ì¸ê³¼ ê°™ì€ ë¦¬ìŠ¤í¬ ìì‚°ì— ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤.")
                        summary_parts.append("ì €ê¸ˆë¦¬ í™˜ê²½ì—ì„œ ëŒ€ì•ˆ íˆ¬ìì²˜ë¡œì„œ ë¹„íŠ¸ì½”ì¸ì˜ ë§¤ë ¥ë„ê°€ ë”ìš± ë¶€ê°ë  ì „ë§ì´ë‹¤.")
                    elif 'hike' in content or 'increase' in content:
                        summary_parts.append("ì—°ì¤€ì˜ ê¸ˆë¦¬ ì¸ìƒ ê²°ì •ì´ ë°œí‘œë˜ì—ˆë‹¤.")
                        summary_parts.append("ë‹¨ê¸°ì ìœ¼ë¡œëŠ” ë¶€ë‹´ì´ì§€ë§Œ ì¸í”Œë ˆì´ì…˜ í—¤ì§€ ìì‚°ìœ¼ë¡œì„œì˜ ë¹„íŠ¸ì½”ì¸ ê°€ì¹˜ëŠ” ì§€ì†ë  ê²ƒì´ë‹¤.")
                        summary_parts.append("ê³ ê¸ˆë¦¬ í™˜ê²½ì—ì„œë„ ë””ì§€í„¸ ê¸ˆìœ¼ë¡œì„œì˜ ì—­í• ì€ ë³€í•¨ì—†ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.")
                
                # ê¸°ë³¸ ì¼€ì´ìŠ¤
                else:
                    summary_parts.append("ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ë°œí‘œê°€ ìˆì—ˆë‹¤.")
                    summary_parts.append("íˆ¬ììë“¤ì€ ì´ë²ˆ ì†Œì‹ì˜ ì‹¤ì œ ì‹œì¥ ì˜í–¥ì„ ë©´ë°€íˆ ë¶„ì„í•˜ê³  ìˆë‹¤.")
                    summary_parts.append("ë‹¨ê¸° ë³€ë™ì„±ì€ ìˆê² ì§€ë§Œ ì¥ê¸° íŠ¸ë Œë“œì—ëŠ” í° ë³€í™”ê°€ ì—†ì„ ê²ƒìœ¼ë¡œ ì „ë§ëœë‹¤.")
            
            return " ".join(summary_parts[:3]) if summary_parts else "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ì†Œì‹ì´ ë°œí‘œë˜ì—ˆë‹¤. ì‹œì¥ ë°˜ì‘ì„ ì§€ì¼œë³¼ í•„ìš”ê°€ ìˆë‹¤. íˆ¬ììë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤."
            
        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë¹„íŠ¸ì½”ì¸ ì‹œì¥ ê´€ë ¨ ì†Œì‹ì´ ë°œí‘œë˜ì—ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì›ë¬¸ì„ í™•ì¸í•˜ì‹œê¸° ë°”ë€ë‹¤. ì‹¤ì œ ì‹œì¥ ë°˜ì‘ì„ ë©´ë°€íˆ ë¶„ì„í•  í•„ìš”ê°€ ìˆë‹¤."
    
    async def _trigger_emergency_alert_enhanced(self, article: Dict):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ê¸´ê¸‰ ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        try:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return
            
            # ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¡œ ê¸°ë¡
            self.processed_news_hashes.add(content_hash)
            
            # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ í¬ê¸° ì œí•œ
            if len(self.processed_news_hashes) > 5000:
                self.processed_news_hashes = set(list(self.processed_news_hashes)[-2500:])
            
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ ê¸°ë¡
            if content_hash not in self.news_first_seen:
                self.news_first_seen[content_hash] = datetime.now()
            
            # ğŸ”¥ğŸ”¥ ê°•í™”ëœ ì´ë²¤íŠ¸ ìƒì„±
            event = {
                'type': 'critical_news',
                'title': article.get('title', ''),
                'title_ko': article.get('title_ko', article.get('title', '')),
                'description': article.get('description', '')[:1600],  # ë” ê¸¸ê²Œ
                'summary': article.get('summary', ''),
                'company': article.get('company', ''),
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self._determine_impact_enhanced(article),
                'expected_change': article.get('expected_change', 'Â±0.5%'),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown'),
                'published_at': article.get('published_at', ''),
                'first_seen': self.news_first_seen[content_hash],
                
                # ğŸ”¥ ì¶”ê°€ ë¶„ì„ ì •ë³´
                'urgency_level': self._calculate_urgency_level(article),
                'market_relevance': self._calculate_market_relevance(article),
                'pattern_match': self._match_historical_pattern_enhanced(
                    (article.get('title', '') + ' ' + article.get('description', '')).lower()
                )
            }
            
            # íŒŒì¼ì— ì €ì¥
            self._save_duplicate_data()
            
            # ë°ì´í„° ì»¬ë ‰í„°ì— ì „ë‹¬
            if hasattr(self, 'data_collector') and self.data_collector:
                self.data_collector.events_buffer.append(event)
            
            logger.critical(f"ğŸš¨ğŸš¨ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤: {event['impact']} - {event['title_ko'][:60]}... (ì˜ˆìƒ: {event['expected_change']})")
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    def _determine_impact_enhanced(self, article: Dict) -> str:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ë‰´ìŠ¤ ì˜í–¥ë„ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        expected_change = self._estimate_price_impact_enhanced(article)
        
        # ì˜ˆìƒ ë³€ë™ë¥ ì— ë”°ë¥¸ ì˜í–¥ë„ (ë” ì„¸ë°€í•˜ê²Œ)
        if 'ğŸš€' in expected_change or any(x in expected_change for x in ['3%', '4%', '5%']):
            return "ğŸš€ ë§¤ìš° ê°•í•œ í˜¸ì¬"
        elif 'ğŸ“ˆ' in expected_change and any(x in expected_change for x in ['1.5%', '2%']):
            return "ğŸ“ˆ ê°•í•œ í˜¸ì¬"
        elif 'ğŸ“ˆ' in expected_change:
            return "ğŸ“ˆ í˜¸ì¬"
        elif 'ğŸ”»' in expected_change or any(x in expected_change for x in ['3%', '4%', '5%']):
            return "ğŸ”» ë§¤ìš° ê°•í•œ ì•…ì¬"
        elif 'ğŸ“‰' in expected_change and any(x in expected_change for x in ['1.5%', '2%']):
            return "ğŸ“‰ ê°•í•œ ì•…ì¬"
        elif 'ğŸ“‰' in expected_change:
            return "ğŸ“‰ ì•…ì¬"
        else:
            return "âš¡ ë³€ë™ì„± í™•ëŒ€"
    
    def _calculate_urgency_level(self, article: Dict) -> str:
        """ê¸´ê¸‰ë„ ë ˆë²¨ ê³„ì‚°"""
        weight = article.get('weight', 0)
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì¦‰ì‹œ ë°˜ì‘ì´ í•„ìš”í•œ í‚¤ì›Œë“œ
        immediate_keywords = ['approved', 'rejected', 'announced', 'breaking', 'urgent', 'alert']
        has_immediate = any(keyword in content for keyword in immediate_keywords)
        
        if weight >= 10 and has_immediate:
            return "ê·¹ë„ ê¸´ê¸‰"
        elif weight >= 9:
            return "ë§¤ìš° ê¸´ê¸‰"
        elif weight >= 8:
            return "ê¸´ê¸‰"
        else:
            return "ì¤‘ìš”"
    
    def _calculate_market_relevance(self, article: Dict) -> str:
        """ì‹œì¥ ê´€ë ¨ì„± ê³„ì‚°"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì§ì ‘ì  ë¹„íŠ¸ì½”ì¸ ê´€ë ¨
        if any(word in content for word in ['bitcoin', 'btc']):
            return "ì§ì ‘ì "
        
        # ì•”í˜¸í™”í ì¼ë°˜
        elif any(word in content for word in ['crypto', 'cryptocurrency']):
            return "ì•”í˜¸í™”í"
        
        # ê±°ì‹œê²½ì œ
        elif any(word in content for word in ['fed', 'rate', 'inflation', 'gdp']):
            return "ê±°ì‹œê²½ì œ"
        
        # ì§€ì •í•™ì 
        elif any(word in content for word in ['war', 'sanctions', 'conflict']):
            return "ì§€ì •í•™ì "
        
        else:
            return "ê°„ì ‘ì "
    
    async def _add_to_news_buffer_enhanced(self, article: Dict):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€"""
        try:
            # ì¤‘ë³µ ì²´í¬
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return
            
            # ì œëª© ìœ ì‚¬ì„± ì²´í¬
            new_title = article.get('title', '').lower()
            for existing in self.news_buffer:
                if self._is_similar_news_enhanced(new_title, existing.get('title', '')):
                    return
            
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì²´í¬ (ë” ê´€ëŒ€í•˜ê²Œ)
            for company in self.important_companies:
                if company.lower() in new_title:
                    important_keywords = ['bitcoin', 'btc', 'crypto', 'purchase', 'bought', 'investment']
                    if any(keyword in new_title for keyword in important_keywords):
                        if self.company_news_count.get(company.lower(), 0) >= 3:  # 2 â†’ 3ê°œë¡œ ì¦ê°€
                            return
                        self.company_news_count[company.lower()] = self.company_news_count.get(company.lower(), 0) + 1
            
            # ë²„í¼ì— ì¶”ê°€
            self.news_buffer.append(article)
            self.processed_news_hashes.add(content_hash)
            
            # íŒŒì¼ì— ì €ì¥
            self._save_duplicate_data()
            
            # ë²„í¼ í¬ê¸° ê´€ë¦¬ (ìµœëŒ€ 100ê°œë¡œ ì¦ê°€)
            if len(self.news_buffer) > 100:
                # ê°€ì¤‘ì¹˜ì™€ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
                self.news_buffer.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
                self.news_buffer = self.news_buffer[:100]
            
            logger.debug(f"âœ… ì¤‘ìš” ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€: {new_title[:50]}...")
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def _is_similar_news_enhanced(self, title1: str, title2: str) -> bool:
        """ê°•í™”ëœ ìœ ì‚¬ ë‰´ìŠ¤ íŒë³„"""
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean1 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title1.lower())
        clean2 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title2.lower())
        
        clean1 = re.sub(r'\s+', ' ', clean1).strip()
        clean2 = re.sub(r'\s+', ' ', clean2).strip()
        
        # íŠ¹ì • íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ ì²´í¬
        for company in self.important_companies:
            company_lower = company.lower()
            if company_lower in clean1 and company_lower in clean2:
                bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'crypto', 'purchase', 'bought']
                if any(keyword in clean1 for keyword in bitcoin_keywords) and \
                   any(keyword in clean2 for keyword in bitcoin_keywords):
                    return True
        
        # ë‹¨ì–´ ì§‘í•© ë¹„êµ (ë” ì—„ê²©í•˜ê²Œ)
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        if not words1 or not words2:
            return False
        
        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        similarity = intersection / union if union > 0 else 0
        
        # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ (ë” ì—„ê²©í•˜ê²Œ)
        return similarity > 0.8
    
    async def monitor_reddit_enhanced(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ Reddit ëª¨ë‹ˆí„°ë§ - 5ë¶„ë§ˆë‹¤"""
        reddit_subreddits = [
            {'name': 'Bitcoin', 'threshold': 300, 'weight': 9},  # ì„ê³„ê°’ ë‚®ì¶¤
            {'name': 'CryptoCurrency', 'threshold': 800, 'weight': 8},
            {'name': 'BitcoinMarkets', 'threshold': 200, 'weight': 9},
            {'name': 'investing', 'threshold': 1000, 'weight': 7},  # ì¶”ê°€
            {'name': 'Economics', 'threshold': 500, 'weight': 7},  # ì¶”ê°€
        ]
        
        while True:
            try:
                for sub_info in reddit_subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{sub_info['name']}/hot.json?limit=15"
                        
                        async with self.session.get(url, headers={'User-Agent': 'Bitcoin Monitor Bot 1.0'}) as response:
                            if response.status == 200:
                                data = await response.json()
                                posts = data['data']['children']
                                
                                for post in posts:
                                    post_data = post['data']
                                    
                                    if post_data['ups'] > sub_info['threshold']:
                                        article = {
                                            'title': post_data['title'],
                                            'title_ko': post_data['title'],
                                            'description': post_data.get('selftext', '')[:1600],
                                            'url': f"https://reddit.com{post_data['permalink']}",
                                            'source': f"Reddit r/{sub_info['name']}",
                                            'published_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                                            'upvotes': post_data['ups'],
                                            'weight': sub_info['weight'],
                                            'category': 'social'
                                        }
                                        
                                        if self._is_bitcoin_or_macro_related_enhanced(article):
                                            # ê¸°ì—…ëª… ì¶”ì¶œ
                                            company = self._extract_company_from_content(
                                                article['title'],
                                                article.get('description', '')
                                            )
                                            if company:
                                                article['company'] = company
                                            
                                            if self._is_critical_news_enhanced(article):
                                                # Redditì—ì„œëŠ” ë²ˆì—­ ì œí•œì ìœ¼ë¡œë§Œ
                                                if self._should_translate(article):
                                                    article['title_ko'] = await self.translate_text(article['title'])
                                                
                                                # Redditì—ì„œëŠ” ìš”ì•½ ê±°ì˜ ì‚¬ìš© ì•ˆí•¨
                                                if self._should_use_gpt_summary(article):
                                                    summary = await self.summarize_article_enhanced(
                                                        article['title'],
                                                        article.get('description', '')
                                                    )
                                                    if summary:
                                                        article['summary'] = summary
                                                
                                                if not self._is_duplicate_emergency(article):
                                                    article['expected_change'] = self._estimate_price_impact_enhanced(article)
                                                    await self._trigger_emergency_alert_enhanced(article)
                                            
                                            elif self._is_important_news_enhanced(article):
                                                await self._add_to_news_buffer_enhanced(article)
                    
                    except Exception as e:
                        logger.warning(f"Reddit ì˜¤ë¥˜ {sub_info['name']}: {str(e)[:50]}")
                
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤
                
            except Exception as e:
                logger.error(f"Reddit ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(600)
    
    async def aggressive_api_rotation_enhanced(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ API ìˆœí™˜ ì‚¬ìš©"""
        while True:
            try:
                self._reset_daily_usage()
                
                # NewsAPI (ë” ìì£¼)
                if self.newsapi_key and self.api_usage['newsapi_today'] < self.api_limits['newsapi']:
                    try:
                        await self._call_newsapi_enhanced()
                        self.api_usage['newsapi_today'] += 1
                        logger.info(f"âœ… NewsAPI í˜¸ì¶œ ({self.api_usage['newsapi_today']}/{self.api_limits['newsapi']})")
                    except Exception as e:
                        logger.error(f"NewsAPI ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(600)  # 10ë¶„ ëŒ€ê¸°
                
                # NewsData API
                if self.newsdata_key and self.api_usage['newsdata_today'] < self.api_limits['newsdata']:
                    try:
                        await self._call_newsdata_enhanced()
                        self.api_usage['newsdata_today'] += 1
                        logger.info(f"âœ… NewsData í˜¸ì¶œ ({self.api_usage['newsdata_today']}/{self.api_limits['newsdata']})")
                    except Exception as e:
                        logger.error(f"NewsData ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(600)  # 10ë¶„ ëŒ€ê¸°
                
                # Alpha Vantage
                if self.alpha_vantage_key and self.api_usage['alpha_vantage_today'] < self.api_limits['alpha_vantage']:
                    try:
                        await self._call_alpha_vantage_enhanced()
                        self.api_usage['alpha_vantage_today'] += 1
                        logger.info(f"âœ… Alpha Vantage í˜¸ì¶œ ({self.api_usage['alpha_vantage_today']}/{self.api_limits['alpha_vantage']})")
                    except Exception as e:
                        logger.error(f"Alpha Vantage ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(1200)  # 20ë¶„ ëŒ€ê¸°
                
            except Exception as e:
                logger.error(f"API ìˆœí™˜ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1800)
    
    async def _call_newsapi_enhanced(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ NewsAPI í˜¸ì¶œ"""
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': '(bitcoin OR btc OR "bitcoin etf" OR "fed rate" OR "trump tariffs" OR "trade deal" OR "inflation data" OR "china manufacturing" OR "powell speech" OR "fomc decision" OR "cpi report" OR "unemployment rate" OR "sec bitcoin" OR "tesla bitcoin" OR "microstrategy bitcoin" OR "blackrock bitcoin" OR "russia bitcoin" OR "ukraine war" OR "china sanctions") AND NOT ("altcoin only" OR "how to mine" OR "price prediction tutorial")',
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.newsapi_key,
                'pageSize': 100,  # 50 â†’ 100ìœ¼ë¡œ ì¦ê°€
                'from': (datetime.now() - timedelta(hours=3)).isoformat()  # 6ì‹œê°„ â†’ 3ì‹œê°„ìœ¼ë¡œ ë‹¨ì¶• (ë” ë¹ ë¥¸ ê°ì§€)
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('articles', [])
                    
                    processed = 0
                    critical_found = 0
                    translated_count = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('description', '')[:1600],
                            'url': article.get('url', ''),
                            'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                            'published_at': article.get('publishedAt', ''),
                            'weight': 9,
                            'category': 'api'
                        }
                        
                        if self._is_bitcoin_or_macro_related_enhanced(formatted_article):
                            # ê¸°ì—…ëª… ì¶”ì¶œ
                            company = self._extract_company_from_content(
                                formatted_article['title'],
                                formatted_article.get('description', '')
                            )
                            if company:
                                formatted_article['company'] = company
                            
                            # ë²ˆì—­ - Claude ìš°ì„  ì‚¬ìš©
                            if self._should_translate(formatted_article):
                                formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                                translated_count += 1
                            
                            if self._is_critical_news_enhanced(formatted_article):
                                # ìš”ì•½ (ì„ íƒì )
                                if self._should_use_gpt_summary(formatted_article):
                                    summary = await self.summarize_article_enhanced(
                                        formatted_article['title'],
                                        formatted_article.get('description', '')
                                    )
                                    if summary:
                                        formatted_article['summary'] = summary
                                
                                if not self._is_duplicate_emergency(formatted_article):
                                    formatted_article['expected_change'] = self._estimate_price_impact_enhanced(formatted_article)
                                    await self._trigger_emergency_alert_enhanced(formatted_article)
                                processed += 1
                                critical_found += 1
                            elif self._is_important_news_enhanced(formatted_article):
                                await self._add_to_news_buffer_enhanced(formatted_article)
                                processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ”¥ NewsAPI: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ, ë²ˆì—­: {translated_count}ê°œ)")
                else:
                    logger.warning(f"NewsAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsAPI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_newsdata_enhanced(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ NewsData API í˜¸ì¶œ"""
        try:
            url = "https://newsdata.io/api/1/news"
            params = {
                'apikey': self.newsdata_key,
                'q': 'bitcoin OR btc OR "bitcoin etf" OR "bitcoin regulation" OR "russia bitcoin" OR "sberbank bitcoin" OR "fed rate decision" OR "trump tariffs" OR "trade deal" OR "inflation data" OR "china manufacturing" OR "powell speech" OR "fomc decision" OR "tesla bitcoin" OR "microstrategy bitcoin" OR "sec bitcoin" OR "ukraine war" OR "china sanctions"',
                'language': 'en',
                'category': 'business,top,politics',  # ì¹´í…Œê³ ë¦¬ í™•ì¥
                'size': 50  # 30 â†’ 50ìœ¼ë¡œ ì¦ê°€
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('results', [])
                    
                    processed = 0
                    critical_found = 0
                    translated_count = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('description', '')[:1600],
                            'url': article.get('link', ''),
                            'source': f"NewsData ({article.get('source_id', 'Unknown')})",
                            'published_at': article.get('pubDate', ''),
                            'weight': 8,
                            'category': 'api'
                        }
                        
                        if self._is_bitcoin_or_macro_related_enhanced(formatted_article):
                            # ê¸°ì—…ëª… ì¶”ì¶œ
                            company = self._extract_company_from_content(
                                formatted_article['title'],
                                formatted_article.get('description', '')
                            )
                            if company:
                                formatted_article['company'] = company
                            
                            # ë²ˆì—­ - Claude ìš°ì„  ì‚¬ìš©
                            if self._should_translate(formatted_article):
                                formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                                translated_count += 1
                            
                            if self._is_critical_news_enhanced(formatted_article):
                                # ìš”ì•½ (ì„ íƒì )
                                if self._should_use_gpt_summary(formatted_article):
                                    summary = await self.summarize_article_enhanced(
                                        formatted_article['title'],
                                        formatted_article.get('description', '')
                                    )
                                    if summary:
                                        formatted_article['summary'] = summary
                                
                                if not self._is_duplicate_emergency(formatted_article):
                                    formatted_article['expected_change'] = self._estimate_price_impact_enhanced(formatted_article)
                                    await self._trigger_emergency_alert_enhanced(formatted_article)
                                processed += 1
                                critical_found += 1
                            elif self._is_important_news_enhanced(formatted_article):
                                await self._add_to_news_buffer_enhanced(formatted_article)
                                processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ”¥ NewsData: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ, ë²ˆì—­: {translated_count}ê°œ)")
                else:
                    logger.warning(f"NewsData ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsData í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_alpha_vantage_enhanced(self):
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ Alpha Vantage API í˜¸ì¶œ"""
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': 'CRYPTO:BTC,TSLA,MSTR',  # í‹°ì»¤ í™•ì¥
                'topics': 'financial_markets,technology,earnings,economy',  # í† í”½ í™•ì¥
                'apikey': self.alpha_vantage_key,
                'sort': 'LATEST',
                'limit': 50  # 20 â†’ 50ìœ¼ë¡œ ì¦ê°€
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('feed', [])
                    
                    processed = 0
                    critical_found = 0
                    translated_count = 0
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'title_ko': article.get('title', ''),
                            'description': article.get('summary', '')[:1600],
                            'url': article.get('url', ''),
                            'source': f"Alpha Vantage ({article.get('source', 'Unknown')})",
                            'published_at': article.get('time_published', ''),
                            'weight': 9,
                            'category': 'api',
                            'sentiment': article.get('overall_sentiment_label', 'Neutral')
                        }
                        
                        if self._is_bitcoin_or_macro_related_enhanced(formatted_article):
                            # ê¸°ì—…ëª… ì¶”ì¶œ
                            company = self._extract_company_from_content(
                                formatted_article['title'],
                                formatted_article.get('description', '')
                            )
                            if company:
                                formatted_article['company'] = company
                            
                            # ë²ˆì—­ - Claude ìš°ì„  ì‚¬ìš©
                            if self._should_translate(formatted_article):
                                formatted_article['title_ko'] = await self.translate_text(formatted_article['title'])
                                translated_count += 1
                            
                            if self._is_critical_news_enhanced(formatted_article):
                                # ìš”ì•½ (ì„ íƒì )
                                if self._should_use_gpt_summary(formatted_article):
                                    summary = await self.summarize_article_enhanced(
                                        formatted_article['title'],
                                        formatted_article.get('description', '')
                                    )
                                    if summary:
                                        formatted_article['summary'] = summary
                                
                                if not self._is_duplicate_emergency(formatted_article):
                                    formatted_article['expected_change'] = self._estimate_price_impact_enhanced(formatted_article)
                                    await self._trigger_emergency_alert_enhanced(formatted_article)
                                processed += 1
                                critical_found += 1
                            elif self._is_important_news_enhanced(formatted_article):
                                await self._add_to_news_buffer_enhanced(formatted_article)
                                processed += 1
                    
                    if processed > 0:
                        logger.info(f"ğŸ”¥ Alpha Vantage: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ ì²˜ë¦¬ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ, ë²ˆì—­: {translated_count}ê°œ)")
                else:
                    logger.warning(f"Alpha Vantage ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"Alpha Vantage í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _parse_rss_feed_enhanced(self, feed_info: Dict) -> List[Dict]:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ RSS í”¼ë“œ íŒŒì‹±"""
        articles = []
        try:
            async with self.session.get(
                feed_info['url'], 
                timeout=aiohttp.ClientTimeout(total=12),
                headers={'User-Agent': 'Mozilla/5.0 (compatible; BitcoinNewsBot/2.0)'}
            ) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    if feed.entries:
                        # ë” ë§ì€ ê¸°ì‚¬ ì²˜ë¦¬
                        limit = min(25, max(10, feed_info['weight']))
                        
                        for entry in feed.entries[:limit]:
                            try:
                                # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                                pub_time = datetime.now().isoformat()
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    pub_time = datetime(*entry.published_parsed[:6]).isoformat()
                                elif hasattr(entry, 'published'):
                                    try:
                                        from dateutil import parser
                                        pub_time = parser.parse(entry.published).isoformat()
                                    except:
                                        pass
                                
                                article = {
                                    'title': entry.get('title', '').strip(),
                                    'description': entry.get('summary', '').strip()[:1600],
                                    'url': entry.get('link', '').strip(),
                                    'source': feed_info['source'],
                                    'published_at': pub_time,
                                    'weight': feed_info['weight'],
                                    'category': feed_info.get('category', 'unknown')
                                }
                                
                                if article['title'] and article['url']:
                                    articles.append(article)
                                        
                            except Exception as e:
                                logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)[:50]}")
                                continue
        
        except asyncio.TimeoutError:
            logger.debug(f"â° {feed_info['source']}: íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.debug(f"âŒ {feed_info['source']}: {str(e)[:50]}")
        
        return articles
    
    def _extract_company_from_content(self, title: str, description: str = "") -> str:
        """ì»¨í…ì¸ ì—ì„œ ê¸°ì—…ëª… ì¶”ì¶œ"""
        content = (title + " " + description).lower()
        
        # ì¤‘ìš” ê¸°ì—… í™•ì¸
        found_companies = []
        for company in self.important_companies:
            if company.lower() in content:
                # ì›ë˜ ëŒ€ì†Œë¬¸ì ìœ ì§€
                for original in self.important_companies:
                    if original.lower() == company.lower():
                        found_companies.append(original)
                        break
        
        # ì²« ë²ˆì§¸ ë°œê²¬ëœ ê¸°ì—… ë°˜í™˜
        if found_companies:
            return found_companies[0]
        
        return ""
    
    def _reset_daily_usage(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        today = datetime.now().date()
        if today > self.api_usage['last_reset']:
            old_usage = dict(self.api_usage)
            self.api_usage.update({
                'newsapi_today': 0,
                'newsdata_today': 0,
                'alpha_vantage_today': 0,
                'last_reset': today
            })
            self.company_news_count = {}
            self.claude_translation_count = 0
            self.gpt_translation_count = 0
            self.summary_count = 0
            self.last_translation_reset = datetime.now()
            self.last_summary_reset = datetime.now()
            self.news_first_seen = {}
            logger.info(f"ğŸ”„ ì¼ì¼ ë¦¬ì…‹ ì™„ë£Œ (Claude: {self.max_claude_translations_per_15min}/15ë¶„, GPT: {self.max_gpt_translations_per_15min}/15ë¶„, ìš”ì•½: {self.max_summaries_per_15min}/15ë¶„)")
    
    async def get_recent_news_enhanced(self, hours: int = 12) -> List[Dict]:
        """ğŸ”¥ğŸ”¥ ê°•í™”ëœ ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_news = []
            seen_hashes = set()
            
            # ë” ë§ì€ ë‰´ìŠ¤ ë°˜í™˜
            for article in sorted(self.news_buffer, key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True):
                try:
                    # ì‹œê°„ ì²´í¬
                    if article.get('published_at'):
                        pub_time_str = article.get('published_at', '')
                        try:
                            if 'T' in pub_time_str:
                                pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                            else:
                                from dateutil import parser
                                pub_time = parser.parse(pub_time_str)
                            
                            if pub_time > cutoff_time:
                                # ì¤‘ë³µ ì²´í¬
                                content_hash = self._generate_content_hash(article.get('title', ''), '')
                                if content_hash not in seen_hashes:
                                    recent_news.append(article)
                                    seen_hashes.add(content_hash)
                        except:
                            pass
                except:
                    pass
            
            # ì •ë ¬: ê°€ì¤‘ì¹˜ â†’ ì‹œê°„
            recent_news.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
            
            logger.info(f"ğŸ”¥ ìµœê·¼ {hours}ì‹œê°„ ë‰´ìŠ¤: {len(recent_news)}ê°œ (ì´ ë²„í¼: {len(self.news_buffer)}ê°œ)")
            
            return recent_news[:25]  # 15 â†’ 25ê°œë¡œ ì¦ê°€
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def get_recent_news(self, hours: int = 12) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼)"""
        return await self.get_recent_news_enhanced(hours)
    
    def _is_critical_news(self, article: Dict) -> bool:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        return self._is_critical_news_enhanced(article)
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥
            self._save_duplicate_data()
            
            if self.session:
                await self.session.close()
                logger.info("ğŸ”š Claude ìš°ì„  ë²ˆì—­ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ")
                logger.info(f"ğŸ¤– ìµœì¢… Claude ë²ˆì—­: {self.claude_translation_count}, GPT ë²ˆì—­: {self.gpt_translation_count}")
                logger.info(f"ğŸ“ ìµœì¢… GPT ìš”ì•½: {self.summary_count}")
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

import aiohttp
import asyncio
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional
import pytz
from bs4 import BeautifulSoup
import feedparser

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    def __init__(self, config):
        self.config = config
        self.session = None
        self.news_buffer = []
        
        # ëª¨ë“  API í‚¤ë“¤
        self.newsapi_key = getattr(config, 'NEWSAPI_KEY', None)
        self.newsdata_key = getattr(config, 'NEWSDATA_KEY', None)
        self.alpha_vantage_key = getattr(config, 'ALPHA_VANTAGE_KEY', None)
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ (ì¦‰ì‹œ ì•Œë¦¼ìš©)
        self.critical_keywords = [
            'trump bitcoin', 'trump crypto', 'trump ban', 'trump announces', 'trump says bitcoin',
            'fed rate decision', 'fed raises', 'fed cuts', 'powell says', 'fomc decides', 'fed meeting',
            'sec lawsuit bitcoin', 'sec sues', 'sec enforcement', 'sec charges bitcoin',
            'china bans bitcoin', 'china crypto ban', 'government bans crypto', 'regulatory ban',
            'bitcoin crash', 'crypto crash', 'market crash', 'flash crash', 'bitcoin plunge',
            'bitcoin etf approved', 'bitcoin etf rejected', 'etf decision', 'etf filing'
        ]
        
        # RSS í”¼ë“œ (ë©”ì¸ ì†ŒìŠ¤)
        self.rss_feeds = [
            # ì•”í˜¸í™”í ì „ë¬¸ (ìµœìš°ì„ )
            {'url': 'https://cointelegraph.com/rss', 'source': 'Cointelegraph', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'source': 'CoinDesk', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://decrypt.co/feed', 'source': 'Decrypt', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://bitcoinmagazine.com/.rss/full/', 'source': 'Bitcoin Magazine', 'weight': 9, 'category': 'crypto'},
            
            # ì¼ë°˜ ê¸ˆìœµ (ê³ ìš°ì„ ìˆœìœ„)
            {'url': 'https://feeds.reuters.com/reuters/businessNews', 'source': 'Reuters Business', 'weight': 9, 'category': 'finance'},
            {'url': 'https://feeds.bloomberg.com/markets/news.rss', 'source': 'Bloomberg Markets', 'weight': 9, 'category': 'finance'},
            {'url': 'https://rss.cnn.com/rss/money_news_economy.rss', 'source': 'CNN Business', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.finance.yahoo.com/rss/headline', 'source': 'Yahoo Finance', 'weight': 7, 'category': 'finance'},
            
            # ì •ì¹˜/ì •ì±… (ì¤‘ìš”)
            {'url': 'https://feeds.reuters.com/Reuters/PoliticsNews', 'source': 'Reuters Politics', 'weight': 8, 'category': 'politics'},
            {'url': 'https://feeds.washingtonpost.com/rss/politics', 'source': 'Washington Post Politics', 'weight': 7, 'category': 'politics'},
            
            # ì¶”ê°€ ì†ŒìŠ¤
            {'url': 'https://feeds.cnbc.com/cnbc/world', 'source': 'CNBC World', 'weight': 8, 'category': 'finance'},
            {'url': 'https://www.marketwatch.com/rss/topstories', 'source': 'MarketWatch', 'weight': 7, 'category': 'finance'},
        ]
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.api_usage = {
            'newsapi_today': 0,
            'newsdata_today': 0,
            'alpha_vantage_today': 0,
            'last_reset': datetime.now().date()
        }
        
        # API ì¼ì¼ í•œë„
        self.api_limits = {
            'newsapi': 20,      # í•˜ë£¨ 20íšŒ
            'newsdata': 10,     # í•˜ë£¨ 10íšŒ (ì›” 200ê±´ì˜ 1/3)
            'alpha_vantage': 1  # í•˜ë£¨ 1íšŒ (ì›” 25ê±´ì˜ 1/4)
        }
        
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ - API í‚¤ ìƒíƒœ: NewsAPI={bool(self.newsapi_key)}, NewsData={bool(self.newsdata_key)}, AlphaVantage={bool(self.alpha_vantage_key)}")
    
    async def start_monitoring(self):
        """ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        logger.info("ğŸ” ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - RSS ì¤‘ì‹¬ + ìŠ¤ë§ˆíŠ¸ API ì‚¬ìš©")
        
        tasks = [
            self.monitor_rss_feeds(),      # ë©”ì¸: RSS (30ì´ˆë§ˆë‹¤)
            self.monitor_reddit(),         # ë³´ì¡°: Reddit (5ë¶„ë§ˆë‹¤)
            self.smart_api_rotation()      # ì œí•œì : 3ê°œ API ìˆœí™˜ ì‚¬ìš©
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_rss_feeds(self):
        """RSS í”¼ë“œ ëª¨ë‹ˆí„°ë§ - ë©”ì¸ ì†ŒìŠ¤"""
        while True:
            try:
                # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ì†ŒìŠ¤ë¶€í„° ì²˜ë¦¬
                sorted_feeds = sorted(self.rss_feeds, key=lambda x: x['weight'], reverse=True)
                
                for feed_info in sorted_feeds:
                    try:
                        articles = await self._parse_rss_feed(feed_info)
                        
                        for article in articles:
                            # ê°€ì¤‘ì¹˜ 8 ì´ìƒì€ í¬ë¦¬í‹°ì»¬ ì²´í¬
                            if feed_info['weight'] >= 8:
                                if self._is_critical_news(article):
                                    await self._trigger_emergency_alert(article)
                            
                            # ëª¨ë“  RSSëŠ” ì¤‘ìš” ë‰´ìŠ¤ ì²´í¬
                            if self._is_important_news(article):
                                await self._add_to_news_buffer(article)
                    
                    except Exception as e:
                        logger.error(f"RSS ì˜¤ë¥˜ {feed_info['source']}: {e}")
                        continue
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì „ì²´ RSS ì²´í¬
                
            except Exception as e:
                logger.error(f"RSS ëª¨ë‹ˆí„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(180)
    
    async def monitor_reddit(self):
        """Reddit ëª¨ë‹ˆí„°ë§"""
        reddit_subreddits = [
            {'name': 'Bitcoin', 'threshold': 300, 'weight': 8},
            {'name': 'CryptoCurrency', 'threshold': 500, 'weight': 7},
            {'name': 'investing', 'threshold': 1000, 'weight': 6},
            {'name': 'wallstreetbets', 'threshold': 3000, 'weight': 5}
        ]
        
        while True:
            try:
                for sub_info in reddit_subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{sub_info['name']}/hot.json?limit=15"
                        
                        async with self.session.get(url, headers={'User-Agent': 'Bitcoin Monitor Bot'}) as response:
                            if response.status == 200:
                                data = await response.json()
                                posts = data['data']['children']
                                
                                for post in posts:
                                    post_data = post['data']
                                    
                                    if post_data['ups'] > sub_info['threshold']:
                                        article = {
                                            'title': post_data['title'],
                                            'description': post_data.get('selftext', '')[:200],
                                            'url': f"https://reddit.com{post_data['permalink']}",
                                            'source': f"Reddit r/{sub_info['name']}",
                                            'published_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                                            'upvotes': post_data['ups'],
                                            'weight': sub_info['weight']
                                        }
                                        
                                        if self._is_critical_news(article):
                                            await self._trigger_emergency_alert(article)
                                        elif self._is_important_news(article):
                                            await self._add_to_news_buffer(article)
                    
                    except Exception as e:
                        logger.error(f"Reddit ì˜¤ë¥˜ {sub_info['name']}: {e}")
                        continue
                
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ Reddit ì²´í¬
                
            except Exception as e:
                logger.error(f"Reddit ëª¨ë‹ˆí„°ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(600)
    
    async def smart_api_rotation(self):
        """3ê°œ API ìŠ¤ë§ˆíŠ¸ ìˆœí™˜ ì‚¬ìš©"""
        while True:
            try:
                self._reset_daily_usage()
                
                # NewsAPI (ê°€ì¥ ë¹ˆë²ˆíˆ ì‚¬ìš©)
                if self.newsapi_key and self.api_usage['newsapi_today'] < self.api_limits['newsapi']:
                    await self._call_newsapi()
                    self.api_usage['newsapi_today'] += 1
                    logger.info(f"NewsAPI í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['newsapi_today']}/{self.api_limits['newsapi']})")
                
                await asyncio.sleep(1800)  # 30ë¶„ ëŒ€ê¸°
                
                # NewsData API (ì¤‘ê°„ ë¹ˆë„)
                if self.newsdata_key and self.api_usage['newsdata_today'] < self.api_limits['newsdata']:
                    await self._call_newsdata()
                    self.api_usage['newsdata_today'] += 1
                    logger.info(f"NewsData API í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['newsdata_today']}/{self.api_limits['newsdata']})")
                
                await asyncio.sleep(1800)  # 30ë¶„ ëŒ€ê¸°
                
                # Alpha Vantage (ê°€ì¥ ì œí•œì )
                if self.alpha_vantage_key and self.api_usage['alpha_vantage_today'] < self.api_limits['alpha_vantage']:
                    await self._call_alpha_vantage()
                    self.api_usage['alpha_vantage_today'] += 1
                    logger.info(f"Alpha Vantage API í˜¸ì¶œ ì™„ë£Œ ({self.api_usage['alpha_vantage_today']}/{self.api_limits['alpha_vantage']})")
                
                await asyncio.sleep(3600)  # 1ì‹œê°„ ëŒ€ê¸°
                
            except Exception as e:
                logger.error(f"API ìˆœí™˜ ì‚¬ìš© ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)
    
    async def _parse_rss_feed(self, feed_info: Dict) -> List[Dict]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            async with self.session.get(feed_info['url'], timeout=15) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    articles = []
                    
                    # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜ ê²°ì •
                    limit = min(20, max(8, feed_info['weight'] + 2))
                    
                    for entry in feed.entries[:limit]:
                        try:
                            # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                            pub_time = datetime.now().isoformat()
                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                pub_time = datetime(*entry.published_parsed[:6]).isoformat()
                        except:
                            pub_time = datetime.now().isoformat()
                        
                        article = {
                            'title': entry.get('title', ''),
                            'description': entry.get('summary', '')[:400],
                            'url': entry.get('link', ''),
                            'source': feed_info['source'],
                            'published_at': pub_time,
                            'weight': feed_info['weight'],
                            'category': feed_info['category']
                        }
                        
                        # ìµœê·¼ 4ì‹œê°„ ë‚´ ê¸°ì‚¬ë§Œ (RSSëŠ” ë¹ ë¥¸ ì—…ë°ì´íŠ¸)
                        try:
                            article_time = datetime.fromisoformat(pub_time.replace('Z', ''))
                            if datetime.now() - article_time < timedelta(hours=4):
                                articles.append(article)
                        except:
                            articles.append(article)
                    
                    return articles
        
        except Exception as e:
            logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜ {feed_info['url']}: {e}")
        
        return []
    
    async def _call_newsapi(self):
        """NewsAPI í˜¸ì¶œ"""
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': '(trump AND bitcoin) OR (fed AND rate) OR (sec AND bitcoin) OR "bitcoin etf" OR (powell AND crypto)',
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.newsapi_key,
                'pageSize': 15,
                'from': (datetime.now() - timedelta(hours=1)).isoformat()
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('articles', [])
                    
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'description': article.get('description', ''),
                            'url': article.get('url', ''),
                            'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                            'published_at': article.get('publishedAt', ''),
                            'weight': 10,
                            'category': 'api'
                        }
                        
                        if self._is_critical_news(formatted_article):
                            await self._trigger_emergency_alert(formatted_article)
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
        
        except Exception as e:
            logger.error(f"NewsAPI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_newsdata(self):
        """NewsData API í˜¸ì¶œ"""
        try:
            url = "https://newsdata.io/api/1/news"
            params = {
                'apikey': self.newsdata_key,
                'q': 'bitcoin OR crypto OR "federal reserve" OR SEC',
                'language': 'en',
                'category': 'business,politics',
                'size': 10
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('results', [])
                    
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'description': article.get('description', ''),
                            'url': article.get('link', ''),
                            'source': f"NewsData ({article.get('source_id', 'Unknown')})",
                            'published_at': article.get('pubDate', ''),
                            'weight': 9,
                            'category': 'api'
                        }
                        
                        if self._is_critical_news(formatted_article):
                            await self._trigger_emergency_alert(formatted_article)
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
        
        except Exception as e:
            logger.error(f"NewsData API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_alpha_vantage(self):
        """Alpha Vantage API í˜¸ì¶œ"""
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': 'CRYPTO:BTC',
                'topics': 'financial_markets,economy_monetary',
                'apikey': self.alpha_vantage_key,
                'sort': 'LATEST',
                'limit': 8
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('feed', [])
                    
                    for article in articles:
                        formatted_article = {
                            'title': article.get('title', ''),
                            'description': article.get('summary', ''),
                            'url': article.get('url', ''),
                            'source': f"Alpha Vantage ({article.get('source', 'Unknown')})",
                            'published_at': article.get('time_published', ''),
                            'weight': 10,
                            'category': 'api',
                            'sentiment': article.get('overall_sentiment_label', 'Neutral')
                        }
                        
                        if self._is_critical_news(formatted_article):
                            await self._trigger_emergency_alert(formatted_article)
                        elif self._is_important_news(formatted_article):
                            await self._add_to_news_buffer(formatted_article)
        
        except Exception as e:
            logger.error(f"Alpha Vantage API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    def _reset_daily_usage(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        today = datetime.now().date()
        if today > self.api_usage['last_reset']:
            self.api_usage.update({
                'newsapi_today': 0,
                'newsdata_today': 0,
                'alpha_vantage_today': 0,
                'last_reset': today
            })
            logger.info("API ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹ë¨")
    
    def _is_critical_news(self, article: Dict) -> bool:
        """í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        for keyword in self.critical_keywords:
            if keyword.lower() in content:
                # ì‹ ë¢°í•  ë§Œí•œ ì†ŒìŠ¤ì—ì„œë§Œ (ê°€ì¤‘ì¹˜ 7 ì´ìƒ)
                if article.get('weight', 0) >= 7:
                    logger.warning(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ê°ì§€: {article.get('title', '')[:60]}...")
                    return True
        
        return False
    
    def _is_important_news(self, article: Dict) -> bool:
        """ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸/ì•”í˜¸í™”í ê´€ë ¨
        crypto_keywords = ['bitcoin', 'btc', 'crypto', 'cryptocurrency', 'digital asset']
        has_crypto = any(word in content for word in crypto_keywords)
        
        # ê¸ˆìœµ/ì •ì±… ê´€ë ¨
        finance_keywords = ['fed', 'federal reserve', 'interest rate', 'inflation', 'sec', 'regulation', 'monetary policy']
        has_finance = any(word in content for word in finance_keywords)
        
        # ì •ì¹˜ ê´€ë ¨
        political_keywords = ['trump', 'biden', 'congress', 'government', 'policy', 'administration']
        has_political = any(word in content for word in political_keywords)
        
        # ì‹œì¥ ê´€ë ¨
        market_keywords = ['market', 'trading', 'price', 'surge', 'crash', 'rally', 'dump', 'volatility']
        has_market = any(word in content for word in market_keywords)
        
        # ì¡°ê±´ë“¤
        conditions = [
            has_crypto and (has_finance or has_political or has_market),  # ì•”í˜¸í™”í + ë‹¤ë¥¸ ìš”ì†Œ
            article.get('weight', 0) >= 9 and (has_finance or has_political),  # ê³ ê°€ì¤‘ì¹˜ + ê¸ˆìœµ/ì •ì¹˜
            article.get('category') == 'crypto' and has_market,  # ì•”í˜¸í™”í ì†ŒìŠ¤ + ì‹œì¥ ê´€ë ¨
            has_crypto and 'etf' in content,  # ì•”í˜¸í™”í ETF ê´€ë ¨
        ]
        
        return any(conditions)
    
    async def _trigger_emergency_alert(self, article: Dict):
        """ê¸´ê¸‰ ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        try:
            event = {
                'type': 'critical_news',
                'title': article.get('title', '')[:100],
                'description': article.get('description', '')[:250],
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self._determine_impact(article),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown')
            }
            
            # ë°ì´í„° ì»¬ë ‰í„°ì— ì „ë‹¬
            if hasattr(self, 'data_collector') and self.data_collector:
                self.data_collector.events_buffer.append(event)
            
            logger.critical(f"ğŸš¨ ê¸´ê¸‰ ë‰´ìŠ¤ ì•Œë¦¼: {article.get('source', '')} - {article.get('title', '')}")
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _add_to_news_buffer(self, article: Dict):
        """ë‰´ìŠ¤ ë²„í¼ì— ì¶”ê°€"""
        try:
            # ì¤‘ë³µ ì²´í¬ (ì œëª©ê³¼ ì†ŒìŠ¤ ê¸°ì¤€)
            title_words = set(article.get('title', '').lower().split())
            source = article.get('source', '').lower()
            
            is_duplicate = False
            for existing in self.news_buffer:
                existing_words = set(existing.get('title', '').lower().split())
                existing_source = existing.get('source', '').lower()
                
                # ì œëª© ìœ ì‚¬ë„ 70% ì´ìƒì´ê³  ì†ŒìŠ¤ê°€ ê°™ìœ¼ë©´ ì¤‘ë³µ
                if len(title_words & existing_words) / len(title_words | existing_words) > 0.7 and source == existing_source:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                self.news_buffer.append(article)
                
                # ë²„í¼ ê´€ë¦¬: ê°€ì¤‘ì¹˜ì™€ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 40ê°œë§Œ ìœ ì§€
                if len(self.news_buffer) > 40:
                    self.news_buffer.sort(
                        key=lambda x: (x.get('weight', 0), x.get('published_at', '')), 
                        reverse=True
                    )
                    self.news_buffer = self.news_buffer[:40]
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def _determine_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ ì˜í–¥ë„ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ê°•í•œ ì•…ì¬
        if any(word in content for word in ['ban', 'lawsuit', 'crash', 'crackdown', 'reject', 'enforcement action']):
            return "â–ê°•í•œ ì•…ì¬"
        
        # ê°•í•œ í˜¸ì¬
        if any(word in content for word in ['approval', 'approved', 'adoption', 'surge', 'breakthrough', 'all-time high']):
            return "â•ê°•í•œ í˜¸ì¬"
        
        # ì¼ë°˜ ì•…ì¬
        if any(word in content for word in ['concern', 'worry', 'decline', 'fall', 'drop', 'uncertainty']):
            return "â–ì•…ì¬ ì˜ˆìƒ"
        
        # ì¼ë°˜ í˜¸ì¬
        if any(word in content for word in ['growth', 'rise', 'increase', 'positive', 'rally', 'optimistic']):
            return "â•í˜¸ì¬ ì˜ˆìƒ"
        
        # ì„¼í‹°ë¨¼íŠ¸ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° (Alpha Vantage)
        if article.get('sentiment'):
            sentiment = article.get('sentiment', '').lower()
            if 'bearish' in sentiment:
                return "â–ì•…ì¬ ì˜ˆìƒ"
            elif 'bullish' in sentiment:
                return "â•í˜¸ì¬ ì˜ˆìƒ"
        
        return "ì¤‘ë¦½"
    
    async def get_recent_news(self, hours: int = 6) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_news = []
            
            for article in self.news_buffer:
                try:
                    if article.get('published_at'):
                        pub_time_str = article.get('published_at', '').replace('Z', '').replace('T', ' ')
                        pub_time = datetime.fromisoformat(pub_time_str)
                        if pub_time > cutoff_time:
                            recent_news.append(article)
                    else:
                        # ì‹œê°„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ìµœê·¼ ë‰´ìŠ¤ë¡œ ê°„ì£¼
                        recent_news.append(article)
                except:
                    recent_news.append(article)
            
            # ê°€ì¤‘ì¹˜, ì¹´í…Œê³ ë¦¬, ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
            def sort_key(x):
                weight = x.get('weight', 0)
                category_priority = {'crypto': 3, 'api': 2, 'finance': 1, 'politics': 1}
                cat_score = category_priority.get(x.get('category', ''), 0)
                pub_time = x.get('published_at', '')
                return (weight, cat_score, pub_time)
            
            recent_news.sort(key=sort_key, reverse=True)
            
            logger.info(f"ìµœê·¼ {hours}ì‹œê°„ ë‰´ìŠ¤ {len(recent_news)}ê±´ ë°˜í™˜")
            return recent_news[:12]  # ìµœëŒ€ 12ê°œ
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ")

import aiohttp
import asyncio
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional, Set
import pytz
from bs4 import BeautifulSoup
import feedparser
import openai
import os
import hashlib
import re
import json
import random

logger = logging.getLogger(__name__)

class RealisticNewsCollector:
    def __init__(self, config):
        self.config = config
        self.session = None
        self.news_buffer = []
        self.emergency_alerts_sent = {}
        self.processed_news_hashes = set()
        self.news_title_cache = {}
        self.company_news_count = {}
        self.news_first_seen = {}
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° íŒŒì¼
        self.persistence_file = 'news_duplicates.json'
        self.processed_reports_file = 'processed_critical_reports.json'
        
        # ì „ì†¡ëœ ë‰´ìŠ¤ ì œëª© ìºì‹œ
        self.sent_news_titles = {}
        self.sent_critical_reports = {}
        
        # ë²ˆì—­ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.translation_cache = {}
        self.claude_translation_count = 0
        self.gpt_translation_count = 0
        self.claude_error_count = 0
        self.last_translation_reset = datetime.now()
        self.max_claude_translations_per_15min = 15
        self.max_gpt_translations_per_15min = 30
        self.translation_reset_interval = 900
        self.claude_cooldown_until = None
        self.claude_cooldown_duration = 300
        
        # Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.anthropic_client = None
        if hasattr(config, 'ANTHROPIC_API_KEY') and config.ANTHROPIC_API_KEY:
            try:
                import anthropic
                self.anthropic_client = anthropic.AsyncAnthropic(api_key=config.ANTHROPIC_API_KEY)
                logger.info("âœ… Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                logger.warning("âŒ anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            except Exception as e:
                logger.warning(f"Claude API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = None
        if hasattr(config, 'OPENAI_API_KEY') and config.OPENAI_API_KEY:
            self.openai_client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        
        # GPT ìš”ì•½ ì‚¬ìš©ëŸ‰ ì œí•œ
        self.summary_count = 0
        self.max_summaries_per_15min = 25
        self.last_summary_reset = datetime.now()
        
        # API í‚¤ë“¤
        self.newsapi_key = getattr(config, 'NEWSAPI_KEY', None)
        self.newsdata_key = getattr(config, 'NEWSDATA_KEY', None)
        self.alpha_vantage_key = getattr(config, 'ALPHA_VANTAGE_KEY', None)
        
        # ğŸ”¥ğŸ”¥ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ (ê¸°ì¤€ ì™„í™”)
        self.critical_keywords = [
            # ë¹„íŠ¸ì½”ì¸ ETF
            'bitcoin etf approved', 'bitcoin etf rejected', 'spot bitcoin etf', 'etf decision',
            'blackrock bitcoin etf', 'fidelity bitcoin etf', 'bitcoin etf launches',
            'SEC approves bitcoin', 'SEC rejects bitcoin', 'bitcoin etf trading',
            
            # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤
            'tesla bought bitcoin', 'microstrategy bought bitcoin', 'bought bitcoin', 'buys bitcoin',
            'gamestop bitcoin purchase', 'bitcoin acquisition', 'adds bitcoin',
            'purchases bitcoin', 'bitcoin investment', 'bitcoin holdings',
            
            # êµ­ê°€/ì€í–‰ ì±„íƒ
            'russia bitcoin', 'sberbank bitcoin', 'bitcoin bonds', 'government bitcoin',
            'country adopts bitcoin', 'central bank bitcoin', 'china bitcoin',
            'putin bitcoin', 'russia legalize bitcoin',
            
            # ë¹„íŠ¸ì½”ì¸ ê·œì œ
            'bitcoin ban', 'bitcoin regulation', 'bitcoin lawsuit', 'sec bitcoin',
            'china bans bitcoin', 'government bans bitcoin', 'trump bitcoin',
            'regulatory approval bitcoin', 'coinbase lawsuit',
            
            # ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì´ì •í‘œ
            'bitcoin crosses 100k', 'bitcoin hits 100000', 'bitcoin 100k',
            'bitcoin all time high', 'bitcoin ath', 'bitcoin breaks',
            'bitcoin reaches', 'bitcoin milestone',
            
            # Fed ê¸ˆë¦¬ ë° ê±°ì‹œê²½ì œ
            'fed rate decision', 'fomc decision', 'powell speech', 'interest rate decision',
            'fed minutes', 'inflation report', 'cpi data', 'unemployment rate',
            
            # ë¬´ì—­/ê´€ì„¸
            'trump tariffs', 'china tariffs', 'trade war', 'trade deal',
            'trade agreement', 'tariff announcement',
            
            # ê¸°íƒ€ ì¤‘ìš”
            'bitcoin hack', 'bitcoin stolen', 'exchange hacked bitcoin',
            'whale alert bitcoin', 'large bitcoin transfer'
        ]
        
        # ì œì™¸ í‚¤ì›Œë“œ
        self.exclude_keywords = [
            'how to mine', 'mining tutorial', 'price prediction tutorial',
            'altcoin only', 'ethereum only', 'nft only', 'defi only',
            'celebrity news', 'entertainment', 'sports', 'weather'
        ]
        
        # ì¤‘ìš” ê¸°ì—…
        self.important_companies = [
            'tesla', 'microstrategy', 'square', 'block', 'paypal',
            'gamestop', 'gme', 'blackrock', 'fidelity', 'ark invest',
            'coinbase', 'binance', 'kraken', 'bitget',
            'metaplanet', 'sberbank', 'jpmorgan', 'goldman sachs'
        ]
        
        # ğŸ”¥ğŸ”¥ User-Agent ë¡œí…Œì´ì…˜ (403 ì˜¤ë¥˜ í•´ê²°)
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0'
        ]
        
        # ğŸ”¥ğŸ”¥ RSS í”¼ë“œ ê°œì„  (403 ì˜¤ë¥˜ í”¼ë“œ ì œê±°/êµì²´)
        self.rss_feeds = [
            # ì•”í˜¸í™”í ì „ë¬¸ (ë¬¸ì œì—†ëŠ” í”¼ë“œë§Œ)
            {'url': 'https://cointelegraph.com/rss', 'source': 'Cointelegraph', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'source': 'CoinDesk', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://decrypt.co/feed', 'source': 'Decrypt', 'weight': 9, 'category': 'crypto'},
            {'url': 'https://bitcoinmagazine.com/.rss/full/', 'source': 'Bitcoin Magazine', 'weight': 10, 'category': 'crypto'},
            {'url': 'https://cryptopotato.com/feed/', 'source': 'CryptoPotato', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://u.today/rss', 'source': 'U.Today', 'weight': 8, 'category': 'crypto'},
            # CryptoSlate ì œê±° (403 ì˜¤ë¥˜)
            {'url': 'https://cryptonews.com/news/feed/', 'source': 'Cryptonews', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://www.newsbtc.com/feed/', 'source': 'NewsBTC', 'weight': 8, 'category': 'crypto'},
            {'url': 'https://beincrypto.com/feed/', 'source': 'BeInCrypto', 'weight': 8, 'category': 'crypto'},
            
            # ê¸ˆìœµ ë‰´ìŠ¤
            {'url': 'https://www.marketwatch.com/rss/topstories', 'source': 'MarketWatch', 'weight': 8, 'category': 'finance'},
            {'url': 'https://feeds.reuters.com/reuters/businessNews', 'source': 'Reuters Business', 'weight': 8, 'category': 'news'},
            {'url': 'https://feeds.cnbc.com/cnbc/ID/100003114/device/rss/rss.html', 'source': 'CNBC Markets', 'weight': 8, 'category': 'finance'},
            
            # ê¸°ìˆ  ë‰´ìŠ¤
            {'url': 'https://techcrunch.com/feed/', 'source': 'TechCrunch', 'weight': 7, 'category': 'tech'}
        ]
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.api_usage = {
            'newsapi_today': 0,
            'newsdata_today': 0,
            'alpha_vantage_today': 0,
            'last_reset': datetime.now().date()
        }
        
        # API í•œë„
        self.api_limits = {
            'newsapi': 60,
            'newsdata': 30,
            'alpha_vantage': 8
        }
        
        # ë‰´ìŠ¤ ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_articles_checked': 0,
            'bitcoin_related_found': 0,
            'critical_news_found': 0,
            'important_news_found': 0,
            'alerts_sent': 0,
            'translation_attempts': 0,
            'translation_successes': 0,
            'api_errors': 0,
            'rss_errors': 0,
            'last_reset': datetime.now()
        }
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ
        self._load_duplicate_data()
        self._load_critical_reports()
        
        logger.info(f"ğŸ”¥ğŸ”¥ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ (403 ì˜¤ë¥˜ í•´ê²° ë²„ì „)")
        logger.info(f"ğŸ§  GPT API: {'í™œì„±í™”' if self.openai_client else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ¤– Claude API: {'í™œì„±í™”' if self.anthropic_client else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ¯ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ: {len(self.critical_keywords)}ê°œ")
        logger.info(f"ğŸ¢ ì¶”ì  ê¸°ì—…: {len(self.important_companies)}ê°œ")
        logger.info(f"ğŸ“¡ RSS ì†ŒìŠ¤: {len(self.rss_feeds)}ê°œ (403 ì˜¤ë¥˜ í”¼ë“œ ì œê±°)")
        
        # ğŸ”¥ğŸ”¥ ì¤‘ë³µ ë°©ì§€ ê¸°ì¤€ ì™„í™” ì„¤ì •
        self.duplicate_check_hours = 2  # 2ì‹œê°„ ì´ë‚´ ì¤‘ë³µë§Œ ì²´í¬ (ê¸°ì¡´ 4ì‹œê°„ì—ì„œ ì™„í™”)
        self.critical_report_cooldown_minutes = 60  # 1ì‹œê°„ ì¿¨ë‹¤ìš´ (ê¸°ì¡´ 240ë¶„ì—ì„œ ì™„í™”)
    
    def _get_random_user_agent(self) -> str:
        """ëœë¤ User-Agent ë°˜í™˜"""
        return random.choice(self.user_agents)
    
    def _load_duplicate_data(self):
        """ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.persistence_file):
                with open(self.persistence_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.processed_news_hashes = set(data.get('processed_news_hashes', []))
                
                # ê¸´ê¸‰ ì•Œë¦¼ ë°ì´í„° ë¡œë“œ (ì‹œê°„ ì™„í™”)
                emergency_data = data.get('emergency_alerts_sent', {})
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=self.duplicate_check_hours)
                
                for hash_key, time_str in emergency_data.items():
                    try:
                        alert_time = datetime.fromisoformat(time_str)
                        if alert_time > cutoff_time:
                            self.emergency_alerts_sent[hash_key] = alert_time
                    except:
                        continue
                
                # ì œëª© ìºì‹œ ë¡œë“œ (ì‹œê°„ ì™„í™”)
                title_data = data.get('sent_news_titles', {})
                cutoff_time = current_time - timedelta(hours=self.duplicate_check_hours)
                
                for title_hash, time_str in title_data.items():
                    try:
                        sent_time = datetime.fromisoformat(time_str)
                        if sent_time > cutoff_time:
                            self.sent_news_titles[title_hash] = sent_time
                    except:
                        continue
                
                # í¬ê¸° ì œí•œ
                if len(self.processed_news_hashes) > 2000:
                    self.processed_news_hashes = set(list(self.processed_news_hashes)[-1000:])
                
                logger.info(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ: {len(self.processed_news_hashes)}ê°œ (ê¸°ì¤€ ì™„í™”: {self.duplicate_check_hours}ì‹œê°„)")
                
        except Exception as e:
            logger.warning(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.processed_news_hashes = set()
            self.emergency_alerts_sent = {}
            self.sent_news_titles = {}
    
    def _load_critical_reports(self):
        """í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.processed_reports_file):
                with open(self.processed_reports_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(minutes=self.critical_report_cooldown_minutes)
                
                for item in data:
                    try:
                        report_time = datetime.fromisoformat(item['time'])
                        if report_time > cutoff_time:
                            self.sent_critical_reports[item['hash']] = report_time
                    except:
                        continue
                
                logger.info(f"í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€: {len(self.sent_critical_reports)}ê°œ (ì¿¨ë‹¤ìš´: {self.critical_report_cooldown_minutes}ë¶„)")
                
        except Exception as e:
            logger.warning(f"í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sent_critical_reports = {}
    
    def _save_duplicate_data(self):
        """ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥"""
        try:
            emergency_data = {}
            for hash_key, alert_time in self.emergency_alerts_sent.items():
                emergency_data[hash_key] = alert_time.isoformat()
            
            title_data = {}
            for title_hash, sent_time in self.sent_news_titles.items():
                title_data[title_hash] = sent_time.isoformat()
            
            data = {
                'processed_news_hashes': list(self.processed_news_hashes),
                'emergency_alerts_sent': emergency_data,
                'sent_news_titles': title_data,
                'last_updated': datetime.now().isoformat()
            }
            
            with open(self.persistence_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_critical_reports(self):
        """í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥"""
        try:
            data_to_save = []
            for report_hash, report_time in self.sent_critical_reports.items():
                data_to_save.append({
                    'hash': report_hash,
                    'time': report_time.isoformat()
                })
            
            with open(self.processed_reports_file, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _reset_translation_count_if_needed(self):
        """ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        now = datetime.now()
        if (now - self.last_translation_reset).total_seconds() > self.translation_reset_interval:
            old_claude_count = self.claude_translation_count
            old_gpt_count = self.gpt_translation_count
            old_error_count = self.claude_error_count
            self.claude_translation_count = 0
            self.gpt_translation_count = 0
            self.claude_error_count = 0
            self.last_translation_reset = now
            
            if self.claude_cooldown_until and now > self.claude_cooldown_until:
                self.claude_cooldown_until = None
                logger.info("Claude ì¿¨ë‹¤ìš´ í•´ì œ")
            
            if old_claude_count > 0 or old_gpt_count > 0:
                logger.info(f"ë²ˆì—­ ì¹´ìš´íŠ¸ ë¦¬ì…‹: GPT {old_gpt_count} â†’ 0, Claude {old_claude_count} â†’ 0")
    
    def _reset_summary_count_if_needed(self):
        """ìš”ì•½ ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        now = datetime.now()
        if (now - self.last_summary_reset).total_seconds() > self.translation_reset_interval:
            old_count = self.summary_count
            self.summary_count = 0
            self.last_summary_reset = now
            if old_count > 0:
                logger.info(f"ìš”ì•½ ì¹´ìš´íŠ¸ ë¦¬ì…‹: {old_count} â†’ 0")
    
    def _should_translate_for_emergency_report(self, article: Dict) -> bool:
        """ê¸´ê¸‰ ë¦¬í¬íŠ¸ ì „ì†¡ ì‹œì—ë§Œ ë²ˆì—­"""
        if not self._is_critical_news_enhanced(article):
            return False
            
        if article.get('title_ko') and article['title_ko'] != article.get('title', ''):
            return False
        
        self._reset_translation_count_if_needed()
        
        can_use_gpt = self.openai_client and self.gpt_translation_count < self.max_gpt_translations_per_15min
        can_use_claude = self._is_claude_available()
        
        return can_use_gpt or can_use_claude
    
    def _is_claude_available(self) -> bool:
        """Claude API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        if not self.anthropic_client:
            return False
        
        if self.claude_cooldown_until and datetime.now() < self.claude_cooldown_until:
            return False
        
        self._reset_translation_count_if_needed()
        
        if self.claude_translation_count >= self.max_claude_translations_per_15min:
            return False
        
        if self.claude_error_count >= 2:
            self.claude_cooldown_until = datetime.now() + timedelta(seconds=self.claude_cooldown_duration)
            logger.warning(f"Claude API ì—ëŸ¬ {self.claude_error_count}íšŒ, ì¿¨ë‹¤ìš´ ì‹œì‘")
            return False
        
        return True
    
    async def translate_text_with_claude(self, text: str, max_length: int = 400) -> str:
        """Claude API ë²ˆì—­"""
        if not self._is_claude_available():
            return ""
        
        cache_key = f"claude_{hashlib.md5(text.encode()).hexdigest()}"
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]
        
        try:
            self.processing_stats['translation_attempts'] += 1
            
            response = await self.anthropic_client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=200,
                timeout=10.0,
                messages=[{
                    "role": "user", 
                    "content": f"""ë‹¤ìŒ ì˜ë¬¸ ë‰´ìŠ¤ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

ìµœëŒ€ {max_length}ì ì´ë‚´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.

ì œëª©: {text}"""
                }]
            )
            
            translated = response.content[0].text.strip()
            
            if len(translated) > max_length:
                translated = translated[:max_length-3] + "..."
            
            self.translation_cache[cache_key] = translated
            self.claude_translation_count += 1
            self.processing_stats['translation_successes'] += 1
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.translation_cache) > 300:
                keys_to_remove = list(self.translation_cache.keys())[:150]
                for key in keys_to_remove:
                    del self.translation_cache[key]
            
            logger.info(f"ğŸ¤– Claude ë²ˆì—­ ì™„ë£Œ ({self.claude_translation_count}/{self.max_claude_translations_per_15min})")
            return translated
            
        except Exception as e:
            self.claude_error_count += 1
            self.processing_stats['api_errors'] += 1
            error_str = str(e)
            
            if "529" in error_str or "rate" in error_str.lower():
                self.claude_cooldown_until = datetime.now() + timedelta(minutes=30)
                logger.warning(f"Claude API rate limit, 30ë¶„ ì¿¨ë‹¤ìš´")
            else:
                logger.warning(f"Claude ë²ˆì—­ ì‹¤íŒ¨: {error_str[:50]}")
            
            return ""
    
    async def translate_text_with_gpt(self, text: str, max_length: int = 400) -> str:
        """GPT API ë²ˆì—­"""
        if not self.openai_client:
            return text
        
        self._reset_translation_count_if_needed()
        
        if self.gpt_translation_count >= self.max_gpt_translations_per_15min:
            logger.warning(f"GPT ë²ˆì—­ í•œë„ ì´ˆê³¼: {self.gpt_translation_count}/{self.max_gpt_translations_per_15min}")
            return text
        
        cache_key = f"gpt_{hashlib.md5(text.encode()).hexdigest()}"
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]
        
        try:
            self.processing_stats['translation_attempts'] += 1
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë¹„íŠ¸ì½”ì¸ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì˜ë¬¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ìµœëŒ€ {max_length}ì):\n\n{text}"}
                ],
                max_tokens=150,
                temperature=0.2,
                timeout=15.0
            )
            
            translated = response.choices[0].message.content.strip()
            
            if len(translated) > max_length:
                translated = translated[:max_length-3] + "..."
            
            self.translation_cache[cache_key] = translated
            self.gpt_translation_count += 1
            self.processing_stats['translation_successes'] += 1
            
            logger.info(f"ğŸ§  GPT ë²ˆì—­ ì™„ë£Œ ({self.gpt_translation_count}/{self.max_gpt_translations_per_15min})")
            return translated
            
        except Exception as e:
            self.processing_stats['api_errors'] += 1
            logger.warning(f"GPT ë²ˆì—­ ì‹¤íŒ¨: {str(e)[:50]}")
            return text
    
    async def translate_text(self, text: str, max_length: int = 400) -> str:
        """í†µí•© ë²ˆì—­ í•¨ìˆ˜ - GPT ìš°ì„ , Claude ë³´ì¡°"""
        try:
            # GPT ìš°ì„ 
            if self.openai_client:
                result = await self.translate_text_with_gpt(text, max_length)
                if result != text:
                    return result
            
            # Claude ë³´ì¡°
            if self._is_claude_available():
                result = await self.translate_text_with_claude(text, max_length)
                if result:
                    return result
            
            return text
            
        except Exception as e:
            logger.error(f"ë²ˆì—­ í•¨ìˆ˜ ì˜¤ë¥˜: {e}")
            return text
    
    def _should_use_gpt_summary(self, article: Dict) -> bool:
        """GPT ìš”ì•½ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        self._reset_summary_count_if_needed()
        
        if self.summary_count >= self.max_summaries_per_15min:
            return False
        
        if not self._is_critical_news_enhanced(article):
            return False
        
        description = article.get('description', '')
        if len(description) < 100:
            return False
        
        return True
    
    def _generate_content_hash(self, title: str, description: str = "") -> str:
        """ë‰´ìŠ¤ ë‚´ìš© í•´ì‹œ ìƒì„±"""
        content = f"{title} {description[:200]}".lower()
        
        # ìˆ«ì ì •ê·œí™”
        content = re.sub(r'[\d,]+', lambda m: m.group(0).replace(',', ''), content)
        
        # íšŒì‚¬ëª… ì¶”ì¶œ
        companies_found = []
        for company in self.important_companies:
            if company.lower() in content:
                companies_found.append(company.lower())
        
        # ì•¡ì…˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        action_keywords = []
        actions = ['bought', 'purchased', 'acquired', 'adds', 'buys', 'sells', 'sold', 
                  'announced', 'launches', 'approves', 'rejects', 'bans', 'crosses', 'hits']
        for action in actions:
            if action in content:
                action_keywords.append(action)
        
        # ê³ ìœ  ì‹ë³„ì ìƒì„±
        unique_parts = []
        if companies_found:
            unique_parts.append('_'.join(sorted(companies_found)))
        if action_keywords:
            unique_parts.append('_'.join(sorted(action_keywords)))
        
        if unique_parts:
            hash_content = '|'.join(unique_parts)
        else:
            words = re.findall(r'\b[a-z]{4,}\b', content)
            important_words = [w for w in words if w not in ['that', 'this', 'with', 'from', 'have', 'been']]
            hash_content = ' '.join(sorted(important_words[:8]))
        
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def _is_duplicate_emergency(self, article: Dict, time_window: int = None) -> bool:
        """ğŸ”¥ğŸ”¥ ê¸´ê¸‰ ì•Œë¦¼ ì¤‘ë³µ í™•ì¸ (ê¸°ì¤€ ëŒ€í­ ì™„í™”)"""
        try:
            if time_window is None:
                time_window = self.critical_report_cooldown_minutes
                
            current_time = datetime.now()
            content_hash = self._generate_content_hash(
                article.get('title', ''), 
                article.get('description', '')
            )
            
            # í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ì²´í¬
            if content_hash in self.sent_critical_reports:
                last_sent = self.sent_critical_reports[content_hash]
                time_since_last = current_time - last_sent
                
                if time_since_last < timedelta(minutes=time_window):
                    logger.info(f"ğŸ”„ ì¤‘ë³µ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ë°©ì§€ ({time_window}ë¶„): {article.get('title', '')[:50]}...")
                    return True
            
            # ìƒˆë¡œìš´ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ë¡œ ê¸°ë¡
            self.sent_critical_reports[content_hash] = current_time
            
            # ì˜¤ë˜ëœ ê¸°ë¡ ì •ë¦¬
            cutoff_time = current_time - timedelta(hours=4)
            self.sent_critical_reports = {
                k: v for k, v in self.sent_critical_reports.items()
                if v > cutoff_time
            }
            
            self._save_critical_reports()
            
            # ê¸°ì¡´ ì•Œë¦¼ ì²´í¬
            cutoff_time = current_time - timedelta(minutes=time_window)
            self.emergency_alerts_sent = {
                k: v for k, v in self.emergency_alerts_sent.items()
                if v > cutoff_time
            }
            
            if content_hash in self.emergency_alerts_sent:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ê¸´ê¸‰ ì•Œë¦¼ ë°©ì§€: {article.get('title', '')[:50]}...")
                return True
            
            self.emergency_alerts_sent[content_hash] = current_time
            self._save_duplicate_data()
            
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _is_recent_news(self, article: Dict, hours: int = 6) -> bool:
        """ë‰´ìŠ¤ ìµœì‹ ì„± í™•ì¸ (6ì‹œê°„ìœ¼ë¡œ í™•ì¥)"""
        try:
            pub_time_str = article.get('published_at', '')
            if not pub_time_str:
                return True
            
            try:
                if 'T' in pub_time_str:
                    pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                else:
                    from dateutil import parser
                    pub_time = parser.parse(pub_time_str)
                
                if pub_time.tzinfo is None:
                    pub_time = pytz.UTC.localize(pub_time)
                
                time_diff = datetime.now(pytz.UTC) - pub_time
                return time_diff.total_seconds() < (hours * 3600)
            except:
                return True
        except:
            return True
    
    async def start_monitoring(self):
        """ğŸ”¥ğŸ”¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (403 ì˜¤ë¥˜ í•´ê²° ë²„ì „)"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=20),
                connector=aiohttp.TCPConnector(limit=100, limit_per_host=30)
            )
        
        logger.info("ğŸ”¥ğŸ”¥ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (403 ì˜¤ë¥˜ í•´ê²° + ê¸°ì¤€ ì™„í™”)")
        logger.info(f"ğŸ§  GPT API: {'í™œì„±í™”' if self.openai_client else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ¤– Claude API: {'í™œì„±í™”' if self.anthropic_client else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ“Š RSS ì²´í¬: 3ì´ˆë§ˆë‹¤")
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì²´í¬: {self.duplicate_check_hours}ì‹œê°„")
        logger.info(f"â° í¬ë¦¬í‹°ì»¬ ì¿¨ë‹¤ìš´: {self.critical_report_cooldown_minutes}ë¶„")
        logger.info(f"ğŸ¯ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ: {len(self.critical_keywords)}ê°œ")
        logger.info(f"ğŸ“¡ RSS ì†ŒìŠ¤: {len(self.rss_feeds)}ê°œ")
        
        self.company_news_count = {}
        
        tasks = [
            self.monitor_rss_feeds_enhanced(),
            self.monitor_reddit_enhanced(),
            self.aggressive_api_rotation_enhanced(),
            self.log_stats_periodically()
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def log_stats_periodically(self):
        """ì •ê¸°ì  í†µê³„ ë¡œê·¸"""
        while True:
            try:
                await asyncio.sleep(1800)  # 30ë¶„ë§ˆë‹¤
                self._log_processing_stats()
            except Exception as e:
                logger.error(f"í†µê³„ ë¡œê·¸ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1800)
    
    def _log_processing_stats(self):
        """ì²˜ë¦¬ í†µê³„ ë¡œê·¸"""
        try:
            current_time = datetime.now()
            time_since_reset = current_time - self.processing_stats['last_reset']
            hours = time_since_reset.total_seconds() / 3600
            
            if hours >= 0.5:
                stats = self.processing_stats
                logger.info(f"ğŸ“Š ë‰´ìŠ¤ ì²˜ë¦¬ í†µê³„ (ìµœê·¼ {hours:.1f}ì‹œê°„):")
                logger.info(f"  ì´ ê¸°ì‚¬ í™•ì¸: {stats['total_articles_checked']}ê°œ")
                logger.info(f"  ë¹„íŠ¸ì½”ì¸ ê´€ë ¨: {stats['bitcoin_related_found']}ê°œ")
                logger.info(f"  í¬ë¦¬í‹°ì»¬ ë°œê²¬: {stats['critical_news_found']}ê°œ")
                logger.info(f"  ì¤‘ìš” ë‰´ìŠ¤: {stats['important_news_found']}ê°œ")
                logger.info(f"  ì•Œë¦¼ ì „ì†¡: {stats['alerts_sent']}ê°œ")
                
                if stats['total_articles_checked'] > 0:
                    bitcoin_rate = stats['bitcoin_related_found'] / stats['total_articles_checked'] * 100
                    logger.info(f"  ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ë¥ : {bitcoin_rate:.1f}%")
                
                if stats['bitcoin_related_found'] > 0:
                    critical_rate = stats['critical_news_found'] / stats['bitcoin_related_found'] * 100
                    logger.info(f"  í¬ë¦¬í‹°ì»¬ ë¹„ìœ¨: {critical_rate:.1f}%")
                
                # í†µê³„ ë¦¬ì…‹
                self.processing_stats = {
                    'total_articles_checked': 0,
                    'bitcoin_related_found': 0,
                    'critical_news_found': 0,
                    'important_news_found': 0,
                    'alerts_sent': 0,
                    'translation_attempts': 0,
                    'translation_successes': 0,
                    'api_errors': 0,
                    'rss_errors': 0,
                    'last_reset': current_time
                }
        except Exception as e:
            logger.error(f"í†µê³„ ë¡œê·¸ ì˜¤ë¥˜: {e}")
    
    async def monitor_rss_feeds_enhanced(self):
        """ğŸ”¥ğŸ”¥ RSS í”¼ë“œ ëª¨ë‹ˆí„°ë§ (403 ì˜¤ë¥˜ í•´ê²°)"""
        consecutive_errors = 0
        max_consecutive_errors = 8
        
        while True:
            try:
                sorted_feeds = sorted(self.rss_feeds, key=lambda x: x['weight'], reverse=True)
                successful_feeds = 0
                processed_articles = 0
                critical_found = 0
                
                for feed_info in sorted_feeds:
                    try:
                        articles = await self._parse_rss_feed_enhanced(feed_info)
                        
                        if articles:
                            successful_feeds += 1
                            
                            for article in articles:
                                self.processing_stats['total_articles_checked'] += 1
                                
                                try:
                                    # ìµœì‹  ë‰´ìŠ¤ë§Œ ì²˜ë¦¬ (6ì‹œê°„ìœ¼ë¡œ í™•ì¥)
                                    if not self._is_recent_news(article, hours=6):
                                        continue
                                    
                                    # ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ì²´í¬
                                    if not self._is_bitcoin_or_macro_related_enhanced(article):
                                        continue
                                    
                                    self.processing_stats['bitcoin_related_found'] += 1
                                    
                                    # ê¸°ì—…ëª… ì¶”ì¶œ
                                    company = self._extract_company_from_content(
                                        article.get('title', ''),
                                        article.get('description', '')
                                    )
                                    if company:
                                        article['company'] = company
                                    
                                    # í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ì²´í¬
                                    if self._is_critical_news_enhanced(article):
                                        self.processing_stats['critical_news_found'] += 1
                                        
                                        # ë²ˆì—­ ì‹œë„
                                        try:
                                            if self._should_translate_for_emergency_report(article):
                                                translated = await self.translate_text(article.get('title', ''))
                                                article['title_ko'] = translated
                                            else:
                                                article['title_ko'] = article.get('title', '')
                                        except Exception as e:
                                            logger.warning(f"ë²ˆì—­ ì˜¤ë¥˜: {e}")
                                            article['title_ko'] = article.get('title', '')
                                        
                                        # ìš”ì•½ ì‹œë„
                                        try:
                                            if self._should_use_gpt_summary(article):
                                                summary = await self.summarize_article_enhanced(
                                                    article['title'],
                                                    article.get('description', '')
                                                )
                                                if summary:
                                                    article['summary'] = summary
                                        except Exception as e:
                                            logger.warning(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
                                        
                                        # ì¤‘ë³µ ì²´í¬ í›„ ì•Œë¦¼ ì „ì†¡
                                        if not self._is_duplicate_emergency(article):
                                            article['expected_change'] = self._estimate_price_impact_enhanced(article)
                                            await self._trigger_emergency_alert_enhanced(article)
                                            processed_articles += 1
                                            critical_found += 1
                                            self.processing_stats['alerts_sent'] += 1
                                    
                                    # ì¤‘ìš” ë‰´ìŠ¤ëŠ” ë²„í¼ì— ì¶”ê°€
                                    elif self._is_important_news_enhanced(article):
                                        self.processing_stats['important_news_found'] += 1
                                        await self._add_to_news_buffer_enhanced(article)
                                        processed_articles += 1
                                
                                except Exception as e:
                                    logger.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                                    continue
                    
                    except Exception as e:
                        self.processing_stats['rss_errors'] += 1
                        logger.warning(f"RSS í”¼ë“œ ì˜¤ë¥˜ {feed_info['source']}: {str(e)[:50]}")
                        continue
                
                if processed_articles > 0:
                    logger.info(f"ğŸ”¥ RSS ìŠ¤ìº”: {successful_feeds}ê°œ í”¼ë“œ, {processed_articles}ê°œ ê´€ë ¨ ë‰´ìŠ¤ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ)")
                    consecutive_errors = 0
                else:
                    logger.debug(f"ğŸ“¡ RSS ìŠ¤ìº”: {successful_feeds}ê°œ í”¼ë“œ í™œì„±, ìƒˆ ë‰´ìŠ¤ ì—†ìŒ")
                
                await asyncio.sleep(3)  # 3ì´ˆë§ˆë‹¤
                
            except Exception as e:
                consecutive_errors += 1
                self.processing_stats['rss_errors'] += 1
                logger.error(f"RSS ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜ ({consecutive_errors}/{max_consecutive_errors}): {e}")
                
                if consecutive_errors >= max_consecutive_errors:
                    logger.error(f"ì—°ì† {max_consecutive_errors}íšŒ ì˜¤ë¥˜, 30ì´ˆ ëŒ€ê¸°")
                    await asyncio.sleep(30)
                    consecutive_errors = 0
                else:
                    await asyncio.sleep(10)
    
    def _is_bitcoin_or_macro_related_enhanced(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ì„± ì²´í¬ (ê¸°ì¤€ ì™„í™”)"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰
        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸']
        if any(keyword in content for keyword in bitcoin_keywords):
            return True
        
        # ì•”í˜¸í™”í + ì¤‘ìš” í‚¤ì›Œë“œ
        crypto_keywords = ['crypto', 'cryptocurrency', 'ì•”í˜¸í™”í']
        if any(keyword in content for keyword in crypto_keywords):
            important_terms = ['etf', 'sec', 'regulation', 'approval', 'russia', 'sberbank', 'bonds']
            if any(term in content for term in important_terms):
                return True
        
        # Fed ê¸ˆë¦¬ (ì¤‘ìš”)
        fed_keywords = ['fed rate', 'fomc', 'powell', 'federal reserve', 'interest rate decision']
        if any(keyword in content for keyword in fed_keywords):
            return True
        
        # ê²½ì œ ì§€í‘œ
        economic_keywords = ['inflation data', 'cpi report', 'unemployment rate', 'gdp growth']
        if any(keyword in content for keyword in economic_keywords):
            return True
        
        # ë¬´ì—­/ê´€ì„¸
        trade_keywords = ['trump tariffs', 'china tariffs', 'trade war', 'trade deal']
        if any(keyword in content for keyword in trade_keywords):
            return True
        
        # ì¤‘ìš” ê¸°ì—…
        for company in self.important_companies:
            if company.lower() in content:
                relevant_terms = ['bitcoin', 'crypto', 'investment', 'purchase', 'announces']
                if any(term in content for term in relevant_terms):
                    return True
        
        return False
    
    def _is_critical_news_enhanced(self, article: Dict) -> bool:
        """ğŸ”¥ğŸ”¥ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨ (ê¸°ì¤€ ëŒ€í­ ì™„í™”)"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        if not self._is_bitcoin_or_macro_related_enhanced(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ğŸ”¥ğŸ”¥ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ì²´í¬ (ê¸°ì¤€ ì™„í™”)
        for keyword in self.critical_keywords:
            if keyword.lower() in content:
                # ë¶€ì •ì  í•„í„°
                negative_filters = ['rumor', 'speculation', 'unconfirmed', 'fake', 'allegedly']
                if any(neg in content for neg in negative_filters):
                    continue
                
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ê°ì§€: '{keyword}' - {article.get('title', '')[:50]}...")
                return True
        
        # ğŸ”¥ğŸ”¥ íŒ¨í„´ ë§¤ì¹­ (ê¸°ì¤€ ì™„í™”)
        critical_patterns = [
            ('bitcoin', 'etf'),
            ('bitcoin', 'sec'),
            ('bitcoin', 'ban'),
            ('bitcoin', 'regulation'),
            ('bitcoin', 'crosses'),
            ('bitcoin', '100k'),
            ('tesla', 'bitcoin'),
            ('microstrategy', 'bitcoin'),
            ('sberbank', 'bitcoin'),
            ('russia', 'bitcoin'),
            ('fed', 'rate'),
            ('trump', 'tariffs'),
            ('inflation', 'data'),
            ('trade', 'deal')
        ]
        
        score = 0
        for pattern in critical_patterns:
            if all(word in content for word in pattern):
                score += 1
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ íŒ¨í„´: {pattern}")
        
        # ğŸ”¥ğŸ”¥ ê¸°ì¤€ ì ìˆ˜ ì™„í™” (1ì  ì´ìƒì´ë©´ í¬ë¦¬í‹°ì»¬)
        if score >= 1:
            logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ ìŠ¹ì¸: íŒ¨í„´ ì ìˆ˜ {score}ì ")
            return True
        
        return False
    
    def _is_important_news_enhanced(self, article: Dict) -> bool:
        """ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨ (ê¸°ì¤€ ì™„í™”)"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        if not self._is_bitcoin_or_macro_related_enhanced(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        # ì¡°ê±´ë“¤ (ê¸°ì¤€ ì™„í™”)
        conditions = [
            category == 'crypto' and weight >= 4,  # 5 â†’ 4
            category == 'finance' and weight >= 4,  # 5 â†’ 4
            category == 'api' and weight >= 5,  # 6 â†’ 5
            any(company.lower() in content for company in self.important_companies) and 
            any(word in content for word in ['bitcoin', 'crypto', 'investment']),
            any(word in content for word in ['fed rate', 'inflation', 'trade deal']) and weight >= 4
        ]
        
        return any(conditions)
    
    def _estimate_price_impact_enhanced(self, article: Dict) -> str:
        """í˜„ì‹¤ì  ê°€ê²© ì˜í–¥ ì¶”ì •"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ETF ê´€ë ¨
        if 'etf approved' in content or 'etf approval' in content:
            return 'ğŸš€ ìƒìŠ¹ 2.0~3.5% (24ì‹œê°„ ë‚´)'
        elif 'etf rejected' in content or 'etf delay' in content:
            return 'ğŸ”» í•˜ë½ 1.5~2.5% (12ì‹œê°„ ë‚´)'
        
        # Fed ê´€ë ¨
        elif 'fed cuts rates' in content or 'rate cut' in content:
            return 'ğŸ“ˆ ìƒìŠ¹ 1.0~2.0% (8ì‹œê°„ ë‚´)'
        elif 'fed raises rates' in content or 'rate hike' in content:
            return 'ğŸ“‰ í•˜ë½ 0.8~1.5% (6ì‹œê°„ ë‚´)'
        
        # ê¸°ì—… êµ¬ë§¤
        elif 'tesla' in content and 'bitcoin' in content:
            return 'ğŸš€ ìƒìŠ¹ 1.2~2.5% (18ì‹œê°„ ë‚´)'
        elif 'microstrategy' in content and 'bitcoin' in content:
            return 'ğŸ“ˆ ìƒìŠ¹ 0.4~1.0% (8ì‹œê°„ ë‚´)'
        
        # êµ¬ì¡°í™” ìƒí’ˆ
        elif any(word in content for word in ['structured', 'bonds', 'linked']):
            return 'ğŸ“Š ë¯¸ë¯¸í•œ ë°˜ì‘ +0.05~0.2% (4ì‹œê°„ ë‚´)'
        
        # ê·œì œ
        elif 'china bans bitcoin' in content or 'bitcoin banned' in content:
            return 'ğŸ”» í•˜ë½ 2.0~4.0% (24ì‹œê°„ ë‚´)'
        elif 'regulatory clarity' in content or 'bitcoin approved' in content:
            return 'ğŸ“ˆ ìƒìŠ¹ 0.8~1.8% (12ì‹œê°„ ë‚´)'
        
        # ë¬´ì—­/ê´€ì„¸
        elif 'tariffs' in content or 'trade war' in content:
            return 'ğŸ“‰ í•˜ë½ 0.3~1.0% (6ì‹œê°„ ë‚´)'
        elif 'trade deal' in content:
            return 'ğŸ“ˆ ìƒìŠ¹ 0.2~0.8% (8ì‹œê°„ ë‚´)'
        
        # ì¸í”Œë ˆì´ì…˜
        elif 'inflation' in content or 'cpi' in content:
            return 'ğŸ“ˆ ìƒìŠ¹ 0.3~1.0% (6ì‹œê°„ ë‚´)'
        
        # í•´í‚¹
        elif 'hack' in content or 'stolen' in content:
            return 'ğŸ“‰ í•˜ë½ 0.2~1.0% (4ì‹œê°„ ë‚´)'
        
        # ê¸°ë³¸ê°’
        return 'âš¡ ë³€ë™ Â±0.2~0.8% (ë‹¨ê¸°)'
    
    async def summarize_article_enhanced(self, title: str, description: str, max_length: int = 200) -> str:
        """ê°œì„ ëœ ìš”ì•½"""
        # ê¸°ë³¸ ìš”ì•½ ìš°ì„ 
        try:
            basic_summary = self._generate_basic_summary_enhanced(title, description)
            if basic_summary and len(basic_summary.strip()) > 30:
                return basic_summary
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ìš”ì•½ ì˜¤ë¥˜: {e}")
        
        # GPT ìš”ì•½
        if not self.openai_client or not description:
            return "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤. íˆ¬ììë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤."
        
        if len(description) <= 150:
            return basic_summary or "ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ë°œí‘œê°€ ìˆì—ˆë‹¤."
        
        self._reset_summary_count_if_needed()
        
        if self.summary_count >= self.max_summaries_per_15min:
            return basic_summary or "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤."
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë¹„íŠ¸ì½”ì¸ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”."},
                    {"role": "user", "content": f"3ë¬¸ì¥ ìš”ì•½ (ìµœëŒ€ {max_length}ì):\n\nì œëª©: {title}\n\në‚´ìš©: {description[:600]}"}
                ],
                max_tokens=200,
                temperature=0.2,
                timeout=15.0
            )
            
            summary = response.choices[0].message.content.strip()
            
            if len(summary) > max_length:
                sentences = summary.split('.')
                result = ""
                for sentence in sentences[:3]:
                    if len(result + sentence + ".") <= max_length - 3:
                        result += sentence + "."
                    else:
                        break
                summary = result.strip() or summary[:max_length-3] + "..."
            
            self.summary_count += 1
            logger.info(f"ğŸ“ GPT ìš”ì•½ ì™„ë£Œ ({self.summary_count}/{self.max_summaries_per_15min})")
            
            return summary
            
        except Exception as e:
            self.processing_stats['api_errors'] += 1
            logger.warning(f"GPT ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}")
            return basic_summary or "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤."
    
    def _generate_basic_summary_enhanced(self, title: str, description: str) -> str:
        """ê¸°ë³¸ ìš”ì•½ ìƒì„±"""
        try:
            content = (title + " " + description).lower()
            summary_parts = []
            
            # êµ¬ì¡°í™” ìƒí’ˆ íŠ¹ë³„ ì²˜ë¦¬
            if 'sberbank' in content and any(word in content for word in ['structured', 'bonds', 'linked']):
                summary_parts.append("ëŸ¬ì‹œì•„ ìµœëŒ€ ì€í–‰ ìŠ¤ë² ë¥´ë°©í¬ê°€ ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì— ì—°ë™ëœ êµ¬ì¡°í™” ì±„ê¶Œì„ ì¶œì‹œí–ˆë‹¤.")
                summary_parts.append("ì´ëŠ” ì§ì ‘ì ì¸ ë¹„íŠ¸ì½”ì¸ ë§¤ìˆ˜ê°€ ì•„ë‹Œ ê°€ê²© ì¶”ì  ìƒí’ˆìœ¼ë¡œ, ì‹¤ì œ BTC ìˆ˜ìš” ì°½ì¶œ íš¨ê³¼ëŠ” ì œí•œì ì´ë‹¤.")
                summary_parts.append("ëŸ¬ì‹œì•„ ì œì¬ ìƒí™©ê³¼ OTC ê±°ë˜ë¡œ ì¸í•´ ê¸€ë¡œë²Œ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì¦‰ê°ì  ì˜í–¥ì€ ë¯¸ë¯¸í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.")
                return " ".join(summary_parts)
            
            # ê¸°ì—… êµ¬ë§¤
            if any(company in content for company in ['tesla', 'microstrategy']) and 'bitcoin' in content:
                if 'tesla' in content:
                    summary_parts.append("í…ŒìŠ¬ë¼ê°€ ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ë§¤ì…ì„ ë°œí‘œí–ˆë‹¤.")
                    summary_parts.append("ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ ì˜í–¥ë ¥ê³¼ í•¨ê»˜ ì‹œì¥ì— ìƒë‹¹í•œ ê´€ì‹¬ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.")
                elif 'microstrategy' in content:
                    summary_parts.append("ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€ê°€ ë¹„íŠ¸ì½”ì¸ì„ ì¶”ê°€ ë§¤ì…í–ˆë‹¤.")
                    summary_parts.append("ê¸°ì—…ì˜ ì§€ì†ì ì¸ ë¹„íŠ¸ì½”ì¸ ë§¤ì… ì „ëµì˜ ì¼í™˜ìœ¼ë¡œ ì‹œì¥ì— ê¸ì •ì  ì‹ í˜¸ë¥¼ ë³´ë‚¸ë‹¤.")
                summary_parts.append("ê¸°ì—…ì˜ ë¹„íŠ¸ì½”ì¸ ì±„íƒ í™•ì‚°ì— ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ì „ë§ì´ë‹¤.")
                return " ".join(summary_parts)
            
            # ETF ê´€ë ¨
            if 'etf' in content:
                if 'approved' in content:
                    summary_parts.append("ë¹„íŠ¸ì½”ì¸ í˜„ë¬¼ ETF ìŠ¹ì¸ ì†Œì‹ì´ ì „í•´ì¡Œë‹¤.")
                    summary_parts.append("ETF ìŠ¹ì¸ì€ ê¸°ê´€ íˆ¬ììë“¤ì˜ ìê¸ˆ ìœ ì…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì¤‘ìš”í•œ ì´ì •í‘œë‹¤.")
                    summary_parts.append("ë¹„íŠ¸ì½”ì¸ ì‹œì¥ì˜ ì œë„í™”ì™€ ì£¼ë¥˜ ì±„íƒì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.")
                else:
                    summary_parts.append("ë¹„íŠ¸ì½”ì¸ ETF ê´€ë ¨ ì¤‘ìš”í•œ ë°œí‘œê°€ ìˆì—ˆë‹¤.")
                    summary_parts.append("ETF ìŠ¹ì¸ì€ ê¸°ê´€ íˆ¬ììë“¤ì˜ ë¹„íŠ¸ì½”ì¸ ê´€ì‹¬ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤.")
                    summary_parts.append("ì‹œì¥ì˜ ì œë„í™” ì§„í–‰ ìƒí™©ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ í‰ê°€ëœë‹¤.")
                return " ".join(summary_parts)
            
            # Fed ê´€ë ¨
            if 'fed' in content or 'rate' in content:
                summary_parts.append("ì—°ì¤€ì˜ ê¸ˆë¦¬ ì •ì±… ë°œí‘œê°€ ìˆì—ˆë‹¤.")
                summary_parts.append("ê¸ˆë¦¬ ì •ì±…ì€ ë¹„íŠ¸ì½”ì¸ì„ í¬í•¨í•œ ë¦¬ìŠ¤í¬ ìì‚°ì— ì§ì ‘ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤.")
                summary_parts.append("íˆ¬ììë“¤ì€ ì •ì±… ë°©í–¥ì„±ì— ë”°ë¥¸ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì •ì„ ê³ ë ¤í•˜ê³  ìˆë‹¤.")
                return " ".join(summary_parts)
            
            # ê´€ì„¸ ê´€ë ¨
            if 'tariff' in content or 'trade' in content:
                summary_parts.append("ë¯¸êµ­ì˜ ë¬´ì—­ ì •ì±… ë°œí‘œê°€ ìˆì—ˆë‹¤.")
                summary_parts.append("ë¬´ì—­ ì •ì±… ë³€í™”ëŠ” ê¸€ë¡œë²Œ ì‹œì¥ê³¼ ë‹¬ëŸ¬ ê°•ì„¸ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.")
                summary_parts.append("ë‹¬ëŸ¬ ì•½ì„¸ ìš”ì¸ì´ ë¹„íŠ¸ì½”ì¸ì—ëŠ” ì¤‘ì¥ê¸°ì ìœ¼ë¡œ ìœ ë¦¬í•  ê²ƒìœ¼ë¡œ ë¶„ì„ëœë‹¤.")
                return " ".join(summary_parts)
            
            # ê¸°ë³¸ ì¼€ì´ìŠ¤
            if title and len(title) > 10:
                summary_parts.append("ë¹„íŠ¸ì½”ì¸ ì‹œì¥ê³¼ ê´€ë ¨ëœ ì¤‘ìš”í•œ ì†Œì‹ì´ ë°œí‘œë˜ì—ˆë‹¤.")
            else:
                summary_parts.append("ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆë‹¤.")
            
            summary_parts.append("íˆ¬ììë“¤ì€ ì´ë²ˆ ì†Œì‹ì˜ ì‹¤ì œ ì‹œì¥ ì˜í–¥ì„ ë¶„ì„í•˜ê³  ìˆë‹¤.")
            summary_parts.append("ë‹¨ê¸° ë³€ë™ì„±ì€ ìˆê² ì§€ë§Œ ì¥ê¸° íŠ¸ë Œë“œ ì§€ì†ì´ ì˜ˆìƒëœë‹¤.")
            
            return " ".join(summary_parts[:3])
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ì†Œì‹ì´ ë°œí‘œë˜ì—ˆë‹¤. ì‹œì¥ ë°˜ì‘ì„ ì§€ì¼œë³¼ í•„ìš”ê°€ ìˆë‹¤."
    
    async def _trigger_emergency_alert_enhanced(self, article: Dict):
        """ê¸´ê¸‰ ì•Œë¦¼ íŠ¸ë¦¬ê±° (ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return
            
            self.processed_news_hashes.add(content_hash)
            
            # í¬ê¸° ì œí•œ
            if len(self.processed_news_hashes) > 3000:
                self.processed_news_hashes = set(list(self.processed_news_hashes)[-1500:])
            
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ ê¸°ë¡
            if content_hash not in self.news_first_seen:
                self.news_first_seen[content_hash] = datetime.now()
            
            # ë²ˆì—­ ì‹œë„ (ì‹¤íŒ¨í•´ë„ ë‰´ìŠ¤ ì „ì†¡)
            try:
                if self._should_translate_for_emergency_report(article):
                    translated_title = await self.translate_text(article.get('title', ''))
                    article['title_ko'] = translated_title
                else:
                    article['title_ko'] = article.get('title', '')
            except Exception as e:
                logger.warning(f"ë²ˆì—­ ì˜¤ë¥˜, ì›ë¬¸ ì‚¬ìš©: {e}")
                article['title_ko'] = article.get('title', '')
            
            # ì´ë²¤íŠ¸ ìƒì„±
            event = {
                'type': 'critical_news',
                'title': article.get('title', ''),
                'title_ko': article.get('title_ko', article.get('title', '')),
                'description': article.get('description', '')[:1200],
                'summary': article.get('summary', ''),
                'company': article.get('company', ''),
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self._determine_impact_enhanced(article),
                'expected_change': article.get('expected_change', 'Â±0.5%'),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown'),
                'published_at': article.get('published_at', ''),
                'first_seen': self.news_first_seen[content_hash]
            }
            
            self._save_duplicate_data()
            
            # ë°ì´í„° ì»¬ë ‰í„°ì— ì „ë‹¬
            if hasattr(self, 'data_collector') and self.data_collector:
                self.data_collector.events_buffer.append(event)
            
            logger.critical(f"ğŸš¨ğŸš¨ í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤: {event['title_ko'][:60]}... (ì˜ˆìƒ: {event['expected_change']})")
            
        except Exception as e:
            logger.error(f"ê¸´ê¸‰ ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # í´ë°± ì´ë²¤íŠ¸ ìƒì„± (ì˜¤ë¥˜ ì‹œì—ë„ ë‰´ìŠ¤ ì „ì†¡)
            try:
                fallback_event = {
                    'type': 'critical_news',
                    'title': article.get('title', 'Unknown Title'),
                    'title_ko': article.get('title', 'Unknown Title'),
                    'description': article.get('description', '')[:500],
                    'source': article.get('source', 'Unknown'),
                    'timestamp': datetime.now(),
                    'severity': 'critical',
                    'impact': 'ğŸ“Š ì‹œì¥ ê´€ì‹¬',
                    'expected_change': 'âš¡ ë³€ë™ Â±0.3~1.0%',
                    'weight': 5
                }
                
                if hasattr(self, 'data_collector') and self.data_collector:
                    self.data_collector.events_buffer.append(fallback_event)
                
                logger.warning(f"ğŸš¨ í´ë°± ì´ë²¤íŠ¸ ìƒì„±: {article.get('title', '')[:50]}...")
                
            except Exception as e2:
                logger.error(f"í´ë°± ì´ë²¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e2}")
    
    def _determine_impact_enhanced(self, article: Dict) -> str:
        """ì˜í–¥ë„ íŒë‹¨"""
        expected_change = self._estimate_price_impact_enhanced(article)
        
        if 'ğŸš€' in expected_change or any(x in expected_change for x in ['3%', '4%', '2.5%']):
            return "ğŸš€ ë§¤ìš° ê°•í•œ í˜¸ì¬"
        elif 'ğŸ“ˆ' in expected_change and any(x in expected_change for x in ['1.5%', '2%']):
            return "ğŸ“ˆ ê°•í•œ í˜¸ì¬"
        elif 'ğŸ“ˆ' in expected_change:
            return "ğŸ“ˆ í˜¸ì¬"
        elif 'ğŸ”»' in expected_change or any(x in expected_change for x in ['3%', '4%']):
            return "ğŸ”» ë§¤ìš° ê°•í•œ ì•…ì¬"
        elif 'ğŸ“‰' in expected_change and any(x in expected_change for x in ['1.5%', '2%']):
            return "ğŸ“‰ ê°•í•œ ì•…ì¬"
        elif 'ğŸ“‰' in expected_change:
            return "ğŸ“‰ ì•…ì¬"
        else:
            return "âš¡ ë³€ë™ì„± í™•ëŒ€"
    
    async def _add_to_news_buffer_enhanced(self, article: Dict):
        """ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€"""
        try:
            content_hash = self._generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return
            
            # ì œëª© ìœ ì‚¬ì„± ì²´í¬
            new_title = article.get('title', '').lower()
            for existing in self.news_buffer:
                if self._is_similar_news_enhanced(new_title, existing.get('title', '')):
                    return
            
            # íšŒì‚¬ë³„ ë‰´ìŠ¤ ì¹´ìš´íŠ¸ ì²´í¬
            for company in self.important_companies:
                if company.lower() in new_title:
                    if self.company_news_count.get(company.lower(), 0) >= 5:
                        return
                    self.company_news_count[company.lower()] = self.company_news_count.get(company.lower(), 0) + 1
            
            self.news_buffer.append(article)
            self.processed_news_hashes.add(content_hash)
            self._save_duplicate_data()
            
            # ë²„í¼ í¬ê¸° ê´€ë¦¬
            if len(self.news_buffer) > 150:
                self.news_buffer.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
                self.news_buffer = self.news_buffer[:150]
            
            logger.debug(f"âœ… ì¤‘ìš” ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€: {new_title[:50]}...")
        
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë²„í¼ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    def _is_similar_news_enhanced(self, title1: str, title2: str) -> bool:
        """ìœ ì‚¬ ë‰´ìŠ¤ íŒë³„"""
        try:
            clean1 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title1.lower())
            clean2 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title2.lower())
            
            clean1 = re.sub(r'\s+', ' ', clean1).strip()
            clean2 = re.sub(r'\s+', ' ', clean2).strip()
            
            # íšŒì‚¬ë³„ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ ì²´í¬
            for company in self.important_companies:
                company_lower = company.lower()
                if company_lower in clean1 and company_lower in clean2:
                    bitcoin_keywords = ['bitcoin', 'btc', 'crypto']
                    if any(keyword in clean1 for keyword in bitcoin_keywords) and \
                       any(keyword in clean2 for keyword in bitcoin_keywords):
                        return True
            
            # ë‹¨ì–´ ì§‘í•© ë¹„êµ
            words1 = set(clean1.split())
            words2 = set(clean2.split())
            
            if not words1 or not words2:
                return False
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            similarity = intersection / union if union > 0 else 0
            
            return similarity > 0.75  # 75% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
        except Exception as e:
            logger.error(f"ìœ ì‚¬ ë‰´ìŠ¤ íŒë³„ ì˜¤ë¥˜: {e}")
            return False
    
    async def monitor_reddit_enhanced(self):
        """Reddit ëª¨ë‹ˆí„°ë§"""
        reddit_subreddits = [
            {'name': 'Bitcoin', 'threshold': 200, 'weight': 9},
            {'name': 'CryptoCurrency', 'threshold': 500, 'weight': 8},
            {'name': 'BitcoinMarkets', 'threshold': 100, 'weight': 9},
            {'name': 'investing', 'threshold': 600, 'weight': 7},
        ]
        
        while True:
            try:
                for sub_info in reddit_subreddits:
                    try:
                        url = f"https://www.reddit.com/r/{sub_info['name']}/hot.json?limit=10"
                        headers = {'User-Agent': self._get_random_user_agent()}
                        
                        async with self.session.get(url, headers=headers) as response:
                            if response.status == 200:
                                data = await response.json()
                                posts = data['data']['children']
                                
                                for post in posts:
                                    try:
                                        post_data = post['data']
                                        
                                        if post_data['ups'] > sub_info['threshold']:
                                            article = {
                                                'title': post_data['title'],
                                                'title_ko': post_data['title'],
                                                'description': post_data.get('selftext', '')[:1200],
                                                'url': f"https://reddit.com{post_data['permalink']}",
                                                'source': f"Reddit r/{sub_info['name']}",
                                                'published_at': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                                                'upvotes': post_data['ups'],
                                                'weight': sub_info['weight'],
                                                'category': 'social'
                                            }
                                            
                                            self.processing_stats['total_articles_checked'] += 1
                                            
                                            if self._is_bitcoin_or_macro_related_enhanced(article):
                                                self.processing_stats['bitcoin_related_found'] += 1
                                                
                                                if self._is_critical_news_enhanced(article):
                                                    self.processing_stats['critical_news_found'] += 1
                                                    
                                                    if not self._is_duplicate_emergency(article):
                                                        article['expected_change'] = self._estimate_price_impact_enhanced(article)
                                                        await self._trigger_emergency_alert_enhanced(article)
                                                        self.processing_stats['alerts_sent'] += 1
                                                
                                                elif self._is_important_news_enhanced(article):
                                                    self.processing_stats['important_news_found'] += 1
                                                    await self._add_to_news_buffer_enhanced(article)
                                    
                                    except Exception as e:
                                        logger.warning(f"Reddit í¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                                        continue
                    
                    except Exception as e:
                        self.processing_stats['rss_errors'] += 1
                        logger.warning(f"Reddit ì˜¤ë¥˜ {sub_info['name']}: {str(e)[:50]}")
                
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤
                
            except Exception as e:
                logger.error(f"Reddit ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(600)
    
    async def aggressive_api_rotation_enhanced(self):
        """API ìˆœí™˜ ì‚¬ìš©"""
        while True:
            try:
                self._reset_daily_usage()
                
                # NewsAPI
                if self.newsapi_key and self.api_usage['newsapi_today'] < self.api_limits['newsapi']:
                    try:
                        await self._call_newsapi_enhanced()
                        self.api_usage['newsapi_today'] += 1
                        logger.info(f"âœ… NewsAPI í˜¸ì¶œ ({self.api_usage['newsapi_today']}/{self.api_limits['newsapi']})")
                    except Exception as e:
                        self.processing_stats['api_errors'] += 1
                        logger.error(f"NewsAPI ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(600)
                
                # NewsData API
                if self.newsdata_key and self.api_usage['newsdata_today'] < self.api_limits['newsdata']:
                    try:
                        await self._call_newsdata_enhanced()
                        self.api_usage['newsdata_today'] += 1
                        logger.info(f"âœ… NewsData í˜¸ì¶œ ({self.api_usage['newsdata_today']}/{self.api_limits['newsdata']})")
                    except Exception as e:
                        self.processing_stats['api_errors'] += 1
                        logger.error(f"NewsData ì˜¤ë¥˜: {str(e)[:100]}")
                
                await asyncio.sleep(1200)
                
            except Exception as e:
                logger.error(f"API ìˆœí™˜ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1800)
    
    async def _call_newsapi_enhanced(self):
        """NewsAPI í˜¸ì¶œ (ì˜¤ë¥˜ ë°©ì§€)"""
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': '(bitcoin OR btc OR "bitcoin etf" OR "fed rate" OR "trump tariffs" OR "trade deal" OR "sberbank bitcoin" OR "russia bitcoin" OR "bitcoin crosses 100k") AND NOT ("altcoin only" OR "how to mine")',
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.newsapi_key,
                'pageSize': 80,
                'from': (datetime.now() - timedelta(hours=4)).isoformat()
            }
            
            headers = {'User-Agent': self._get_random_user_agent()}
            
            async with self.session.get(url, params=params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('articles', [])
                    
                    processed = 0
                    critical_found = 0
                    for article in articles:
                        try:
                            formatted_article = {
                                'title': article.get('title', ''),
                                'title_ko': article.get('title', ''),
                                'description': article.get('description', '')[:1200],
                                'url': article.get('url', ''),
                                'source': f"NewsAPI ({article.get('source', {}).get('name', 'Unknown')})",
                                'published_at': article.get('publishedAt', ''),
                                'weight': 9,
                                'category': 'api'
                            }
                            
                            self.processing_stats['total_articles_checked'] += 1
                            
                            if self._is_bitcoin_or_macro_related_enhanced(formatted_article):
                                self.processing_stats['bitcoin_related_found'] += 1
                                
                                if self._is_critical_news_enhanced(formatted_article):
                                    self.processing_stats['critical_news_found'] += 1
                                    
                                    if not self._is_duplicate_emergency(formatted_article):
                                        formatted_article['expected_change'] = self._estimate_price_impact_enhanced(formatted_article)
                                        await self._trigger_emergency_alert_enhanced(formatted_article)
                                    processed += 1
                                    critical_found += 1
                                    self.processing_stats['alerts_sent'] += 1
                                elif self._is_important_news_enhanced(formatted_article):
                                    self.processing_stats['important_news_found'] += 1
                                    await self._add_to_news_buffer_enhanced(formatted_article)
                                    processed += 1
                        
                        except Exception as e:
                            logger.warning(f"NewsAPI ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            continue
                    
                    if processed > 0:
                        logger.info(f"ğŸ”¥ NewsAPI: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ)")
                else:
                    logger.warning(f"NewsAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsAPI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _call_newsdata_enhanced(self):
        """NewsData API í˜¸ì¶œ"""
        try:
            url = "https://newsdata.io/api/1/news"
            params = {
                'apikey': self.newsdata_key,
                'q': 'bitcoin OR btc OR "bitcoin etf" OR "sberbank bitcoin" OR "russia bitcoin" OR "fed rate decision" OR "trump tariffs" OR "bitcoin crosses 100k"',
                'language': 'en',
                'category': 'business,top',
                'size': 40
            }
            
            headers = {'User-Agent': self._get_random_user_agent()}
            
            async with self.session.get(url, params=params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    articles = data.get('results', [])
                    
                    processed = 0
                    critical_found = 0
                    for article in articles:
                        try:
                            formatted_article = {
                                'title': article.get('title', ''),
                                'title_ko': article.get('title', ''),
                                'description': article.get('description', '')[:1200],
                                'url': article.get('link', ''),
                                'source': f"NewsData ({article.get('source_id', 'Unknown')})",
                                'published_at': article.get('pubDate', ''),
                                'weight': 8,
                                'category': 'api'
                            }
                            
                            self.processing_stats['total_articles_checked'] += 1
                            
                            if self._is_bitcoin_or_macro_related_enhanced(formatted_article):
                                self.processing_stats['bitcoin_related_found'] += 1
                                
                                if self._is_critical_news_enhanced(formatted_article):
                                    self.processing_stats['critical_news_found'] += 1
                                    
                                    if not self._is_duplicate_emergency(formatted_article):
                                        formatted_article['expected_change'] = self._estimate_price_impact_enhanced(formatted_article)
                                        await self._trigger_emergency_alert_enhanced(formatted_article)
                                    processed += 1
                                    critical_found += 1
                                    self.processing_stats['alerts_sent'] += 1
                                elif self._is_important_news_enhanced(formatted_article):
                                    self.processing_stats['important_news_found'] += 1
                                    await self._add_to_news_buffer_enhanced(formatted_article)
                                    processed += 1
                        
                        except Exception as e:
                            logger.warning(f"NewsData ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            continue
                    
                    if processed > 0:
                        logger.info(f"ğŸ”¥ NewsData: {processed}ê°œ ê´€ë ¨ ë‰´ìŠ¤ (í¬ë¦¬í‹°ì»¬: {critical_found}ê°œ)")
                else:
                    logger.warning(f"NewsData ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
        
        except Exception as e:
            logger.error(f"NewsData í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    async def _parse_rss_feed_enhanced(self, feed_info: Dict) -> List[Dict]:
        """ğŸ”¥ğŸ”¥ RSS í”¼ë“œ íŒŒì‹± (403 ì˜¤ë¥˜ í•´ê²°)"""
        articles = []
        try:
            # ëœë¤ User-Agent ì‚¬ìš©
            headers = {
                'User-Agent': self._get_random_user_agent(),
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'en-US,en;q=0.9',
                'Cache-Control': 'no-cache'
            }
            
            # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            async with self.session.get(
                feed_info['url'], 
                timeout=aiohttp.ClientTimeout(total=8),
                headers=headers
            ) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    if feed.entries:
                        limit = min(15, max(6, feed_info['weight']))
                        
                        for entry in feed.entries[:limit]:
                            try:
                                # ë°œí–‰ ì‹œê°„ ì²˜ë¦¬
                                pub_time = datetime.now().isoformat()
                                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                    try:
                                        pub_time = datetime(*entry.published_parsed[:6]).isoformat()
                                    except:
                                        pass
                                elif hasattr(entry, 'published'):
                                    try:
                                        from dateutil import parser
                                        pub_time = parser.parse(entry.published).isoformat()
                                    except:
                                        pass
                                
                                title = entry.get('title', '').strip()
                                description = entry.get('summary', '').strip()
                                url = entry.get('link', '').strip()
                                
                                # ê¸°ë³¸ ê²€ì¦
                                if not title or len(title) < 10:
                                    continue
                                if not url or not url.startswith('http'):
                                    continue
                                
                                article = {
                                    'title': title[:400],
                                    'description': description[:1200],
                                    'url': url,
                                    'source': feed_info['source'],
                                    'published_at': pub_time,
                                    'weight': feed_info['weight'],
                                    'category': feed_info.get('category', 'unknown')
                                }
                                
                                articles.append(article)
                                        
                            except Exception as e:
                                logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)[:50]}")
                                continue
                
                elif response.status == 403:
                    logger.warning(f"ğŸš« {feed_info['source']}: ì ‘ê·¼ ê±°ë¶€ (403) - User-Agent ë¡œí…Œì´ì…˜ ì¤‘")
                elif response.status == 429:
                    logger.warning(f"â° {feed_info['source']}: Rate limit (429)")
                else:
                    logger.warning(f"âŒ {feed_info['source']}: HTTP {response.status}")
        
        except asyncio.TimeoutError:
            logger.debug(f"â° {feed_info['source']}: íƒ€ì„ì•„ì›ƒ")
        except aiohttp.ClientConnectorError:
            logger.debug(f"ğŸ”Œ {feed_info['source']}: ì—°ê²° ì˜¤ë¥˜")
        except Exception as e:
            logger.debug(f"âŒ {feed_info['source']}: {str(e)[:50]}")
        
        return articles
    
    def _extract_company_from_content(self, title: str, description: str = "") -> str:
        """ì»¨í…ì¸ ì—ì„œ ê¸°ì—…ëª… ì¶”ì¶œ"""
        try:
            content = (title + " " + description).lower()
            
            for company in self.important_companies:
                if company.lower() in content:
                    # ì›ë˜ ëŒ€ì†Œë¬¸ì ìœ ì§€
                    for original in self.important_companies:
                        if original.lower() == company.lower():
                            return original.title()
            
            return ""
        except Exception as e:
            logger.error(f"ê¸°ì—…ëª… ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""
    
    def _reset_daily_usage(self):
        """ì¼ì¼ ì‚¬ìš©ëŸ‰ ë¦¬ì…‹"""
        try:
            today = datetime.now().date()
            if today > self.api_usage['last_reset']:
                self.api_usage.update({
                    'newsapi_today': 0,
                    'newsdata_today': 0,
                    'alpha_vantage_today': 0,
                    'last_reset': today
                })
                self.company_news_count = {}
                self.claude_translation_count = 0
                self.gpt_translation_count = 0
                self.claude_error_count = 0
                self.summary_count = 0
                self.last_translation_reset = datetime.now()
                self.last_summary_reset = datetime.now()
                self.news_first_seen = {}
                self.claude_cooldown_until = None
                
                # í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì •ë¦¬
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=8)
                self.sent_critical_reports = {
                    k: v for k, v in self.sent_critical_reports.items()
                    if v > cutoff_time
                }
                self._save_critical_reports()
                
                logger.info(f"ğŸ”„ ì¼ì¼ ë¦¬ì…‹ ì™„ë£Œ (ì¤‘ë³µ ì²´í¬: {self.duplicate_check_hours}ì‹œê°„, ì¿¨ë‹¤ìš´: {self.critical_report_cooldown_minutes}ë¶„)")
        except Exception as e:
            logger.error(f"ì¼ì¼ ë¦¬ì…‹ ì˜¤ë¥˜: {e}")
    
    async def get_recent_news_enhanced(self, hours: int = 8) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ í™•ì¥)"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_news = []
            seen_hashes = set()
            
            for article in sorted(self.news_buffer, key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True):
                try:
                    if article.get('published_at'):
                        pub_time_str = article.get('published_at', '')
                        try:
                            if 'T' in pub_time_str:
                                pub_time = datetime.fromisoformat(pub_time_str.replace('Z', ''))
                            else:
                                from dateutil import parser
                                pub_time = parser.parse(pub_time_str)
                            
                            if pub_time > cutoff_time:
                                content_hash = self._generate_content_hash(article.get('title', ''), '')
                                if content_hash not in seen_hashes:
                                    recent_news.append(article)
                                    seen_hashes.add(content_hash)
                        except:
                            pass
                except:
                    pass
            
            recent_news.sort(key=lambda x: (x.get('weight', 0), x.get('published_at', '')), reverse=True)
            
            logger.info(f"ğŸ”¥ ìµœê·¼ {hours}ì‹œê°„ ë‰´ìŠ¤: {len(recent_news)}ê°œ")
            
            return recent_news[:40]  # 40ê°œë¡œ ì¦ê°€
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def get_recent_news(self, hours: int = 8) -> List[Dict]:
        """ìµœê·¼ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (í˜¸í™˜ì„±)"""
        return await self.get_recent_news_enhanced(hours)
    
    def _is_critical_news(self, article: Dict) -> bool:
        """ê¸°ì¡´ í˜¸í™˜ì„±"""
        return self._is_critical_news_enhanced(article)
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            self._save_duplicate_data()
            self._save_critical_reports()
            
            if self.session:
                await self.session.close()
                logger.info("ğŸ”š ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì„¸ì…˜ ì¢…ë£Œ (403 ì˜¤ë¥˜ í•´ê²° ë²„ì „)")
                logger.info(f"ğŸ”„ ìµœì¢… ì„¤ì •: ì¤‘ë³µ ì²´í¬ {self.duplicate_check_hours}ì‹œê°„, ì¿¨ë‹¤ìš´ {self.critical_report_cooldown_minutes}ë¶„")
                
                stats = self.processing_stats
                if stats['total_articles_checked'] > 0:
                    logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
                    logger.info(f"  ì´ ê¸°ì‚¬: {stats['total_articles_checked']}ê°œ")
                    logger.info(f"  í¬ë¦¬í‹°ì»¬: {stats['critical_news_found']}ê°œ")
                    logger.info(f"  ì•Œë¦¼ ì „ì†¡: {stats['alerts_sent']}ê°œ")
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì˜¤ë¥˜: {e}")

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
import hashlib
import re
import json
import os

logger = logging.getLogger(__name__)

class NewsProcessor:
    """ë‰´ìŠ¤ ë¶„ì„, ë¶„ë¥˜, ì¤‘ë³µ ì²´í¬, ì´ë²¤íŠ¸ ìƒì„± ì „ë‹´"""
    
    def __init__(self, config):
        self.config = config
        
        # ì¤‘ìš” ê¸°ì—… ë¦¬ìŠ¤íŠ¸
        self.important_companies = [
            'tesla', 'microstrategy', 'square', 'block', 'paypal', 'mastercard', 'visa',
            'gamestop', 'gme', 'blackrock', 'fidelity', 'ark invest', 'grayscale',
            'coinbase', 'binance', 'kraken', 'bitget', 'okx', 'bybit',
            'metaplanet', 'ë©”íƒ€í”Œë˜ë‹›', 'í…ŒìŠ¬ë¼', 'ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€',
            'sberbank', 'ìŠ¤ë² ë¥´ë°©í¬', 'jpmorgan', 'goldman sachs', 'morgan stanley',
            'nvidia', 'amd', 'intel', 'apple', 'microsoft', 'amazon',
            'ì‚¼ì„±', 'samsung', 'lg', 'sk', 'hyundai'
        ]
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ
        self.critical_keywords = [
            # ë¹„íŠ¸ì½”ì¸ ETF ê´€ë ¨ (ìµœìš°ì„ )
            'bitcoin etf approved', 'bitcoin etf rejected', 'spot bitcoin etf', 'etf decision',
            'blackrock bitcoin etf', 'fidelity bitcoin etf', 'ark bitcoin etf', 'grayscale bitcoin etf',
            'SEC ë¹„íŠ¸ì½”ì¸ ETF', 'ETF ìŠ¹ì¸', 'ETF ê±°ë¶€', 'SEC approves bitcoin', 'SEC rejects bitcoin',
            
            # ê¸°ì—… ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤ (ì§ì ‘ì )
            'tesla bought bitcoin', 'microstrategy bought bitcoin', 'bought bitcoin', 'buys bitcoin',
            'gamestop bitcoin purchase', 'metaplanet bitcoin', 'corporate bitcoin purchase',
            'bitcoin acquisition', 'adds bitcoin', 'bitcoin investment', 'purchases bitcoin',
            'ë¹„íŠ¸ì½”ì¸ êµ¬ë§¤', 'ë¹„íŠ¸ì½”ì¸ ë§¤ì…', 'BTC êµ¬ë§¤', 'BTC ë§¤ì…', 'bitcoin holdings',
            
            # êµ­ê°€/ì€í–‰ ì±„íƒ
            'central bank bitcoin', 'russia bitcoin', 'sberbank bitcoin', 'bitcoin bonds',
            'government bitcoin', 'country adopts bitcoin', 'bitcoin legal tender',
            'ì¤‘ì•™ì€í–‰ ë¹„íŠ¸ì½”ì¸', 'ëŸ¬ì‹œì•„ ë¹„íŠ¸ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸ ì±„ê¶Œ', 'el salvador bitcoin',
            'putin bitcoin', 'russia legalize bitcoin', 'china bitcoin ban lifted',
            
            # ë¹„íŠ¸ì½”ì¸ ê·œì œ (ì§ì ‘ì )
            'sec bitcoin lawsuit', 'bitcoin ban', 'bitcoin regulation', 'bitcoin lawsuit',
            'china bans bitcoin', 'government bans bitcoin', 'court bitcoin', 'biden bitcoin',
            'regulatory approval bitcoin', 'regulatory rejection bitcoin', 'trump bitcoin',
            'SEC ë¹„íŠ¸ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸ ê¸ˆì§€', 'ë¹„íŠ¸ì½”ì¸ ê·œì œ', 'coinbase lawsuit',
            
            # ë¹„íŠ¸ì½”ì¸ ì‹œì¥ ê¸‰ë³€ë™
            'bitcoin crash', 'bitcoin surge', 'bitcoin breaks', 'bitcoin plunge',
            'bitcoin all time high', 'bitcoin ath', 'bitcoin tumbles', 'bitcoin soars',
            'ë¹„íŠ¸ì½”ì¸ í­ë½', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë“±', 'ë¹„íŠ¸ì½”ì¸ ê¸‰ë½', 'bitcoin reaches',
            'bitcoin hits', 'bitcoin falls below', 'bitcoin crosses',
            
            # ê°€ê²© ì´ì •í‘œ ê´€ë ¨
            'bitcoin crosses 100k', 'bitcoin hits 100000', 'bitcoin 100k milestone',
            'bitcoin google search', 'bitcoin interest low', 'bitcoin searches unchanged',
            
            # Fed ê¸ˆë¦¬ ê²°ì • (ë¹„íŠ¸ì½”ì¸ ì˜í–¥)
            'fed rate decision', 'fomc decision', 'powell speech', 'interest rate decision',
            'federal reserve meeting', 'fed minutes', 'inflation report', 'cpi data',
            'ì—°ì¤€ ê¸ˆë¦¬', 'ê¸°ì¤€ê¸ˆë¦¬', 'í†µí™”ì •ì±…', 'jobless claims', 'unemployment rate',
            
            # ê±°ì‹œê²½ì œ ì˜í–¥
            'us economic policy', 'treasury secretary', 'inflation data', 'cpi report',
            'unemployment rate', 'gdp growth', 'recession fears', 'economic stimulus',
            'quantitative easing', 'dollar strength', 'dollar weakness', 'dxy index',
            'ë‹¬ëŸ¬ ê°•ì„¸', 'ë‹¬ëŸ¬ ì•½ì„¸', 'ì¸í”Œë ˆì´ì…˜', 'ê²½ê¸°ì¹¨ì²´', 'china economic data',
            
            # ë¯¸êµ­ ê´€ì„¸ ë° ë¬´ì—­
            'trump tariffs', 'china tariffs', 'trade war', 'trade deal', 'trade agreement',
            'customs duties', 'import tariffs', 'export restrictions', 'trade negotiations',
            'trade talks deadline', 'tariff exemption', 'tariff extension', 'wto ruling',
            'ê´€ì„¸', 'ë¬´ì—­í˜‘ìƒ', 'ë¬´ì—­ì „ìŸ', 'ë¬´ì—­í•©ì˜', 'usmca agreement',
        ]
        
        # ì œì™¸ í‚¤ì›Œë“œ
        self.exclude_keywords = [
            'how to mine', 'ì§‘ì—ì„œ ì±„êµ´', 'mining at home', 'mining tutorial',
            'price prediction tutorial', 'ê°€ê²© ì˜ˆì¸¡ ë°©ë²•', 'technical analysis tutorial',
            'altcoin only', 'ethereum only', 'ripple only', 'cardano only', 'solana only', 
            'dogecoin only', 'shiba only', 'nft only', 'web3 only', 'metaverse only',
            'defi only', 'gamefi only', 'celebrity news', 'entertainment only',
            'sports only', 'weather', 'local news', 'obituary', 'wedding',
            'movie review', 'book review', 'restaurant review', 'travel guide'
        ]
        
        # ì¤‘ë³µ ë°©ì§€ ë°ì´í„°
        self.processed_news_hashes = set()
        self.sent_news_titles = {}
        self.sent_critical_reports = {}
        self.company_news_count = {}
        self.news_first_seen = {}
        
        # íŒŒì¼ ê²½ë¡œ
        self.news_data_file = 'news_duplicates.json'
        self.processed_reports_file = 'processed_critical_reports.json'
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        self._load_duplicate_data()
        self._load_critical_reports()
        
        # í˜„ì‹¤ì ì¸ ë‰´ìŠ¤ ì˜í–¥ íŒ¨í„´
        self.historical_patterns = {
            'etf_approval': {'avg_impact': 3.5, 'duration_hours': 24, 'confidence': 0.95},
            'etf_rejection': {'avg_impact': -2.8, 'duration_hours': 12, 'confidence': 0.9},
            'tesla_purchase': {'avg_impact': 2.2, 'duration_hours': 18, 'confidence': 0.9},
            'microstrategy_purchase': {'avg_impact': 0.7, 'duration_hours': 8, 'confidence': 0.85},
            'price_milestone': {'avg_impact': 0.2, 'duration_hours': 8, 'confidence': 0.6},
            'price_milestone_low_interest': {'avg_impact': 0.1, 'duration_hours': 4, 'confidence': 0.5},
            'ai_prediction': {'avg_impact': 0.05, 'duration_hours': 2, 'confidence': 0.3},
            'energy_crisis_prediction': {'avg_impact': 0.1, 'duration_hours': 4, 'confidence': 0.4},
            'fed_rate_hike': {'avg_impact': -1.2, 'duration_hours': 6, 'confidence': 0.7},
            'fed_rate_cut': {'avg_impact': 1.5, 'duration_hours': 8, 'confidence': 0.75},
            'new_tariffs': {'avg_impact': -0.8, 'duration_hours': 6, 'confidence': 0.65},
            'trade_deal': {'avg_impact': 0.6, 'duration_hours': 8, 'confidence': 0.7},
            'corporate_structured_product': {'avg_impact': 0.05, 'duration_hours': 2, 'confidence': 0.3},
        }
        
        logger.info(f"ğŸ“Š ë‰´ìŠ¤ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ¯ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ: {len(self.critical_keywords)}ê°œ")
        logger.info(f"ğŸ¢ ì¶”ì  ê¸°ì—…: {len(self.important_companies)}ê°œ")
        logger.info(f"ğŸ’¾ ì¤‘ë³µ ë°©ì§€: ì²˜ë¦¬ëœ ë‰´ìŠ¤ {len(self.processed_news_hashes)}ê°œ")
    
    def _load_duplicate_data(self):
        """ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.news_data_file):
                with open(self.news_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.processed_news_hashes = set(data.get('processed_news_hashes', []))
                
                # ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ì •ë¦¬
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=12)
                
                # ë‰´ìŠ¤ ì œëª© ìºì‹œ
                title_data = data.get('sent_news_titles', {})
                for title_hash, time_str in title_data.items():
                    try:
                        sent_time = datetime.fromisoformat(time_str)
                        if sent_time > cutoff_time:
                            self.sent_news_titles[title_hash] = sent_time
                    except:
                        continue
                
                # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œ í¬ê¸° ì œí•œ
                if len(self.processed_news_hashes) > 3000:
                    self.processed_news_hashes = set(list(self.processed_news_hashes)[-1500:])
                
                logger.info(f"âœ… ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ: {len(self.processed_news_hashes)}ê°œ")
                
        except Exception as e:
            logger.warning(f"âŒ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.processed_news_hashes = set()
            self.sent_news_titles = {}
    
    def _load_critical_reports(self):
        """í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists(self.processed_reports_file):
                with open(self.processed_reports_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=4)
                
                for item in data:
                    try:
                        report_time = datetime.fromisoformat(item['time'])
                        if report_time > cutoff_time:
                            self.sent_critical_reports[item['hash']] = report_time
                    except:
                        continue
                
                logger.info(f"âœ… í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ë¡œë“œ: {len(self.sent_critical_reports)}ê°œ")
                
        except Exception as e:
            logger.warning(f"âŒ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sent_critical_reports = {}
    
    def _save_duplicate_data(self):
        """ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥"""
        try:
            title_data = {}
            for title_hash, sent_time in self.sent_news_titles.items():
                title_data[title_hash] = sent_time.isoformat()
            
            data = {
                'processed_news_hashes': list(self.processed_news_hashes),
                'sent_news_titles': title_data,
                'last_updated': datetime.now().isoformat()
            }
            
            with open(self.news_data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_critical_reports(self):
        """í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì €ì¥"""
        try:
            data_to_save = []
            for report_hash, report_time in self.sent_critical_reports.items():
                data_to_save.append({
                    'hash': report_hash,
                    'time': report_time.isoformat()
                })
            
            with open(self.processed_reports_file, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def is_bitcoin_or_macro_related(self, article: Dict) -> bool:
        """ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ê´€ë ¨ì„± + ê±°ì‹œê²½ì œ ì˜í–¥ ì²´í¬"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ë¨¼ì € ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # 1. ë¹„íŠ¸ì½”ì¸ ì§ì ‘ ì–¸ê¸‰
        bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'bitcoins']
        has_bitcoin = any(keyword in content for keyword in bitcoin_keywords)
        
        if has_bitcoin:
            return True
        
        # 2. ì•”í˜¸í™”í ì¼ë°˜ + ì¤‘ìš” ë‚´ìš©
        crypto_keywords = ['crypto', 'cryptocurrency', 'ì•”í˜¸í™”í', 'cryptocurrencies', 'digital currency']
        has_crypto = any(keyword in content for keyword in crypto_keywords)
        
        if has_crypto:
            important_terms = ['etf', 'sec', 'regulation', 'ban', 'approval', 'court', 'lawsuit', 
                             'bonds', 'russia', 'sberbank', 'institutional', 'adoption']
            if any(term in content for term in important_terms):
                return True
        
        # 3. Fed ê¸ˆë¦¬ ê²°ì •
        fed_keywords = ['fed rate decision', 'fomc decides', 'powell announces', 'federal reserve decision',
                       'interest rate decision', 'fed chair', 'fed meeting', 'monetary policy']
        if any(keyword in content for keyword in fed_keywords):
            return True
        
        # 4. ì¤‘ìš” ê²½ì œ ì§€í‘œ
        economic_keywords = ['inflation data', 'cpi report', 'unemployment rate', 'jobs report',
                           'gdp growth', 'pce index', 'retail sales', 'manufacturing pmi']
        if any(keyword in content for keyword in economic_keywords):
            return True
        
        # 5. ë¯¸êµ­ ê´€ì„¸ ë° ë¬´ì—­
        trade_keywords = ['trump tariffs', 'china tariffs', 'trade war escalation', 'trade deal signed',
                         'trade agreement', 'trade negotiations breakthrough']
        if any(keyword in content for keyword in trade_keywords):
            return True
        
        return False
    
    def is_critical_news(self, article: Dict) -> bool:
        """í¬ë¦¬í‹°ì»¬ ë‰´ìŠ¤ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ë¹„íŠ¸ì½”ì¸ + ê±°ì‹œê²½ì œ ê´€ë ¨ì„± ì²´í¬
        if not self.is_bitcoin_or_macro_related(article):
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        # ê°€ì¤‘ì¹˜ ì²´í¬
        if article.get('weight', 0) < 6:
            return False
        
        # í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.critical_keywords:
            if keyword.lower() in content:
                # ë¶€ì •ì  í•„í„° (ë£¨ë¨¸, ì¶”ì¸¡ ë“±)
                negative_filters = ['rumor', 'speculation', 'unconfirmed', 'fake', 'false', 
                                  'ë£¨ë¨¸', 'ì¶”ì¸¡', 'ë¯¸í™•ì¸', 'alleged', 'reportedly']
                if any(neg in content for neg in negative_filters):
                    continue
                
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ í‚¤ì›Œë“œ ê°ì§€: '{keyword}' - {article.get('title', '')[:50]}...")
                return True
        
        # ì¶”ê°€ í¬ë¦¬í‹°ì»¬ íŒ¨í„´
        critical_patterns = [
            ('bitcoin', 'etf', 'approved'),
            ('bitcoin', 'etf', 'rejected'),  
            ('bitcoin', 'billion', 'bought'),
            ('bitcoin', 'crosses', '100k'),
            ('tesla', 'bitcoin', 'purchase'),
            ('fed', 'rate', 'decision'),
            ('trump', 'announces', 'tariffs'),
        ]
        
        for pattern in critical_patterns:
            if all(word in content for word in pattern):
                logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ íŒ¨í„´ ê°ì§€: {pattern} - {article.get('title', '')[:50]}...")
                return True
        
        return False
    
    def is_important_news(self, article: Dict) -> bool:
        """ì¤‘ìš” ë‰´ìŠ¤ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        if not self.is_bitcoin_or_macro_related(article):
            return False
        
        for exclude in self.exclude_keywords:
            if exclude.lower() in content:
                return False
        
        weight = article.get('weight', 0)
        category = article.get('category', '')
        
        conditions = [
            category == 'crypto' and weight >= 6,
            category == 'finance' and weight >= 6 and (
                any(word in content for word in ['bitcoin', 'btc', 'crypto']) or
                any(word in content for word in ['fed', 'rate', 'inflation', 'sec', 'tariffs', 'trade'])
            ),
            category == 'api' and weight >= 7,
            any(company.lower() in content for company in self.important_companies) and 
            any(word in content for word in ['bitcoin', 'btc', 'crypto', 'digital', 'blockchain']),
        ]
        
        return any(conditions)
    
    def extract_company_from_content(self, title: str, description: str = "") -> str:
        """ì»¨í…ì¸ ì—ì„œ ê¸°ì—…ëª… ì¶”ì¶œ"""
        content = (title + " " + description).lower()
        
        found_companies = []
        for company in self.important_companies:
            if company.lower() in content:
                for original in self.important_companies:
                    if original.lower() == company.lower():
                        found_companies.append(original)
                        break
        
        if found_companies:
            return found_companies[0]
        
        return ""
    
    def classify_news_type(self, article: Dict) -> str:
        """ë‰´ìŠ¤ íƒ€ì… ë¶„ë¥˜"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # AI ì˜ˆì¸¡ ê´€ë ¨
        if any(word in content for word in ['ai based', 'ai predicts', 'energy crisis boom']):
            if 'energy crisis' in content and any(word in content for word in ['250000', '25']):
                return 'energy_crisis_prediction'
            else:
                return 'ai_prediction'
        
        # ê°€ê²© ëŒíŒŒ/ì´ì •í‘œ ê´€ë ¨
        if any(word in content for word in ['crosses', '100k', '$100,000', 'milestone', 'breaks', 'hits']):
            if any(word in content for word in ['search', 'google', 'interest', 'attention']):
                return 'price_milestone'
        
        # ETF ê´€ë ¨
        if 'etf' in content:
            if any(word in content for word in ['approved', 'approval', 'launch']):
                return 'etf_approval'
            elif any(word in content for word in ['rejected', 'rejection', 'delay']):
                return 'etf_rejection'
        
        # ê¸°ì—… íˆ¬ì - ì§ì ‘ vs êµ¬ì¡°í™” ìƒí’ˆ êµ¬ë¶„
        if any(company in content for company in ['tesla', 'microstrategy', 'blackrock', 'gamestop']) and \
           any(word in content for word in ['bought', 'purchased', 'buys', 'adds']):
            return 'corporate_purchase_direct'
        
        # êµ¬ì¡°í™” ìƒí’ˆ
        if any(word in content for word in ['structured', 'bonds', 'linked', 'tracking', 'exposure']) and \
           any(word in content for word in ['bitcoin', 'btc']):
            return 'corporate_structured_product'
        
        # ê·œì œ ê´€ë ¨
        if any(word in content for word in ['regulation', 'legal', 'court']) and \
           any(word in content for word in ['positive', 'approved', 'favorable']):
            return 'regulation_positive'
        elif any(word in content for word in ['ban', 'prohibited', 'lawsuit', 'illegal']):
            return 'regulation_negative'
        
        # Fed ê¸ˆë¦¬ ë° ê±°ì‹œê²½ì œ
        if any(word in content for word in ['fed', 'fomc', 'federal reserve', 'interest rate']):
            return 'fed_rate_decision'
        elif any(word in content for word in ['trump', 'tariffs', 'trade war', 'china tariffs']):
            return 'trade_tariffs'
        elif any(word in content for word in ['inflation', 'cpi', 'pce']):
            return 'inflation_data'
        
        else:
            return 'macro_economic_general'
    
    def estimate_price_impact(self, article: Dict) -> str:
        """í˜„ì‹¤ì  ê°€ê²© ì˜í–¥ ì¶”ì •"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ê³¼ê±° íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡
        pattern_match = self._match_historical_pattern(content)
        if pattern_match:
            pattern_data = self.historical_patterns[pattern_match]
            impact = pattern_data['avg_impact']
            duration = pattern_data['duration_hours']
            
            if impact > 0:
                min_impact = max(0.05, impact * 0.8)
                max_impact = impact * 1.2
                direction = "ğŸ“ˆ ìƒìŠ¹"
                emoji = "ğŸš€" if impact >= 2.0 else "ğŸ“ˆ"
            else:
                min_impact = max(0.05, abs(impact) * 0.8)
                max_impact = abs(impact) * 1.2
                direction = "ğŸ“‰ í•˜ë½"
                emoji = "ğŸ”»" if abs(impact) >= 2.0 else "ğŸ“‰"
            
            return f"{emoji} {direction} {min_impact:.2f}~{max_impact:.2f}% ({duration}ì‹œê°„ ë‚´)"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        return self._estimate_by_keywords(content)
    
    def _match_historical_pattern(self, content: str) -> Optional[str]:
        """ê³¼ê±° íŒ¨í„´ ë§¤ì¹­"""
        patterns = {
            'etf_approval': ['bitcoin', 'etf', 'approved', 'sec'],
            'etf_rejection': ['bitcoin', 'etf', 'rejected', 'denied'],
            'tesla_purchase': ['tesla', 'bitcoin', 'bought', 'purchase'],
            'microstrategy_purchase': ['microstrategy', 'bitcoin', 'acquired', 'buy'],
            'price_milestone': ['bitcoin', 'crosses', '100k', 'milestone'],
            'price_milestone_low_interest': ['bitcoin', '100k', 'search', 'google'],
            'ai_prediction': ['ai', 'predicts', 'bitcoin', 'price'],
            'energy_crisis_prediction': ['energy', 'crisis', 'bitcoin', 'boom'],
            'fed_rate_hike': ['fed', 'raises', 'rate', 'hike'],
            'fed_rate_cut': ['fed', 'cuts', 'rate', 'lower'],
            'new_tariffs': ['trump', 'tariffs', 'china', 'new'],
            'trade_deal': ['trade', 'deal', 'agreement', 'signed'],
            'corporate_structured_product': ['structured', 'bonds', 'bitcoin', 'linked'],
        }
        
        for pattern_name, keywords in patterns.items():
            matches = sum(1 for keyword in keywords if keyword in content)
            if matches >= 2:
                return pattern_name
        
        return None
    
    def _estimate_by_keywords(self, content: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°€ê²© ì˜í–¥ ì¶”ì •"""
        # AI ì˜ˆì¸¡ ê´€ë ¨
        if any(word in content for word in ['ai based', 'ai predicts', 'energy crisis boom']):
            return 'âš¡ ë³€ë™ Â±0.05~0.15% (2ì‹œê°„ ë‚´)'
        
        # ê°€ê²© ì´ì •í‘œ ê´€ë ¨
        if any(word in content for word in ['bitcoin crosses 100k', 'bitcoin hits 100000']):
            if any(word in content for word in ['google search', 'search unchanged', 'interest low']):
                return 'ğŸ“Š ë¯¸ë¯¸í•œ ë°˜ì‘ +0.05~0.20% (4ì‹œê°„ ë‚´)'
            else:
                return 'ğŸ“ˆ ìƒìŠ¹ 0.10~0.40% (8ì‹œê°„ ë‚´)'
        
        # ETF ê´€ë ¨
        elif any(word in content for word in ['etf approved', 'etf approval', 'sec approves bitcoin']):
            return 'ğŸš€ ìƒìŠ¹ 2.50~4.00% (24ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['etf rejected', 'etf denial', 'sec rejects bitcoin']):
            return 'ğŸ”» í•˜ë½ 2.00~3.50% (12ì‹œê°„ ë‚´)'
        
        # Fed ê´€ë ¨
        elif any(word in content for word in ['fed raises rates', 'rate hike', 'hawkish fed']):
            return 'ğŸ“‰ í•˜ë½ 0.80~1.50% (6ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['fed cuts rates', 'rate cut', 'dovish fed']):
            return 'ğŸ“ˆ ìƒìŠ¹ 1.00~2.00% (8ì‹œê°„ ë‚´)'
        
        # ê¸°ì—… êµ¬ë§¤
        elif any(word in content for word in ['tesla bought bitcoin', 'tesla bitcoin purchase']):
            return 'ğŸš€ ìƒìŠ¹ 1.50~3.00% (18ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['microstrategy bought bitcoin', 'saylor bitcoin']):
            return 'ğŸ“ˆ ìƒìŠ¹ 0.50~1.20% (8ì‹œê°„ ë‚´)'
        
        # êµ¬ì¡°í™” ìƒí’ˆ
        elif any(word in content for word in ['structured', 'bonds', 'linked']):
            return 'ğŸ“Š ë¯¸ë¯¸í•œ ë°˜ì‘ +0.05~0.20% (2ì‹œê°„ ë‚´)'
        
        # ë¬´ì—­/ê´€ì„¸
        elif any(word in content for word in ['new tariffs', 'trade war']):
            return 'ğŸ“‰ í•˜ë½ 0.50~1.20% (6ì‹œê°„ ë‚´)'
        elif any(word in content for word in ['trade deal', 'trade agreement']):
            return 'ğŸ“ˆ ìƒìŠ¹ 0.40~1.00% (8ì‹œê°„ ë‚´)'
        
        # ê¸°ë³¸ê°’
        return 'âš¡ ë³€ë™ Â±0.20~0.80% (ë‹¨ê¸°)'
    
    def generate_content_hash(self, title: str, description: str = "") -> str:
        """ë‰´ìŠ¤ ë‚´ìš©ì˜ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)"""
        # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        content = f"{title} {description[:200]}".lower()
        
        # ìˆ«ì ì •ê·œí™”
        content = re.sub(r'[\d,]+', lambda m: m.group(0).replace(',', ''), content)
        
        # íšŒì‚¬ëª… ì •ê·œí™”
        companies_found = []
        for company in self.important_companies:
            if company.lower() in content:
                companies_found.append(company.lower())
        
        # ì•¡ì…˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        action_keywords = []
        actions = ['bought', 'purchased', 'acquired', 'adds', 'buys', 'sells', 'sold', 
                  'announced', 'launches', 'approves', 'rejects', 'bans', 'raises', 'cuts',
                  'crosses', 'hits', 'breaks', 'reaches']
        for action in actions:
            if action in content:
                action_keywords.append(action)
        
        # BTC ìˆ˜ëŸ‰ ì¶”ì¶œ
        btc_amounts = re.findall(r'(\d+(?:,\d+)*)\s*(?:btc|bitcoin)', content)
        
        # ê°€ê²© ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
        price_keywords = []
        price_terms = ['100k', '100000', 'milestone', 'search', 'google', 'interest']
        for term in price_terms:
            if term in content:
                price_keywords.append(term)
        
        # ê³ ìœ  ì‹ë³„ì ìƒì„±
        unique_parts = []
        if companies_found:
            unique_parts.append('_'.join(sorted(companies_found)))
        if action_keywords:
            unique_parts.append('_'.join(sorted(action_keywords)))
        if btc_amounts:
            unique_parts.append('_'.join(btc_amounts))
        if price_keywords:
            unique_parts.append('_'.join(sorted(price_keywords)))
        
        # í•´ì‹œ ìƒì„±
        if unique_parts:
            hash_content = '|'.join(unique_parts)
        else:
            # í•µì‹¬ ë‹¨ì–´ë§Œ ì¶”ì¶œ
            words = re.findall(r'\b[a-z]{4,}\b', content)
            important_words = [w for w in words if w not in ['that', 'this', 'with', 'from', 'have', 'been', 'their', 'about']]
            hash_content = ' '.join(sorted(important_words[:10]))
        
        return hashlib.md5(hash_content.encode()).hexdigest()
    
    def is_duplicate_emergency(self, article: Dict, time_window: int = 240) -> bool:
        """ê¸´ê¸‰ ì•Œë¦¼ì´ ì¤‘ë³µì¸ì§€ í™•ì¸ (4ì‹œê°„ ì´ë‚´)"""
        try:
            current_time = datetime.now()
            content_hash = self.generate_content_hash(
                article.get('title', ''), 
                article.get('description', '')
            )
            
            # í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ì²´í¬
            if content_hash in self.sent_critical_reports:
                last_sent = self.sent_critical_reports[content_hash]
                time_since_last = current_time - last_sent
                
                if time_since_last < timedelta(minutes=time_window):
                    logger.info(f"ğŸ”„ ì¤‘ë³µ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ë°©ì§€: {article.get('title', '')[:50]}... (ë§ˆì§€ë§‰ ì „ì†¡: {time_since_last})")
                    return True
            
            # ìƒˆë¡œìš´ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ë¡œ ê¸°ë¡
            self.sent_critical_reports[content_hash] = current_time
            
            # ì˜¤ë˜ëœ í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ê¸°ë¡ ì •ë¦¬
            cutoff_time = current_time - timedelta(hours=6)
            self.sent_critical_reports = {
                k: v for k, v in self.sent_critical_reports.items()
                if v > cutoff_time
            }
            
            # íŒŒì¼ì— ì €ì¥
            self._save_critical_reports()
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
            return False
    
    def determine_impact(self, article: Dict) -> str:
        """ë‰´ìŠ¤ ì˜í–¥ë„ íŒë‹¨"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        expected_change = self.estimate_price_impact(article)
        
        # AI ì˜ˆì¸¡ íŠ¹ë³„ ì²˜ë¦¬
        if any(word in content for word in ['ai predicts', 'energy crisis boom', 'ai based']):
            return "ğŸ’­ ì¶”ì¸¡ì„± ì˜ˆì¸¡"
        
        # ì˜ˆìƒ ë³€ë™ë¥ ì— ë”°ë¥¸ ì˜í–¥ë„
        if 'ğŸš€' in expected_change or any(x in expected_change for x in ['3%', '4%', '5%']):
            return "ğŸš€ ë§¤ìš° ê°•í•œ í˜¸ì¬"
        elif 'ğŸ“ˆ' in expected_change and any(x in expected_change for x in ['1.5%', '2%']):
            return "ğŸ“ˆ ê°•í•œ í˜¸ì¬"
        elif 'ğŸ“ˆ' in expected_change:
            return "ğŸ“ˆ í˜¸ì¬"
        elif 'ğŸ”»' in expected_change or any(x in expected_change for x in ['3%', '4%', '5%']):
            return "ğŸ”» ë§¤ìš° ê°•í•œ ì•…ì¬"
        elif 'ğŸ“‰' in expected_change and any(x in expected_change for x in ['1.5%', '2%']):
            return "ğŸ“‰ ê°•í•œ ì•…ì¬"
        elif 'ğŸ“‰' in expected_change:
            return "ğŸ“‰ ì•…ì¬"
        else:
            return "âš¡ ë³€ë™ì„± í™•ëŒ€"
    
    def calculate_urgency_level(self, article: Dict) -> str:
        """ê¸´ê¸‰ë„ ë ˆë²¨ ê³„ì‚°"""
        weight = article.get('weight', 0)
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì¦‰ì‹œ ë°˜ì‘ì´ í•„ìš”í•œ í‚¤ì›Œë“œ
        immediate_keywords = ['approved', 'rejected', 'announced', 'breaking', 'urgent', 'alert']
        has_immediate = any(keyword in content for keyword in immediate_keywords)
        
        if weight >= 10 and has_immediate:
            return "ê·¹ë„ ê¸´ê¸‰"
        elif weight >= 9:
            return "ë§¤ìš° ê¸´ê¸‰"
        elif weight >= 8:
            return "ê¸´ê¸‰"
        else:
            return "ì¤‘ìš”"
    
    def calculate_market_relevance(self, article: Dict) -> str:
        """ì‹œì¥ ê´€ë ¨ì„± ê³„ì‚°"""
        content = (article.get('title', '') + ' ' + article.get('description', '')).lower()
        
        # ì§ì ‘ì  ë¹„íŠ¸ì½”ì¸ ê´€ë ¨
        if any(word in content for word in ['bitcoin', 'btc']):
            return "ì§ì ‘ì "
        
        # ì•”í˜¸í™”í ì¼ë°˜
        elif any(word in content for word in ['crypto', 'cryptocurrency']):
            return "ì•”í˜¸í™”í"
        
        # ê±°ì‹œê²½ì œ
        elif any(word in content for word in ['fed', 'rate', 'inflation', 'gdp']):
            return "ê±°ì‹œê²½ì œ"
        
        # ì§€ì •í•™ì 
        elif any(word in content for word in ['war', 'sanctions', 'conflict']):
            return "ì§€ì •í•™ì "
        
        else:
            return "ê°„ì ‘ì "
    
    async def create_emergency_event(self, article: Dict, translator=None) -> Dict:
        """ê¸´ê¸‰ ì´ë²¤íŠ¸ ìƒì„± - asyncë¡œ ë³€ê²½"""
        try:
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
            content_hash = self.generate_content_hash(article.get('title', ''), article.get('description', ''))
            if content_hash in self.processed_news_hashes:
                return None
            
            # ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¡œ ê¸°ë¡
            self.processed_news_hashes.add(content_hash)
            
            # í¬ê¸° ì œí•œ
            if len(self.processed_news_hashes) > 5000:
                self.processed_news_hashes = set(list(self.processed_news_hashes)[-2500:])
            
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ ê¸°ë¡
            if content_hash not in self.news_first_seen:
                self.news_first_seen[content_hash] = datetime.now()
            
            # ë²ˆì—­ ì²˜ë¦¬ (translatorê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ)
            if translator and translator.should_translate_for_emergency_report(article):
                translated_title = await translator.translate_text(article.get('title', ''))
                article['title_ko'] = translated_title
                logger.info(f"ğŸ”¥ ê¸´ê¸‰ ë¦¬í¬íŠ¸ ë²ˆì—­ ì™„ë£Œ: {translated_title[:50]}...")
            else:
                article['title_ko'] = article.get('title', '')
            
            # ì´ë²¤íŠ¸ ìƒì„±
            event = {
                'type': 'critical_news',
                'title': article.get('title', ''),
                'title_ko': article.get('title_ko', article.get('title', '')),
                'description': article.get('description', '')[:1600],
                'summary': article.get('summary', ''),
                'company': self.extract_company_from_content(
                    article.get('title', ''),
                    article.get('description', '')
                ),
                'source': article.get('source', ''),
                'url': article.get('url', ''),
                'timestamp': datetime.now(),
                'severity': 'critical',
                'impact': self.determine_impact(article),
                'expected_change': self.estimate_price_impact(article),
                'weight': article.get('weight', 5),
                'category': article.get('category', 'unknown'),
                'published_at': article.get('published_at', ''),
                'first_seen': self.news_first_seen[content_hash],
                'urgency_level': self.calculate_urgency_level(article),
                'market_relevance': self.calculate_market_relevance(article),
                'pattern_match': self._match_historical_pattern(
                    (article.get('title', '') + ' ' + article.get('description', '')).lower()
                )
            }
            
            # íŒŒì¼ì— ì €ì¥
            self._save_duplicate_data()
            
            logger.info(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ ì´ë²¤íŠ¸ ìƒì„±: {event['impact']} - {event['title_ko'][:60]}...")
            
            return event
            
        except Exception as e:
            logger.error(f"âŒ ê¸´ê¸‰ ì´ë²¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def is_similar_news(self, title1: str, title2: str) -> bool:
        """ìœ ì‚¬ ë‰´ìŠ¤ íŒë³„"""
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean1 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title1.lower())
        clean2 = re.sub(r'[0-9$,.\-:;!?@#%^&*()\[\]{}]', '', title2.lower())
        
        clean1 = re.sub(r'\s+', ' ', clean1).strip()
        clean2 = re.sub(r'\s+', ' ', clean2).strip()
        
        # íŠ¹ì • íšŒì‚¬ì˜ ë¹„íŠ¸ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ ì²´í¬
        for company in self.important_companies:
            company_lower = company.lower()
            if company_lower in clean1 and company_lower in clean2:
                bitcoin_keywords = ['bitcoin', 'btc', 'ë¹„íŠ¸ì½”ì¸', 'crypto', 'purchase', 'bought']
                if any(keyword in clean1 for keyword in bitcoin_keywords) and \
                   any(keyword in clean2 for keyword in bitcoin_keywords):
                    return True
        
        # ë‹¨ì–´ ì§‘í•© ë¹„êµ
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        if not words1 or not words2:
            return False
        
        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        similarity = intersection / union if union > 0 else 0
        
        # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
        return similarity > 0.8
    
    def cleanup_old_data(self):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        try:
            current_time = datetime.now()
            
            # ì¼ì¼ ë¦¬ì…‹
            if current_time.date() > getattr(self, 'last_cleanup_date', current_time.date() - timedelta(days=1)):
                self.company_news_count = {}
                self.news_first_seen = {}
                
                # í¬ë¦¬í‹°ì»¬ ë¦¬í¬íŠ¸ ì¤‘ë³µ ë°©ì§€ ë°ì´í„° ì •ë¦¬
                cutoff_time = current_time - timedelta(hours=12)
                self.sent_critical_reports = {
                    k: v for k, v in self.sent_critical_reports.items()
                    if v > cutoff_time
                }
                self._save_critical_reports()
                
                logger.info(f"ğŸ”„ ë‰´ìŠ¤ ì²˜ë¦¬ê¸° ì¼ì¼ ë¦¬ì…‹ ì™„ë£Œ")
                self.last_cleanup_date = current_time.date()
                
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
